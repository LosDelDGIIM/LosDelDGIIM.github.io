Este documento ha sido creado con el objetivo de ser un manual de rápida lectura que sirva de recordatorio de las distribuciones más usadas en las asignaturas de EDIP, Probabilidad e Inferencia Estadística; así como de ciertas propiedades útiles a la hora de calcular ciertas probabilidades. En ningún momento pretende ser un manual riguroso de cómo se definen dichas probabilidades, un resumen de las asignaturas, o un recurso relevante de ningún tipo.

\section{Distribuciones discretas}
\subsection{Distribución degenerada}
\begin{description}
    \item [Espacio muestral.] $\cc{X} = \{c\}$ con $c\in \mathbb{R}$.
    \item [Función masa de probabilidad.] 
        \begin{equation*}
            P[X=x] = \left\{\begin{array}{ll}
                1 & \text{si\ } x=c \\
                0 & \text{si\ } x\neq c
            \end{array}\right. 
        \end{equation*}
    \item [Función de distribución.] 
        \begin{equation*}
            F_X(x) = \left\{\begin{array}{ll}
                0 & \text{si\ } x<c  \\
                1 & \text{si\ } c\leq x
            \end{array}\right. 
        \end{equation*}
    \item [Función generatriz de momentos.] 
        \begin{equation*}
            M_X(t) = e^{tc} \qquad \forall t\in \mathbb{R}
        \end{equation*}
    \item [Esperanza y varianza.] 
        \begin{equation*}
            E[X] = c, \qquad Var(X) = 0
        \end{equation*}
\end{description}

\subsection{Uniforme discreta}
\begin{equation*}
    X\rightsquigarrow U(x_1, \ldots, x_n), \qquad x_1,\ldots,x_n \in \mathbb{R}
\end{equation*}
\begin{description}
    \item [Espacio muestral.] $\cc{X} = \{x_1, \ldots, x_n\}$.
    \item [Función masa de probabilidad.] 
        \begin{equation*}
            P[X=x] = \left\{\begin{array}{ll}
                \nicefrac{1}{n} & \text{si\ } x=x_i,~i \in \{1,\ldots,n\} \\
                0 & \text{si\ } x\neq x_i,~\forall i \in \{1,\ldots,n\}
            \end{array}\right. 
        \end{equation*}
    \item [Función de distribución.] 
        \begin{equation*}
            F_X(x) = \left\{\begin{array}{cl}
                0 & \text{si\ } x<x_1 \\
                \nicefrac{k}{n} & \text{si\ } x\in \left[x_k,x_{k+1}\right[\\
                 1& \text{si\ } x_n\leq x
            \end{array}\right. 
        \end{equation*}
    \item [Función generatriz de momentos.] 
        \begin{equation*}
            M_X(t) = \dfrac{1}{n}\sum_{i =1}^{n} e^{tx_i} \qquad \forall t\in \mathbb{R}
        \end{equation*}
    \item [Esperanza.] 
        \begin{equation*}
            E[X] = \dfrac{1}{n}\sum_{i=1}^{n}x_i
        \end{equation*}
\end{description}

\subsection{Distribución de Bernoulli}
\noindent
Si $X$ modela el número de éxitos en una ocurrencia de un experimento con probablidad $p\in \left]0,1\right[$ de éxito.
\begin{equation*}
    X\rightsquigarrow B(p), \qquad p\in \left]0,1\right[
\end{equation*}

\begin{description}
    \item [Espacio muestral.]  $\cc{X} = \{0,1\}$.
    \item [Función masa de probabilidad.] 
        \begin{equation*}
            P[X=x]= \left\{\begin{array}{ll}
                p & \text{si\ } x=1 \\
                1-p & \text{si\ } x=0 \\
                0 & \text{si\ } x\notin \{0,1\}
            \end{array}\right.  = p^x{(1-p)}^{1-x}
        \end{equation*}
    \item [Función de distribución.] 
        \begin{equation*}
            F_X(x) = \left\{\begin{array}{ll}
                0 & \text{si\ } x<0 \\
                1-p & \text{si\ } 0\leq x<1 \\
                 1 & \text{si\ } 1\leq x
            \end{array}\right. 
        \end{equation*}
    \item [Función generatriz de momentos.] 
        \begin{equation*}
            M_X(t) = 1+p(e^t-1) \qquad \forall t\in \mathbb{R}
        \end{equation*}
    \item [Esperanza y varianza.] 
        \begin{equation*}
            E[X] = p, \qquad Var(X) = p(1-p)
        \end{equation*}
\end{description}

\subsection{Binomial}
\noindent
Si $X$ modela el número de éxitos en $n\in \mathbb{N}$ repeticiones de un experimento de Bernoulli con probabilidad $p\in \left]0,1\right[$ de éxito.
\begin{equation*}
    X\rightsquigarrow B(n,p), \qquad n\in \mathbb{N},\quad  p\in \left]0,1\right[
\end{equation*}
Notemos que $B(1,p) \equiv B(p)$.

\begin{description}
    \item [Espacio muestral.] $\cc{X} = \{0, \ldots, n\}$.
    \item [Función masa de probabilidad.] 
        \begin{equation*}
            P[X=x] = \binom{n}{x}p^x{(1-p)}^{n-x}
        \end{equation*}
    \item [Función generatriz de momentos.] 
        \begin{equation*}
            M_X(t) = {\left[1+p(e^t-1)\right]}^{n} \qquad \forall t\in \mathbb{R}
        \end{equation*}
    \item [Esperanza y varianza.] 
        \begin{equation*}
            E[X] = np, \qquad Var(X) = np(1-p)
        \end{equation*}
    \item [Propiedades.]\ \\
        \begin{itemize}
            \item Sus valores se encuentran tabulados para distintos valores de $n$ y $p$.
            \item Si $X\rightsquigarrow B(n,p)$ y $Y=n-X$ es la variable que contabiliza el número de fracasos, entonces $Y\rightsquigarrow B(n,1-p)$ y:
                \begin{equation*}
                    P[X=x] = P[Y=n-x]
                \end{equation*}
        \end{itemize}
\end{description}

\subsection{Distribución Geométrica}
\noindent
Si $X$ modela el número de fracasos antes de llegar al primer éxito en un experimento de Bernoulli con probabilidad de éxito $p\in \left]0,1\right[$.
\begin{equation*}
    X\rightsquigarrow G(p), \qquad p\in \left]0,1\right[
\end{equation*}
\begin{description}
    \item [Espacio muestral.] $\cc{X} = \mathbb{N}\cup \{0\}$.
    \item [Función masa de probabilidad.] 
        \begin{equation*}
            P[X=x] = {(1-p)}^{x}p
        \end{equation*}
    \item [Función de distribución.] 
        \begin{equation*}
            F_X(x) = \left\{\begin{array}{ll}
                1-{(1-p)}^{x+1} & \text{si\ } x\geq 0 \\
                 0 & \text{si\ } x < 0
            \end{array}\right. 
        \end{equation*}
    \item [Función generatriz de momentos.] 
        \begin{equation*}
            M_X(t) = \dfrac{p}{1-(1-p)e^t} \qquad \forall t < -\ln(1-p)
        \end{equation*}
    \item [Esperanza y varianza.] 
        \begin{equation*}
            E[X] = \dfrac{1-p}{p}, \qquad Var(X) = \dfrac{1-p}{p^2}
        \end{equation*}
    \item [Propiedades.] 
        Cumple la propiedad de la falta de memoria:
        \begin{equation*}
            P[X\geq h+k\ |\ X\geq h] = P[X\geq k] \qquad \forall h,k\in \cc{X}
        \end{equation*}
\end{description}

\subsection{Binomial Negativa}
\noindent
Si $X$ modela el número de fracasos antes de llegar al $r-$ésimo éxito (con $r\in \mathbb{N}$) en varias repeticiones de un experimento de Bernoulli de probabilidad de éxito $p\in \left]0,1\right[$.
\begin{equation*}
    X\rightsquigarrow BN(r,p), \qquad r\in \mathbb{N},\quad p\in \left]0,1\right[
\end{equation*}
Notemos que $BN(1,p)\equiv G(p)$.
\begin{description}
    \item [Espacio muestral.] $\cc{X} = \mathbb{N}\cup \{0\}$.
    \item [Función masa de probabilidad.] 
        \begin{equation*}
            P[X=x] = \binom{x+r-1}{x} {(1-p)}^{x}p^r
        \end{equation*}
    \item [Función generatriz de momentos.] 
        \begin{equation*}
            M_X(t) = {(\dfrac{p}{1-(1-p)e^t})}^{r} \qquad \forall t < -\ln (1-p)
        \end{equation*}
    \item [Esperanza y varianza.] 
        \begin{equation*}
            E[X] = \dfrac{r(1-p)}{p},\qquad Var(X) = \dfrac{r(1-p)}{p^2}
        \end{equation*}
\end{description}

\subsection{Hipergeométrica}
\noindent
Si $X$ modela el número de individuos de una especie de $N_1\in \mathbb{N}$ ejemplares en una población de tamaño $N\in \mathbb{N}$ al tomar una muestra de $n\in \mathbb{N}$ individuos.
\begin{equation*}
    X\rightsquigarrow H(N,N_1,n), \qquad n,N,N_1\in \mathbb{N}, \quad N_1,n\leq N
\end{equation*}
\begin{description}
    \item [Espacio muestral.] $\cc{X} = [\max\{0,n-(N-N_1)\}, \min\{n,N_1\}]$.
    \item [Función masa de probabilidad.] 
        \begin{equation*}
            P[X=x] = \dfrac{\binom{N_1}{x}\binom{N-N_1}{n-x}}{\binom{N}{n}}
        \end{equation*}
    \item [Esperanza y varianza.] 
        \begin{equation*}
            E[X] = n\dfrac{N_1}{N}, \qquad Var(X) = \dfrac{n(N-n)N_1(N-N_1)}{N^2(N-1)}
        \end{equation*}
\end{description}

\subsection{Poisson}
\noindent
Si $X$ modela el número de ocurrencias de un determinado suceso durante un periodo de tiempo fijo en una región fija del espacio con una media de $\lm \in \mathbb{R}^+$ ocurrencias.
\begin{equation*}
    X\rightsquigarrow\cc{P}(\lm), \qquad \lm \in \mathbb{R}^+
\end{equation*}
\begin{description}
    \item [Espacio muestral.] $\cc{X} = \mathbb{N}\cup \{0\}$.
    \item [Función masa de probabilidad.] 
        \begin{equation*}
            P[X=x] = e^{-\lm} \dfrac{\lm^x}{x!}
        \end{equation*}
    \item [Función generatriz de momentos.] 
        \begin{equation*}
            M_X(t) = e^{\lm(e^t-1)} \qquad \forall t\in \mathbb{R}
        \end{equation*}
    \item [Esperanza y varianza.] 
        \begin{equation*}
            E[X] = \lm = Var(X)
        \end{equation*}
\end{description}
Sus valores se encuetran tabulados para distintos valores de $\lm$.

\section{Distribuciones continuas}
\subsection{Uniforme continua}
\begin{equation*}
    X\rightsquigarrow U(a,b), \qquad a,b\in \mathbb{R}, \quad a<b
\end{equation*}
\begin{description}
    \item [Espacio muestral.] $\cc{X} = [a,b]$
    \item [Función masa de probabilidad.] 
        \begin{equation*}
            f(x) = \left\{\begin{array}{ll}
                    \dfrac{1}{b-a} & \text{si\ } x\in [a,b] \\
                    0 & \text{si\ } x\notin [a,b]
            \end{array}\right. 
        \end{equation*}
    \item [Función de distribución.] 
        \begin{equation*}
            F_X(x) = \left\{\begin{array}{ll}
                0 & \text{si\ } x<a \\
                \dfrac{x-a}{b-a} & \text{si\ } x\in [a,b]\\
                 1& \text{si\ } x>b
            \end{array}\right. 
        \end{equation*}
    \item [Función generatriz de momentos.] 
        \begin{equation*}
            M_X(t) = \dfrac{e^{tb}-e^{ta}}{(b-a)t} \qquad \forall t\in \mathbb{R}^\ast
        \end{equation*}
    \item [Esperanza.] 
        \begin{equation*}
            E[X] = \dfrac{b+a}{2}
        \end{equation*}
\end{description}

\subsection{Normal}
\begin{equation*}
    X\rightsquigarrow\cc{N}(\mu, \sigma^2), \qquad \mu\in \mathbb{R}, \quad \sigma^2 \in \mathbb{R}^+
\end{equation*}
\begin{description}
    \item [Espacio muestral.] $\cc{X}=\mathbb{R}$.
    \item [Función masa de probabilidad.] 
        \begin{equation*}
            f(x) = \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{{(x-\mu)}^{2}}{2\sigma^2}}
        \end{equation*}
    \item [Función generatriz de momentos.] 
        \begin{equation*}
            M_X(t) = e^{\mu t + \frac{\sigma^2t^2}{2}}\qquad \forall t\in \mathbb{R}
        \end{equation*}
    \item [Esperanza y varianza.] 
        \begin{equation*}
            E[X] = \mu, \qquad Var(X) = \sigma^2
        \end{equation*}
    \item [Propiedades.] \ \\
        \begin{itemize}
            \item \textbf{Tipificación.} Si $X\rightsquigarrow \cc{N}(\mu, \sigma^2)$, entonces:
                \begin{equation*}
                    Z = \dfrac{X-\mu}{\sigma}\rightsquigarrow\cc{N}(0,1)
                \end{equation*}
            \item \textbf{Simétrica.} Si $X\rightsquigarrow\cc{N}(\mu, \sigma^2)$, entonces:
                \begin{equation*}
                    P[X\leq \mu - x] = P[X\geq \mu + x] \qquad \forall x\in \mathbb{R}
                \end{equation*}
            \item Si $X\rightsquigarrow\cc{N}(\mu,\sigma^2)$, entonces:
                \begin{equation*}
                    \mu = E[X] = Me[X] = Mo[X]
                \end{equation*}
        \end{itemize}
\end{description}

\section{Aproximaciones}
\subsection{De Hipergeométrica a Binomial}
\noindent
Sea $X\rightsquigarrow H(N,N_1,n)$, si $N$ es más grande que $N_1$, si tomamos $p = \frac{N_1}{N}$, tenemos que $X$ puede aproximarse por una distribución binomial $B(n,p)$.

\subsection{De Binomial a Poisson}
\noindent
Sea $X\rightsquigarrow B(n,p)$, si $n$ es muy grande y $p$ es aproximadamente $0$, si tomamos $\lm = np$, tenemos que $X$ puede aproximarse por $\cc{P}(\lm)$.

\subsection{De Binomial a Normal}
\noindent
Sea $X\rightsquigarrow B(n,p)$, para $n$ grande y $p$ lejos de $0$ o $1$, tenemos que $X$ puede aproximarse por una distribución normal $\cc{N}(np, np(1-p))$.

\subsection{De Poisson a Normal}
\noindent
Sea $X\rightsquigarrow\cc{P}(\lm)$, para $\lm$ grande, tenemos que $X$ puede aproximarse por una distribución normal $\cc{N}(\lm, \lm)$.

\subsection{Corrección por continuidad}
Debemos tener cuidado al aproximar variables discretas por continuas:
\begin{itemize}
    \item Para aproximar $P[X=x_i]$ en una variable discreta, lo aproximaremos por:
        \begin{equation*}
            P[x_i - 0.5 \leq X \leq x_i + 0.5]
        \end{equation*}
        en la normal.
    \item Para aproximar $P[X\leq x_i]$, lo aproximaremos por $P[X\leq x_i + 0.5]$.
    \item Para aproximar $P[X\geq x_i]$, lo aproximaremos por $P[X\geq x_i - 0.5]$.
\end{itemize}
