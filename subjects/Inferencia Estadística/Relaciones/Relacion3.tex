\newpage
\section{Suficiencia y completitud}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable $X\rightsquigarrow\{B(k,p) : p\in \left]0,1\right[\}$ y sea $T(X_1, \ldots, X_n) = \sum\limits_{i=1}^n X_i$. Probar
    \begin{enumerate}[label=\alph*)]
        \item usando la definición
        \item aplicando el teorema de factorización
    \end{enumerate}
    que $T$ es suficiente para $p$.

    \begin{enumerate}[label=\alph*)]
        \item Si notamos para abreviar $T=T(X_1, \ldots, X_n)$, tenemos que probar que la distribución de la muestra condicionada a cualquier valor del estadístico no depende del parámetro $p$ para probar que $T$ es suficiente para $p$. Para ello:
            \begin{align*}
                P_p[X_1 = x_1, \ldots, X_n = x_n \ &|\ T=t] = \dfrac{P_p[X_1=x_1, \ldots, X_n = x_n, T=t]}{P_p[T=t]} \\
                                                           &= \left\{\begin{array}{ll}
                                                               0 & \text{si\ } T(x_1,\ldots,x_n)\neq t \\
                                                               \dfrac{P_p[X_1=x_1, \ldots, X_n=x_n]}{P_p[T=t]}& \text{si\ } T(x_1,\ldots,x_n) = t
                                                           \end{array}\right. 
            \end{align*}
            Como $0$ obviamente no depende de $p$, el caso $T(x_1,\ldots,x_2)\neq t$ se encuentra ya estudiado, por lo que nos centramos en el caso $T(x_1,\ldots,x_2)=t$:
            \begin{align*}
                \dfrac{P_p[X_1=x_1, \ldots, X_n = x_n]}{P_p[T=t]} &\stackrel{\text{iid.}}{=} \dfrac{\prod\limits_{i=1}^{n}P_p[X=x_i]}{P_p[T=t]} \AstIg \dfrac{\prod\limits_{i=1}^n \binom{k}{x_i} p^{x_i} {(1-p)}^{k-x_i}}{\binom{nk}{t}p^t{(1-p)}^{nk-t}} \\
                                                                  &= \dfrac{p^{\sum\limits_{i=1}^n x_i} {(1-p)}^{nk - \sum\limits_{i=1}^n x_i}\prod\limits_{i=1}^n \binom{k}{x_i}}{\binom{nk}{t}p^t{(1-p)}^{nk-t}} \stackrel{(\ast\ast)}{=} \dfrac{\prod\limits_{i=1}^{n}\binom{k}{x_i}}{\binom{nk}{t}}
            \end{align*}
            Donde en $(\ast)$ hemos usado que $(X_1, \ldots, X_n)$ es una m.a.s. (variables independientes) y la reproductividad de la Binomial, por lo que $T\rightsquigarrow B(nk,p)$; y en $(\ast\ast)$ hemos usado que $t=T(x_1, \ldots, x_n) = \sum\limits_{i=1}^{n}x_i$. En definitiva, hemos obtenido que la distribución de la muestra condicionada a cualquier valor del estadístico no dependende del parámetro $p$, por lo que $T$ es suficiente para $p$.
        \item Si podemos usar el Teorema de factorización, escribimos la función masa de probabilidad de la distribución conjunta de la muestra aleatoria simple:
            \begin{align*}
                P[X_1=x_1, \ldots, X_n = x_n] &\stackrel{\text{iid.}}{=} \prod_{i=1}^{n} P[X=x_i] = \prod_{i=1}^{n} \binom{k}{x_i} p^{x_i} {(1-p)}^{k-x_i} \\
                                              &= p^{\sum\limits_{i=1}^n x_i} {(1-p)}^{nk - \sum\limits_{i=1}^n x_i} \prod_{i=1}^{n} \binom{k}{x_i} 
            \end{align*}
            Si tomamos:
            \begin{equation*}
                h(x_1, \ldots, x_n) = \prod_{i=1}^{n}\binom{k}{x_i}, \qquad 
                T(X_1, \ldots, X_n) = \sum_{i=1}^{n} X_i, \qquad 
                g_p(t) = p^t {(1-p)}^{nk - t}
            \end{equation*}
            Podemos aplicar el Teorema de Factorización de Neymann-Fisher, obteniendo que $T$ es un estadístico suficiente para $p$.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable $X\rightsquigarrow\{\cc{P}(\lm) : \lm\in \mathbb{R}^+\}$ y sea $T(X_1, \ldots, X_n) = \sum\limits_{i=1}^n X_i$. Probar
    \begin{enumerate}[label=\alph*)]
        \item usando la definición
        \item aplicando el teorema de factorización
    \end{enumerate}
    que $T$ es suficiente para $\lm$.

    \begin{enumerate}[label=\alph*)]
        \item Notando $T=T(X_1, \ldots, X_n)$, seguimos los mismos pasos que en el ejercicio anterior:
            \begin{align*}
                P_\lm[X_1 = x_1, \ldots, X_n = x_n \ &|\ T=t] = \dfrac{P_\lm[X_1=x_1, \ldots, X_n = x_n, T=t]}{P_\lm[T=t]} \\
                                                           &= \left\{\begin{array}{ll}
                                                               0 & \text{si\ } T(x_1,\ldots,x_n)\neq t \\
                                                               \dfrac{P_\lm[X_1=x_1, \ldots, X_n=x_n]}{P_\lm[T=t]}& \text{si\ } T(x_1,\ldots,x_n) = t
                                                           \end{array}\right. 
            \end{align*}
            Y ahora nos interesamos por el segundo término, que es el que puede depender de $\lm$:
            \begin{align*}
                \dfrac{P_\lm[X_1 = x_1, \ldots, X_n = x_n]}{P_\lm[T=t]} &\stackrel{\text{iid.}}{=} \dfrac{\prod\limits_{i=1}^{n} P_\lm[X=x_i]}{P_\lm[T=t]} \AstIg \dfrac{\prod\limits_{i=1}^n e^{-\lm} \dfrac{\lm^{x_i}}{x_i!}}{e^{-n\lm} \dfrac{{(n\lm)}^{t}}{t!}} = \dfrac{e^{-n\lm} \cdot \dfrac{\lm^{\sum\limits_{i=1}^n x_i}}{\prod\limits_{i=1}^n x_i!}}{e^{-n\lm}\cdot  \dfrac{n^t \lm^t}{t!}} \\
                                                                        &\stackrel{(\ast\ast)}{=} \dfrac{t!}{n^t \prod\limits_{i=1}^{n}x_i!}
            \end{align*}
            Donde en $(\ast)$ usamos que $T\rightsquigarrow P(n\lm)$, por la reproductividad de la Poisson y en $(\ast\ast)$ usamos que $t = T(x_1, \ldots, x_n) = \sum_{i=1}^{n} x_i$. Obtenemos una cantidad que no depende de $\lm$, por lo que $T$ es suficiente para $\lm$.
        \item Si podemos aplicar el Teorema de factorización, escribimos:
            \begin{equation*}
                P_\lm[X_1=x_1, \ldots, X_n = x_n] \stackrel{\text{iid.}}{=} \prod_{i=1}^{n} P_\lm[X=x_i] = \prod_{i=1}^{n}e^{-\lm}\dfrac{\lm^{x_i}}{x_i!} = e^{-n\lm} \cdot \dfrac{\lm^{\sum\limits_{i=1}^{n} x_i}}{\prod\limits_{i=1}^{n}x_i!}
            \end{equation*}
            Si tomamos:
            \begin{equation*}
                h(x_1, \ldots, x_n) = \dfrac{1}{\prod\limits_{i=1}^{n}x_i!}, \qquad T(X_1, \ldots, X_n) = \sum_{i=1}^{n}X_i, \qquad g_\lm(t) = e^{-n\lm} \cdot \lm^t
            \end{equation*}
            Por el Teorema de Factorización de Neymann-Fisher obtenemos que $T$ es suficiente para $\lm$.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, X_2, X_3)$ una muestra aleatoria simple de una variable $X\rightsquigarrow\{B(1,p) : p \in \left]0,1\right[\}$. Probar que el estadístico $X_1 + 2X_2 + 3X_3$ no es suficiente.\\

    \noindent
    Consideramos el estadístico $T(X_1, X_2, X_3) = X_1 + 2X_2 + 3X_3$. El espacio muestral de $X$ es $\cc{X}=\{0,1\}$, por lo que el espacio muestral de $T$ es $\cc{T} = \{0, 1, 2, 3, 4, 5, 6\}$. Sabemos por un ejemplo visto en teoría que el ``truco'' para demostrar que $T(X_1, X_2,X_3)$ no es suficiente para $p$ es buscar un valor del espacio muestral $\cc{T}$ que provenga de varias combinaciones de estados del espacio muestral $\cc{X}^3$. Como $0,1,2$ solo provienen de una combinación del espacio muestral $\cc{X}^3$ (son $(0,0,0)$, $(1,0,0)$ y $(0,1,0)$ respectivamente), probamos buscar el contraejemplo con $t=3$, que proviene de considerar las observaciones de la muestra $(1,1,0)$ y $(0,0,1)$.

    Una vez explicado el procedimiento para buscar cuál es el valor de $t$ que funciona, procedemos a probar que la distribución de la muestra condicionada a dicho valor de $t$ depende del parámetro $p$. Para ello:
            \begin{align*}
                P_p[X_1 = x_1, X_2=x_2, X_3 = x_3 \ &|\ T=3] = \dfrac{P_p[X_1=x_1, X_2=x_2, X_3 = x_3, T=3]}{P_p[T=3]} \\
                                                           &= \left\{\begin{array}{ll}
                                                               0 & \text{si\ } T(x_1,x_2,x_2)\neq 3 \\
                                                               \dfrac{P_p[X_1=x_1, X_2=x_2, X_3=x_3]}{P_p[T=3]}& \text{si\ } T(x_1,x_2,x_3) = 3
                                                           \end{array}\right. 
            \end{align*}
            Vemos que el primer caso no puede depender nunca de $p$, por lo que buscamos probar que el segundo caso sí que depende de $p$. Para ello:
            \begin{align*}
                \dfrac{P_p[X_1=x_1, X_2=x_2, X_3=x_3]}{P_p[T=3]} &\stackrel{\text{iid.}}{=} \dfrac{P_p[X=x_1]P_p[X=x_2]P_p[X=x_3]}{P_p[T=3]} \\
                                                                 &\AstIg \dfrac{p^{x_1}{(1-p)}^{1-x_1} p^{x_2}{(1-p)}^{1-x_2}p^{x_3}{(1-p)}^{1-x_3}}{P_p[X_1=1,X_2=1,X_3=0]+P_p[X_1=0,X_2=0,X_3=1]} \\
                                                                 &= \dfrac{p^{x_1}{(1-p)}^{1-x_1} p^{x_2}{(1-p)}^{1-x_2}p^{x_3}{(1-p)}^{1-x_3}}{p\cdot p\cdot (1-p) + (1-p)(1-p)p} \\
            \end{align*}
            Donde en $(\ast)$ he usado la propiedad reproductiva de la Binomial, por lo que $T\rightsquigarrow B(3,p)$, así como que la condición $t=3$ provenía de los valores $(1,1,0)$ y $(0,0,1)$.
            Ahora, si tomamos $(x_1,x_2,x_3) = (1,1,0)$, tenemos que:
            \begin{align*}
                \dfrac{P_p[X_1=1, X_2=1, X_3=0]}{P_p[T=3]} &= \dfrac{p^{1}{(1-p)}^{0} p^{1}{(1-p)}^{0}p^{0}{(1-p)}^{1}}{p\cdot p\cdot (1-p) + (1-p)(1-p)p} \\
                                                                 &= \dfrac{pp(1-p)}{pp(1-p)+(1-p)(1-p)p} \\
                                                                 &= \dfrac{p^2(1-p)}{p(1-p)\cancelto{1}{(p+1-p)}} = p
            \end{align*}
            Que claramente depende de $p$, por lo que $T$ no es suficiente para $p$.
\end{ejercicio}

\begin{ejercicio}
    Aplicando el teorema de factorización, y basándose en una muestra de tamaño arbitrario, encontrar un estadístico suficiente para cada una de las siguientes familias de distribuciones (en las familias biparamétricas, suponer los casos de sólo un parámetro desconocido y de los dos desconocidos).
    \begin{enumerate}[label=\alph*)]
        \item $X\rightsquigarrow\{U(-\nicefrac{\theta}{2},\nicefrac{\theta}{2}) : \theta>0\}$
        \item $X\rightsquigarrow\{\Gamma(p,a) : p,a > 0\}$
        \item $X\rightsquigarrow\{\beta(p,q) : p,q>0\}$
        \item $X\rightsquigarrow\{P_{N_1,N_2} : N_1, N_2 \in \mathbb{N}, N_1 \leq N_2\}$ y la masa de probabilidad viene dada por:
            \begin{equation*}
                P_{N_1, N_2}[X=x] = \dfrac{1}{N_2 - N_1 + 1} \qquad x\in \{N_1, \ldots, N_2\}
            \end{equation*}
    \end{enumerate}

    \noindent
    En lo que sigue, supondremos que tenemos una m.a.s. $(X_1, \ldots, X_n)$ de variables idénticamente distribuidas a la respectiva variable $X$:
    \begin{enumerate}[label=\alph*)]
        \item Si $X\rightsquigarrow U(-\nicefrac{\theta}{2},\nicefrac{\theta}{2})$ con $\theta>0$:
            \begin{equation*}
                f(x_1, \ldots, x_n) \stackrel{\text{iid.}}{=}\prod_{i=1}^{n}f(x_i) = \prod_{i=1}^{n} \dfrac{1}{\theta} = \dfrac{1}{\theta^n} \qquad x_i \in \left]-\nicefrac{\theta}{2},\nicefrac{\theta}{2}\right[, \quad \forall i \in \{1,\ldots,n\}
            \end{equation*}
            Por lo que tendremos $-\nicefrac{\theta}{2}<X_{(1)}\leq X_{(n)} < \nicefrac{\theta}{2}$:
            \begin{equation*}
                f(x_1, \ldots, x_n) = \dfrac{1}{\theta^n} I_{\left]0,+\infty\right[}(X_{(1)}+\nicefrac{\theta}{2}) I_{\left]-\infty,0\right[}(X_{(n)}-\nicefrac{\theta}{n})
            \end{equation*}
            Si tomamos:
            \begin{gather*}
                h(x_1,\ldots,x_n)=1,\qquad T(X_1, \ldots, X_n) = (X_{(1)}, X_{(n)}) \\ g_{\theta}(t_1,t_2) = \dfrac{1}{\theta^n} I_{\left]0,+\infty\right[}(t_1 + \nicefrac{\theta}{2}) I_{\left]-\infty,0\right[}(t_2-\nicefrac{\theta}{2})
            \end{gather*}
            Por el Teorema de factorización, tenemos que $T(X_1, \ldots, X_n)$ es un estadístico suficiente para $\theta$.
        \item Si $X\rightsquigarrow\Gamma(p,a)$ con $p,a>0$:
            \begin{align*}
                f(x_1, \ldots, x_n) &\stackrel{\text{iid.}}{=} \prod_{i=1}^{n}f(x_i) = \prod_{i=1}^{n} \dfrac{a^p}{\Gamma(p)} x_i^{p-1} e^{-ax_i} = {\left(\dfrac{a^p}{\Gamma(p)}\right)}^{n} e^{-a\sum\limits_{i=1}^{n}x_i} \prod_{i=1}^{n}x_i^{p-1} \\ 
                                    &= {\left(\dfrac{a^p}{\Gamma(p)}\right)}^{n} e^{-a\sum\limits_{i=1}^{n}x_i} {\left(\prod_{i=1}^{n}x_i\right)}^{p-1} \qquad x_i \geq 0 \qquad \forall i \in \{1,\ldots,n\}
            \end{align*}
            \begin{itemize}
                \item Suponiendo que $p$ es conocida, podemos tomar:
                    \begin{gather*}
                        h(x_1, \ldots, x_n) = {\left(\prod_{i=1}^{n}x_i\right)}^{p-1}, \qquad  T(X_1, \ldots, X_n) = \sum_{i=1}^{n} X_i \\
                        g_a(t) = {\left(\dfrac{a^p}{\Gamma(p)}\right)}^{n}e^{-at}
                    \end{gather*}
                \item Suponiendo ahora que $a$ es conocida:
                    \begin{gather*}
                        h(x_1, \ldots, x_n) = e^{-a\sum\limits_{i=1}^n x_i}, \qquad T(X_1, \ldots, X_n) = \prod_{i=1}^{n}x_i \\
                        g_p(t) = {\left(\dfrac{a^p}{\Gamma(p)}\right)}^{n} t^{p-1}
                    \end{gather*}
                \item Si ahora tanto $p$ como $a$ son desconocidas, podemos tomar:
                    \begin{gather*}
                        h(x_1, \ldots, x_n) = 1, \qquad T(X_1, \ldots, X_n) = \left(\sum_{i=1}^{n}X_i, \prod_{i=1}^{n} X_i\right) \\
                        g_{(a,p)}(t_1,t_2) = {\left(\dfrac{a^p}{\Gamma(p)}\right)}^{n}e^{-at_1}t_2^{p-1}
                    \end{gather*}
            \end{itemize}
        \item Si $X\rightsquigarrow\beta(p,q)$ con $p,q>0$:
            \begin{align*}
                f(x_1, \ldots, x_n) &\stackrel{\text{iid.}}{=} \prod_{i=1}^{n}f(x_i) = \prod_{i=1}^{n} \dfrac{1}{\beta(p,q)} x_i^{p-1} {(1-x_i)}^{q-1} \\
                &= \dfrac{1}{{\beta(p,q)}^{n}} {\left(\prod_{i=1}^{n}x_i\right)}^{p-1} {\left(\prod_{i=1}^{n}(1-x_i)\right)}^{q-1} \quad x_i \in [0,1], \forall i \in \{1,\ldots,n\}
            \end{align*}
            \begin{itemize}
                \item Si $p$ es conocida, tomamos:
                    \begin{gather*}
                        h(x_1, \ldots, x_n) = {\left(\prod_{i=1}^{n}x_i\right)}^{p-1}, \qquad T(X_1, \ldots, X_n) = \prod_{i=1}^{n}(1-X_i) \\
                        g_q(t) = \dfrac{1}{{\beta(p,q)}^{n}} t^{q-1}
                    \end{gather*}
                \item Si $q$ es conocida:
                    \begin{gather*}
                        h(x_1, \ldots, x_n) = {\left(\prod_{i=1}^{n}(1-x_i)\right)}^{q-1}, \qquad T(X_1, \ldots, X_n) = \prod_{i=1}^{n}X_i \\
                        g_p(t) = \dfrac{1}{{\beta(p,q)}^{n}} t^{p-1}
                    \end{gather*}
                \item Si tanto $p$ como $q$ son parámetros:
                    \begin{gather*}
                        h(x_1, \ldots, x_n) = 1, \qquad T(X_1, \ldots, X_n) = \left(\prod_{i=1}^{n}X_i, \prod_{i=1}^{n}(1-X_i)\right) \\
                        g_{(p,q)}(t_1, t_2) = \dfrac{1}{{\beta(p,q)}^{n}}t_1^{p-1}t_2^{q-1}
                    \end{gather*}
            \end{itemize}
        \item Si $X\rightsquigarrow P_{N_1,N_2}$ con $N_1,N_2\in \mathbb{N}$, $N_1\leq N_2$, entonces:
            \begin{align*}
                P[X_1=x_1, \ldots, X_n=x_n]&\stackrel{\text{iid.}}{=} \prod_{i=1}^{n} P[X=x_i] =\prod_{i=1}^{n} \dfrac{1}{N_2-N_1+1} \\
                                           &= \dfrac{1}{{(N_2-N_1+1)}^{n}}, \quad x_i \in \{N_1, \ldots N_2\} 
            \end{align*}
            Como se tiene $N_1\leq X_{(1)}\leq X_{(n)}\leq N_2$, entonces:
            \begin{equation*}
                P[X_1=x_1, \ldots, X_n=x_n] = \dfrac{I_{-\bb{N}_0}(X_{(1)}-N_1)I_{\bb{N}_0}(X_{(n)}-N_2)}{{(N_2-N_1+1)}^{n}}
            \end{equation*}
            \begin{itemize}
                \item Si $N_1$ es conocido:
                    \begin{gather*}
                        h(x_1, \ldots, x_n) = I_{-\bb{N}_0}(X_{(1)}-N_1), \qquad T(X_1, \ldots, X_n) = X_{(n)} \\
                        g_{N_2}(t) = \dfrac{I_{\bb{N}_0}(t-N_2)}{{(N_2-N_1+1)}^{n}}
                    \end{gather*}
                \item Si $N_2$ es conocida:
                    \begin{gather*}
                        h(x_1, \ldots, x_n) = I_{\bb{N}_0}(X_{(n)}-N_2), \qquad T(X_1, \ldots, X_n) = X_{(1)} \\
                        g_{N_1}(t) = \dfrac{I_{-\bb{N}_0}(t-N_1)}{{(N_2-N_1+1)}^{n}}
                    \end{gather*}
                \item Si tanto $N_1$ como $N_2$ son parámetros:
                    \begin{gather*}
                        h(x_1,\ldots,x_n) = 1, \qquad T(X_1, \ldots, X_n) = \left(X_{(1)}, X_{(n)}\right) \\
                        g_{(N_1,N_2)}(t_1,t_2) = \dfrac{I_{-\bb{N}_0}(t_1-N_1)I_{\bb{N}_0}(t_2-N_2)}{{(N_2-N_1+1)}^{n}}
                    \end{gather*}
            \end{itemize}
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio} % // TODO: TERMINAR
    Sea $X\rightsquigarrow\{P_N : N\in \mathbb{N}\}$, siendo $P_N$ la distribución uniforme en los puntos $\{1,\ldots,N\}$, y sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de $X$. Probar que $\max(X_1, \ldots, X_n)$ es un estadístico suficiente y completo.\\

    \noindent
    Buscamos aplicar el Teorema de factorización de Neymann-Fisher:
    \begin{equation*}
        P[X_1 = x_1, \ldots, X_n = x_n] \stackrel{\text{iid.}}{=} \prod_{i=1}^{n}P[X=x_i] =  \prod_{i=1}^{n} \dfrac{1}{N}, \quad x_i \in \{1,\ldots, N\}, \forall i \in \{1,\ldots,n\}
    \end{equation*}
    Por lo que $1\leq X_{(1)} \leq X_{(n)} \leq N$:
    \begin{equation*}
        P[X_1 = x_1, \ldots, X_n=x_n] = \dfrac{I_{-\bb{N}_0}(X_{(1)}-1)I_{\bb{N}_0}(X_{(n)}-N)}{N^n}
    \end{equation*}
    De donde podemos tomar:
    \begin{gather*}
        h(x_1, \ldots, x_n) = I_{-\bb{N}_0}(X_{(1)}-1), \qquad T(X_1, \ldots, X_n) = X_{(n)} \\ 
        g_N(t) = \dfrac{I_{\bb{N}_0}(t-N)}{N^n}
    \end{gather*}
    Por el Teorema de factorización de Neymann-Fisher, tenemos que el estadístico $T(X_1, \ldots, X_n) = X_{(n)}$ es suficiente. Comprobamos que también es completo: sea $g$ cualquier función medible, supongamos que (abreviaremos $T(X_1, \ldots, X_n) = T$):

    \begin{equation*}
        0 = E[g(T)] = \sum_{t=1}^{N}g(t)P[T=t] \qquad \forall N\in \mathbb{N}
    \end{equation*}
    Calculamos la función masa de probabilidad de $T$:
    \begin{equation*}
        F_T(t) = {(F_X(t))}^{n} \Longrightarrow P[T=t] = P[T\leq t] - P[T\leq t-1] = {(F_X(t))}^{n} - {(F_X(t-1))}^{n}
    \end{equation*}
    Como $F_X(t) = \frac{t}{N}$, tenemos entonces que:
    \begin{equation*}
        P[T=t] = {(F_X(t))}^{n} - {(F_X(t-1))}^{n} = \dfrac{t^n}{N^n} - \dfrac{{(t-1)}^{n}}{N^n} = \dfrac{t^n - {(t-1)}^{n}}{N^n}
    \end{equation*}
    Por lo que:
    \begin{equation*}
        0 = E[g(T)] = \sum_{t=1}^{N} g(t) \dfrac{t^n -{(t-1)}^{n}}{N^n} = \dfrac{1}{N^n} \sum_{t=1}^{N} g(t) (t^n - {(t-1)}^{n}) \qquad \forall N\in \mathbb{N}
    \end{equation*}
    % // TODO: TERMINAR
\end{ejercicio}

\begin{ejercicio}    % // TODO: TERminar
    Basándose en una muestra de tamaño arbitrario, obtener un estadístico suficiente y completo para la familia de distribuciones definidas por todas las densidades de la forma
    \begin{equation*}
        f_\theta(x) = e^{\theta-x}, \qquad x>\theta
    \end{equation*}

    \noindent
    Sea $(X_1, \ldots, X_n)$ una m.a.s. de tamaño $n\in \mathbb{N}$: 
    \begin{equation*}
        f_{\theta}(x_1, \ldots, x_n)  \stackrel{\text{iid.}}{=} \prod_{i=1}^{n}f_\theta(x_i) = \prod_{i=1}^{n}e^{\theta - x_i} = e^{n\theta - \sum\limits_{i=1}^{n}x_i} = \dfrac{e^{n\theta}}{e^{\sum\limits_{i=1}^n x_i}}, \quad x_i > \theta\ \forall i \in \{1,\ldots,n\}
    \end{equation*}
    Por lo que ha de ser $\theta < X_{(1)}$:
    \begin{equation*}
        f_{\theta}(x_1, \ldots, x_n) = \dfrac{e^{n\theta}\cdot I_{\left]-\infty,0\right[}(X_{(1)}-\theta)}{e^{\sum\limits_{i=1}^n x_i}}
    \end{equation*}
    Si tomamos:
    \begin{gather*}
        h(x_1, \ldots, x_n) = e^{-\sum\limits_{i=1}^n x_i}, \qquad T(X_1, \ldots, X_n) = X_{(1)} \\
        g_\theta(t) = e^{n\theta}\cdot I_{\left]-\infty,0\right[}(t-\theta)
    \end{gather*}
    Por el Teorema de factorización de Neymann-Fisher tenemos que el estadístico $X_{(1)}$ es suficiente para $\theta$. Comprobemos si es completo: sea $g$ cualquier función medible, suponemos que (y escribimos $T=T(X_1, \ldots, X_n)$ para abreviar):
    \begin{equation*}
        0 = E[g(T)] = \int_{\theta}^{+\infty} g(t)f_T(t) ~dt  \qquad \forall \theta \in \mathbb{R}
    \end{equation*}
    Como $T=X_{(1)}$, tenemos que:
    \begin{equation*}
        F_T(t) = 1-{(1-F_\theta(t))}^{n} \Longrightarrow f_T(t) = n{(1-F_\theta(t))}^{n-1}f_\theta(t)
    \end{equation*}

    donde:
    \begin{equation*}
        F_\theta(t) = \int_{\theta}^{t} e^{\theta-x} ~dx  = \left[-e^{\theta-x}\right]_{\theta}^t = 1-e^{\theta-t}, \qquad t>\theta
    \end{equation*}

    por lo que:
    \begin{equation*}
        f_T(t) = n{\left(e^{\theta-t}\right)}^{n-1}e^{\theta-t} = n{\left(e^{\theta-t}\right)}^{n}
    \end{equation*}

    volviendo al caso que nos interesa:
    \begin{equation*}
        0 = E[g(T)] = \int_{\theta}^{+\infty} g(t)n{\left(e^{\theta-t}\right)}^{n} ~dt  = ne^{n\theta} \int_{\theta}^{+\infty} g(t)e^{-nt} ~dt  \qquad \forall \theta \in \mathbb{R}
    \end{equation*}
    % // TODO: TERminar
\end{ejercicio}

\begin{ejercicio}
    Comprobar que las siguientes familias de distribuciones son exponenciales uniparamétricas y, considerando una muestra aleatoria simple de una variable con distribución en dicha familia, obtener, si existe, un estadístico suficiente y completo.
    \begin{enumerate}[label=\alph*)]
        \item $\{B(k_0,p) : 0<p<1\}$

            Comprobamos todas las condiciones:
            \begin{enumerate}[label=\arabic*.]
                \item El espacio paramétrico es: $\left]0,1\right[\subseteq \mathbb{R}$.
                \item El espacio muestral es $\cc{X} = \{0,\ldots,k_0\}$, que no depende de $p$.
                \item Para la tercera condición:
                    \begin{align*}
                        P[X=x]_p &= \binom{k_0}{p} p^{x}{(1-p)}^{k_0-x} = exp\left[\ln\left(\binom{k_0}{p}p^{x}{(1-p)}^{k_0-x} \right)\right]\\
                               &= exp\left[\ln\binom{k_0}{p} + x\ln p + (k_0-x)\ln(1-p)\right] \\
                               &= exp\left[\ln\binom{k_0}{p} + x\ln p + k_0 \ln (1-p) - x\ln(1-p)\right] \\
                               &= exp\left[\ln\binom{k_0}{p} + k_0 \ln (1-p) + x\ln\left(\frac{p}{1-p}\right)\right] 
                    \end{align*}
                    Tomando:
                    \begin{align*}
                        T(x) = x, &\qquad S(x) = 0 \\
                        Q(p) = \ln\left(\dfrac{p}{1-p}\right), &\qquad D(p) = \ln\binom{k_0}{p} + k_0\ln(1-p)
                    \end{align*}
                    obtenemos la tercera condición. 
            \end{enumerate}
            Sea $(X_1, \ldots, X_n)$ una m.a.s. de variables aleatorias idénticamente distribuidas a $X\rightsquigarrow B(k_0,p)$ con $p\in \left]0,1\right[$, el Teorema visto en teoría para las familias de distribuciones exponenciales nos dice que el estadístico:
            \begin{equation*}
                T = T(X_1, \ldots, X_n) = \sum_{i=1}^{n}T(X_i) = \sum_{i=1}^{n}X_i
            \end{equation*}
            es suficiente para $p$. Para ver que $T$ es también completo, hemos de ver que $Im Q$ contiene un abierto de $\mathbb{R}$:
            \begin{description}
                \item [Opción 1.] Como $Q:\left]0,1\right[\to \mathbb{R}$ es continua, no constante y definida sobre un intervalo, por el Teorema del Valor Intermedio su imagen ha de ser un intervalo, que es un abierto de $\mathbb{R}$, por lo que $T$ es completo.
                \item [Opción 2.] 
                \begin{align*}
                    Im Q = \left\{\ln\left(\frac{p}{1-p}\right) : p \in \left]0,1\right[\right\}
                \end{align*}
                Si definimos la función $f:\left]0,1\right[\to \mathbb{R}^+$, tenemos que $f$ es sobreyectiva (de hecho es biyectiva):
                \begin{itemize}
                    \item Está bien definida, puesto que si $x\in \left]0,1\right[$, entonces $1-x>0$, con lo que $f(x)\in \mathbb{R}^+$.
                    \item Sea $y\in \mathbb{R}^+$, tenemos que:
                        \begin{equation*}
                            \dfrac{t}{1-t} = y \Longleftrightarrow t = y(1-t) \Longleftrightarrow t=y-yt \Longleftrightarrow t(1+y) = y \Longleftrightarrow t = \dfrac{y}{y+1}<1
                        \end{equation*}
                        Por lo que $f(t) = y$ con $t\in \left]0,1\right[$, con lo que $f$ es sobreyectiva.
                \end{itemize}
                Por tanto, $Im f = \mathbb{R}^+$, de donde deducimos que:
                \begin{equation*}
                    Im Q = \left\{\ln\left(\frac{p}{1-p}\right) : p \in \left]0,1\right[\right\} = \{\ln(f(p)) : p\in \left]0,1\right[\} = \ln(Im f) = \ln(\mathbb{R}^+) = \mathbb{R}
                \end{equation*}
                Como obviamente $\mathbb{R}$ contiene algún abierto de $\mathbb{R}$, deducimos que $T$ era un estadístico completo.
            \end{description}
        \item $\{\cc{P}(\lm) : \lm>0\}$

            Comprobamos las condiciones:
            \begin{enumerate}[label=\arabic*.]
                \item El espacio paramétrico es $\mathbb{R}^+ \subseteq \mathbb{R}$.
                \item El espacio muestral es $\cc{X} = \mathbb{N}\cup \{0\}$, que no depende de $\lm$.
                \item Para la tercera condición:
                    \begin{align*}
                        P_\lm[X=x] &= e^{-\lm} \dfrac{\lm^x}{x!} = exp\left[\ln\left(e^{-\lm} \dfrac{\lm^x}{x!}\right)\right] = exp[-\lm + x\ln \lm - \ln(x!)]
                    \end{align*}
                    Tomando:
                    \begin{align*}
                        T(x) = x, &\qquad S(x) = -\ln(x!) \\
                        Q(\lm) = \ln\lm, &\qquad D(\lm) = -\lm
                    \end{align*}
                    obtenemos la tercera condición.
            \end{enumerate}
            Sea $(X_1, \ldots, X_n)$ una m.a.s. de variables aleatorias idénticamente distribuidas a $X\rightsquigarrow \cc{P}(\lm)$ con $\lm\in \mathbb{R}^+$, el Teorema visto en teoría para las familias de distribuciones exponenciales nos dice que el estadístico:
            \begin{equation*}
                T = T(X_1, \ldots, X_n) = \sum_{i=1}^{n}T(X_i) = \sum_{i=1}^{n}X_i
            \end{equation*}
            es suficiente para $\lm$. Para ver que $T$ es también completo, hemos de ver que $Im Q$ contiene un abierto de $\mathbb{R}$. Como:
            \begin{equation*}
                Im Q = \{\ln(\lm) : \lm \in \mathbb{R}^+\} = \ln(\mathbb{R}^+) = \mathbb{R}
            \end{equation*}
            Tenemos que $Im Q = \mathbb{R}$ claramente contiene un abierto de $\mathbb{R}$, por lo que $T$ es completo.
        \item $\{BN(k_0,p) : 0<p<1\}$

            Comprobamos las condiciones:
            \begin{enumerate}[label=\arabic*.]
                \item El espacio paramétrico es $\left]0,1\right[\subseteq \mathbb{R}$.
                \item El espacio muestral es $\cc{X} = \mathbb{N}\cup \{0\}$, que no depende de $p$.
                \item Para la tercera condición:
                    \begin{align*}
                        P_p[X=x] &= \binom{x+k_0-1}{x} {(1-p)}^{x}p^{k_0 }= exp\left[\ln\left(\binom{x+k_0-1}{x} {(1-p)}^{x}p^{k_0} \right)\right] \\
                                 &= exp\left[\ln\binom{x+k_0-1}{x} + x\ln(1-p) + k_0\ln p\right]
                    \end{align*}
                    Tomando:
                    \begin{align*}
                        T(x) = x, &\qquad S(x) = \ln\binom{x+k_0-1}{x} \\
                        Q(p) = \ln(1-p), &\qquad D(p) = k_0\ln p
                    \end{align*}
                    Obtenemos la tercera condición.
            \end{enumerate}
            Sea $(X_1, \ldots, X_n)$ una m.a.s. de variables aleatorias idénticamente distribuidas a $X\rightsquigarrow BN(k_0,p)$ con $p\in \left]0,1\right[$, el Teorema visto en teoría para las familias de distribuciones exponenciales nos dice que el estadístico:
            \begin{equation*}
                T = T(X_1, \ldots, X_n) = \sum_{i=1}^{n}T(X_i) = \sum_{i=1}^{n}X_i
            \end{equation*}
            es suficiente para $p$. Para ver que $T$ es también completo, hemos de ver que $Im Q$ contiene un abierto de $\mathbb{R}$. Como $Q:\left]0,1\right[\to\mathbb{R}$ es una función continua, no constante y definida en un intervalo, tenemos que su imagen es un intervalo, por lo que contiene abiertos de $\mathbb{R}$, de donde $T$ es completo.
        \item $\{exp(\lm) : \lm>0\}$

            Comprobamos las condiciones:
            \begin{enumerate}[label=\arabic*.]
                \item El espacio paramétrico es $\mathbb{R}^+ \subseteq \mathbb{R}$.
                \item El espacio muestral es $\cc{X} = \mathbb{R}^+$, que no depende de $\lm$.
                \item Para la tercera condición:
                    \begin{equation*}
                        f_\lm(x) = \lm e^{-\lm x} = exp\left[\ln\left(\lm e^{-\lm x} \right)\right] = exp\left[\ln(\lm) -\lm x\right]
                    \end{equation*}
                    Tomando:
                    \begin{align*}
                        T(x) = x, &\qquad S(x) = 0 \\
                        Q(\lm) = -\lm, &\qquad D(\lm) = \ln(\lm)
                    \end{align*}
                    tenemos la tercera condición.
            \end{enumerate}
            Sea $(X_1, \ldots, X_n)$ una m.a.s. de variables aleatorias idénticamente distribuidas a $X\rightsquigarrow exp(\lm)$ con $\lm \in \mathbb{R}^+$, el Teorema visto en teoría para las familias de distribuciones exponenciales nos dice que el estadístico:
            \begin{equation*}
                T = T(X_1, \ldots, X_n) = \sum_{i=1}^{n}T(X_i) = \sum_{i=1}^{n}X_i
            \end{equation*}
            es suficiente para $\lm$. Para ver que $T$ es también completo, hemos de ver que $Im Q$ contiene un abierto de $\mathbb{R}$. Como $Q:\mathbb{R}^+\to\mathbb{R}$ es una función continua, no constante y definida en un intervalo, tenemos que su imagen es un intervalo, por lo que contiene abiertos de $\mathbb{R}$, de donde $T$ es completo.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Estudiar si las siguientes familias de distribuciones son exponenciales biparamétricas.  En caso afirmativo, considerando una muestra aleatoria simple de una variable con distribución en dicha familia, obtener, si existe, un estadístico suficiente y completo.
    \begin{enumerate}[label=\alph*)]
        \item $\{\Gamma(p,a) : p,a>0\}$

            Comprobamos las condiciones:
            \begin{enumerate}[label=\arabic*.]
                \item El espacio paramétrico es $\mathbb{R}^+\times \mathbb{R}^+\subseteq {(\mathbb{R}^+)}^{2}$.
                \item El espacio muestral es $\mathbb{R}^+$, que no depende de $p$ ni de $a$.
                \item Para la tercera condición:
                    \begin{align*}
                        f_{(p,a)}(x) &= \dfrac{a^p}{\Gamma(p)} x^{p-1}e^{-ax} = exp\left[\ln\left(\dfrac{a^p}{\Gamma(p)} x^{p-1}e^{-ax} \right)\right] \\ &= exp\left[\ln\left(\dfrac{a^p}{\Gamma(p)}\right) + (p-1)\ln x - ax\right]
                    \end{align*}
                Tomando:
                \begin{align*}
                    T_1(x) = \ln x, &\qquad  T_2(x) = x, \qquad \qquad   S(x) = 0 \\
                    Q_1(p,a) = (p-1), &\qquad Q_2(p,a) = -a, \qquad D(p,a) = \ln\left(\dfrac{a^p}{\Gamma(p)}\right) 
                \end{align*}
                Tenemos la tercera condición.
            \end{enumerate}
            Sea $(X_1, \ldots, X_n)$ una m.a.s. de variables aleatorias idénticamente distribuidas a $X\rightsquigarrow \Gamma(p,a)$ con $p,a\in \mathbb{R}^+$, el Teorema visto en teoría para las familias de distribuciones exponenciales multiparamétricas nos dice que el estadístico:
            \begin{equation*}
                T = T(X_1, \ldots, X_n) = \left(\sum_{i=1}^{n}T_1(X_i), \sum_{i=1}^{n}T_2(X_i)\right) = \left(\sum_{i=1}^{n}\ln(X_i), \sum_{i=1}^{n}X_i\right)
            \end{equation*}
            es suficiente para $(p,a)$. Para ver que $T$ es también completo, hemos de ver que $Im Q$ contiene un abierto de $\mathbb{R}^2$, donde $Q:{(\mathbb{R}^+)}^{2}\to \mathbb{R}^2$, con $Q=(Q_1,Q_2)$. Para ello:
            \begin{align*}
                Im Q = \left\{(p-1,-a) : (p,a)\in {(\mathbb{R}^+)}^{2}\right\} &= \{(x-1,y) : x\in \mathbb{R}^+, y\in \mathbb{R}^-\} \\ &= \left]-1,+\infty\right[\times \mathbb{R}^-
            \end{align*}
            Como claramente $\left]-1,+\infty\right[\times\mathbb{R}^-$ contiene un abierto de $\mathbb{R}^2$, tenemos que $T$ es completo.
        \item $\{\beta(p,q) : p,q>0\}$

            Comprobamos las condiciones:
            \begin{enumerate}[label=\arabic*.]
                \item El espacio paramétrico es $\mathbb{R}^+\times \mathbb{R}^+\subseteq {(\mathbb{R}^+)}^{2}$.
                \item El espacio muestral es $[0,1]$, que no depende de $p$ ni de $q$.
                \item Para la tercera condición:
                    \begin{align*}
                        f_{(p,q)}(x) &= \dfrac{1}{\beta(p,q)}x^{p-1}{(1-x)}^{q-1} = exp\left[\ln\left(\dfrac{1}{\beta(p,q)}x^{p-1}{(1-x)}^{q-1} \right)\right] \\
                                     &= exp\left[\ln\left(\dfrac{1}{\beta(p,q)}\right) + (p-1)\ln x + (q-1)\ln(1-x)\right]
                    \end{align*}
                Tomando:
                \begin{align*}
                    T_1(x) = \ln x, &\qquad T_2(x) = \ln(1-x), \qquad S(x) = 0 \\
                    Q_1(p,q) = p-1, &\qquad Q_2(p,q) = q-1, \qquad D(p,q) = \ln\left(\dfrac{1}{\beta(p,q)}\right) 
                \end{align*}
                Tenemos la tercera condición.
            \end{enumerate}
            Sea $(X_1, \ldots, X_n)$ una m.a.s. de variables aleatorias idénticamente distribuidas a $X\rightsquigarrow \beta(p,q)$ con $p,q\in \mathbb{R}^+$, el Teorema visto en teoría para las familias de distribuciones exponenciales multiparamétricas nos dice que el estadístico:
            \begin{equation*}
                T = T(X_1, \ldots, X_n) = \left(\sum_{i=1}^{n}T_1(X_i), \sum_{i=1}^{n}T_2(X_i)\right) = \left(\sum_{i=1}^{n}\ln(X_i), \sum_{i=1}^{n}\ln(1-X_i)\right)
            \end{equation*}
            es suficiente para $(p,q)$. Para ver que $T$ es también completo, hemos de ver que $Im Q$ contiene un abierto de $\mathbb{R}^2$, donde $Q:{(\mathbb{R}^+)}^{2}\to \mathbb{R}^2$, con $Q=(Q_1,Q_2)$. Para ello:
            \begin{equation*}
                Im Q = \{(p-1,q-1):p,q\in \mathbb{R}^+\} = \left]-1,+\infty\right[\times \left]-1,+\infty\right[
            \end{equation*}
            Como claramente este conjunto contiene un abierto de $\mathbb{R}^2$, tenemos que $T$ es completo.
    \end{enumerate}
\end{ejercicio}
