\newpage
\section{Suficiencia y completitud}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable $X\rightsquigarrow\{B(k,p) : p\in \left]0,1\right[\}$ y sea $T(X_1, \ldots, X_n) = \sum\limits_{i=1}^n X_i$. Probar
    \begin{enumerate}[label=\alph*)]
        \item usando la definición
        \item aplicando el teorema de factorización
    \end{enumerate}
    que $T$ es suficiente para $p$.

    \begin{enumerate}[label=\alph*)]
        \item Si notamos para abreviar $T=T(X_1, \ldots, X_n)$, tenemos que probar que la distribución de la muestra condicionada a cualquier valor del estadístico no depende del parámetro $p$ para probar que $T$ es suficiente para $p$. Para ello:
            \begin{align*}
                P_p[X_1 = x_1, \ldots, X_n = x_n \ &|\ T=t] = \dfrac{P_p[X_1=x_1, \ldots, X_n = x_n, T=t]}{P_p[T=t]} \\
                                                           &= \left\{\begin{array}{ll}
                                                               0 & \text{si\ } T(x_1,\ldots,x_n)\neq t \\
                                                               \dfrac{P_p[X_1=x_1, \ldots, X_n=x_n]}{P_p[T=t]}& \text{si\ } T(x_1,\ldots,x_n) = t
                                                           \end{array}\right. 
            \end{align*}
            Como $0$ obviamente no depende de $p$, el caso $T(x_1,\ldots,x_2)\neq t$ se encuentra ya estudiado, por lo que nos centramos en el caso $T(x_1,\ldots,x_2)=t$:
            \begin{align*}
                \dfrac{P_p[X_1=x_1, \ldots, X_n = x_n]}{P_p[T=t]} &\stackrel{\text{iid.}}{=} \dfrac{\prod\limits_{i=1}^{n}P_p[X=x_i]}{P_p[T=t]} \AstIg \dfrac{\prod\limits_{i=1}^n \binom{k}{x_i} p^{x_i} {(1-p)}^{k-x_i}}{\binom{nk}{t}p^t{(1-p)}^{nk-t}} \\
                                                                  &= \dfrac{p^{\sum\limits_{i=1}^n x_i} {(1-p)}^{nk - \sum\limits_{i=1}^n x_i}\prod\limits_{i=1}^n \binom{k}{x_i}}{\binom{nk}{t}p^t{(1-p)}^{nk-t}} \stackrel{(\ast\ast)}{=} \dfrac{\prod\limits_{i=1}^{n}\binom{k}{x_i}}{\binom{nk}{t}}
            \end{align*}
            Donde en $(\ast)$ hemos usado que $(X_1, \ldots, X_n)$ es una m.a.s. (variables independientes) y la reproductividad de la Binomial, por lo que $T\rightsquigarrow B(nk,p)$; y en $(\ast\ast)$ hemos usado que $t=T(x_1, \ldots, x_n) = \sum\limits_{i=1}^{n}x_i$. En definitiva, hemos obtenido que la distribución de la muestra condicionada a cualquier valor del estadístico no dependende del parámetro $p$, por lo que $T$ es suficiente para $p$.
        \item Si podemos usar el Teorema de factorización, escribimos la función masa de probabilidad de la distribución conjunta de la muestra aleatoria simple:
            \begin{align*}
                P[X_1=x_1, \ldots, X_n = x_n] &\stackrel{\text{iid.}}{=} \prod_{i=1}^{n} P[X=x_i] = \prod_{i=1}^{n} \binom{k}{x_i} p^{x_i} {(1-p)}^{k-x_i} \\
                                              &= p^{\sum\limits_{i=1}^n x_i} {(1-p)}^{nk - \sum\limits_{i=1}^n x_i} \prod_{i=1}^{n} \binom{k}{x_i} 
            \end{align*}
            Si tomamos:
            \begin{equation*}
                h(x_1, \ldots, x_n) = \prod_{i=1}^{n}\binom{k}{x_i}, \qquad 
                T(X_1, \ldots, X_n) = \sum_{i=1}^{n} X_i, \qquad 
                g_p(t) = p^t {(1-p)}^{nk - t}
            \end{equation*}
            Podemos aplicar el Teorema de Factorización de Neymann-Fisher, obteniendo que $T$ es un estadístico suficiente para $p$.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable $X\rightsquigarrow\{\cc{P}(\lm) : \lm\in \mathbb{R}^+\}$ y sea $T(X_1, \ldots, X_n) = \sum\limits_{i=1}^n X_i$. Probar
    \begin{enumerate}[label=\alph*)]
        \item usando la definición
        \item aplicando el teorema de factorización
    \end{enumerate}
    que $T$ es suficiente para $\lm$.

    \begin{enumerate}[label=\alph*)]
        \item Notando $T=T(X_1, \ldots, X_n)$, seguimos los mismos pasos que en el ejercicio anterior:
            \begin{align*}
                P_\lm[X_1 = x_1, \ldots, X_n = x_n \ &|\ T=t] = \dfrac{P_\lm[X_1=x_1, \ldots, X_n = x_n, T=t]}{P_\lm[T=t]} \\
                                                           &= \left\{\begin{array}{ll}
                                                               0 & \text{si\ } T(x_1,\ldots,x_n)\neq t \\
                                                               \dfrac{P_\lm[X_1=x_1, \ldots, X_n=x_n]}{P_\lm[T=t]}& \text{si\ } T(x_1,\ldots,x_n) = t
                                                           \end{array}\right. 
            \end{align*}
            Y ahora nos interesamos por el segundo término, que es el que puede depender de $\lm$:
            \begin{align*}
                \dfrac{P_\lm[X_1 = x_1, \ldots, X_n = x_n]}{P_\lm[T=t]} &\stackrel{\text{iid.}}{=} \dfrac{\prod\limits_{i=1}^{n} P_\lm[X=x_i]}{P_\lm[T=t]} \AstIg \dfrac{\prod\limits_{i=1}^n e^{-\lm} \dfrac{\lm^{x_i}}{x_i!}}{e^{-n\lm} \dfrac{{(n\lm)}^{t}}{t!}} = \dfrac{e^{-n\lm} \cdot \dfrac{\lm^{\sum\limits_{i=1}^n x_i}}{\prod\limits_{i=1}^n x_i!}}{e^{-n\lm}\cdot  \dfrac{n^t \lm^t}{t!}} \\
                                                                        &\stackrel{(\ast\ast)}{=} \dfrac{t!}{n^t \prod\limits_{i=1}^{n}x_i!}
            \end{align*}
            Donde en $(\ast)$ usamos que $T\rightsquigarrow P(n\lm)$, por la reproductividad de la Poisson y en $(\ast\ast)$ usamos que $t = T(x_1, \ldots, x_n) = \sum_{i=1}^{n} x_i$. Obtenemos una cantidad que no depende de $\lm$, por lo que $T$ es suficiente para $\lm$.
        \item Si podemos aplicar el Teorema de factorización, escribimos:
            \begin{equation*}
                P_\lm[X_1=x_1, \ldots, X_n = x_n] \stackrel{\text{iid.}}{=} \prod_{i=1}^{n} P_\lm[X=x_i] = \prod_{i=1}^{n}e^{-\lm}\dfrac{\lm^{x_i}}{x_i!} = e^{-n\lm} \cdot \dfrac{\lm^{\sum\limits_{i=1}^{n} x_i}}{\prod\limits_{i=1}^{n}x_i!}
            \end{equation*}
            Si tomamos:
            \begin{equation*}
                h(x_1, \ldots, x_n) = \dfrac{1}{\prod\limits_{i=1}^{n}x_i!}, \qquad T(X_1, \ldots, X_n) = \sum_{i=1}^{n}X_i, \qquad g_\lm(t) = e^{-n\lm} \cdot \lm^t
            \end{equation*}
            Por el Teorema de Factorización de Neymann-Fisher obtenemos que $T$ es suficiente para $\lm$.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, X_2, X_3)$ una muestra aleatoria simple de una variable $X\rightsquigarrow\{B(1,p) : p \in \left]0,1\right[\}$. Probar que el estadístico $X_1 + 2X_2 + 3X_3$ no es suficiente.\\

    \noindent
    Consideramos el estadístico $T(X_1, X_2, X_3) = X_1 + 2X_2 + 3X_3$. El espacio muestral de $X$ es $\cc{X}=\{0,1\}$, por lo que el espacio muestral de $T$ es $\cc{T} = \{0, 1, 2, 3, 4, 5, 6\}$. Sabemos por un ejemplo visto en teoría que el ``truco'' para demostrar que $T(X_1, X_2,X_3)$ no es suficiente para $p$ es buscar un valor del espacio muestral $\cc{T}$ que provenga de varias combinaciones de estados del espacio muestral $\cc{X}^3$. Como $0,1,2$ solo provienen de una combinación del espacio muestral $\cc{X}^3$ (son $(0,0,0)$, $(1,0,0)$ y $(0,1,0)$ respectivamente), probamos buscar el contraejemplo con $t=3$, que proviene de considerar las observaciones de la muestra $(1,1,0)$ y $(0,0,1)$.

    Una vez explicado el procedimiento para buscar cuál es el valor de $t$ que funciona, procedemos a probar que la distribución de la muestra condicionada a dicho valor de $t$ depende del parámetro $p$. Para ello:
            \begin{align*}
                P_p[X_1 = x_1, X_2=x_2, X_3 = x_3 \ &|\ T=3] = \dfrac{P_p[X_1=x_1, X_2=x_2, X_3 = x_3, T=3]}{P_p[T=3]} \\
                                                           &= \left\{\begin{array}{ll}
                                                               0 & \text{si\ } T(x_1,x_2,x_2)\neq 3 \\
                                                               \dfrac{P_p[X_1=x_1, X_2=x_2, X_3=x_3]}{P_p[T=3]}& \text{si\ } T(x_1,x_2,x_3) = 3
                                                           \end{array}\right. 
            \end{align*}
            Vemos que el primer caso no puede depender nunca de $p$, por lo que buscamos probar que el segundo caso sí que depende de $p$. Para ello:
            \begin{align*}
                \dfrac{P_p[X_1=x_1, X_2=x_2, X_3=x_3]}{P_p[T=3]} &\stackrel{\text{iid.}}{=} \dfrac{P_p[X=x_1]P_p[X=x_2]P_p[X=x_3]}{P_p[T=3]} \\
                                                                 &\AstIg \dfrac{p^{x_1}{(1-p)}^{1-x_1} p^{x_2}{(1-p)}^{1-x_2}p^{x_3}{(1-p)}^{1-x_3}}{P_p[X_1=1,X_2=1,X_3=0]+P_p[X_1=0,X_2=0,X_3=1]} \\
                                                                 &= \dfrac{p^{x_1}{(1-p)}^{1-x_1} p^{x_2}{(1-p)}^{1-x_2}p^{x_3}{(1-p)}^{1-x_3}}{p\cdot p\cdot (1-p) + (1-p)(1-p)p} \\
            \end{align*}
            Donde en $(\ast)$ he usado la propiedad reproductiva de la Binomial, por lo que $T\rightsquigarrow B(3,p)$, así como que la condición $t=3$ provenía de los valores $(1,1,0)$ y $(0,0,1)$.
            Ahora, si tomamos $(x_1,x_2,x_3) = (1,1,0)$, tenemos que:
            \begin{align*}
                \dfrac{P_p[X_1=1, X_2=1, X_3=0]}{P_p[T=3]} &= \dfrac{p^{1}{(1-p)}^{0} p^{1}{(1-p)}^{0}p^{0}{(1-p)}^{1}}{p\cdot p\cdot (1-p) + (1-p)(1-p)p} \\
                                                                 &= \dfrac{pp(1-p)}{pp(1-p)+(1-p)(1-p)p} \\
                                                                 &= \dfrac{p^2(1-p)}{p(1-p)\cancelto{1}{(p+1-p)}} = p
            \end{align*}
            Que claramente depende de $p$, por lo que $T$ no es suficiente para $p$.
\end{ejercicio}

\begin{ejercicio}
    Aplicando el teorema de factorización, y basándose en una muestra de tamaño arbitrario, encontrar un estadístico suficiente para cada una de las siguientes familias de distribuciones (en las familias biparamétricas, suponer los casos de sólo un parámetro desconocido y de los dos desconocidos).
    \begin{enumerate}[label=\alph*)]
        \item $X\rightsquigarrow\{U(-\nicefrac{\theta}{2},\nicefrac{\theta}{2}) : \theta>0\}$
        \item $X\rightsquigarrow\{\Gamma(p,a) : p,a > 0\}$
        \item $X\rightsquigarrow\{\beta(p,q) : p,q>0\}$
        \item $X\rightsquigarrow\{P_{N_1,N_2} : N_1, N_2 \in \mathbb{N}, N_1 \leq N_2\}$ y la masa de probabilidad viene dada por:
            \begin{equation*}
                P_{N_1, N_2}[X=x] = \dfrac{1}{N_2 - N_1 + 1} \qquad x\in \{N_1, \ldots, N_2\}
            \end{equation*}
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $X\rightsquigarrow\{P_N : N\in \mathbb{N}\}$, siendo $P_N$ la distribución uniforme en los puntos $\{1,\ldots,N\}$, y sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de $X$. Probar que $\max(X_1, \ldots, X_n)$ es un estadístico suficiente y completo.
\end{ejercicio}

\begin{ejercicio}
    Basándose en una muestra de tamaño arbitrario, obtener un estadístico suficiente y completo para la familia de distribuciones definidas por todas las densidades de la forma
    \begin{equation*}
        f_\theta(x) = e^{\theta-x}, \qquad x>\theta
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}
    Comprobar que las siguientes familias de distribuciones son exponenciales uniparamétricas y, considerando una muestra aleatoria simple de una variable con distribución en dicha familia, obtener, si existe, un estadístico suficiente y completo.
    \begin{enumerate}[label=\alph*)]
        \item $\{B(k_0,p) : 0<p<1\}$
        \item $\{\cc{P}(\lm) : \lm>0\}$
        \item $\{BN(k_0,p) : 0<p<1\}$
        \item $\{exp(\lm) : \lm>0\}$
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Estudiar si las siguientes familias de distribuciones son exponenciales biparamétricas.  En caso afirmativo, considerando una muestra aleatoria simple de una variable con distribución en dicha familia, obtener, si existe, un estadístico suficiente y completo.
    \begin{enumerate}[label=\alph*)]
        \item $\{\Gamma(p,a) : p,a>0\}$
        \item $\{\beta(p,1) : p,q>0\}$
    \end{enumerate}
\end{ejercicio}
