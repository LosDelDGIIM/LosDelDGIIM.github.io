\newpage
\section{Distribuciones en el muestreo de poblaciones normales}

\begin{ejercicio}
    Se toma una muestra aleatoria simple de tamaño 5 de una variable aleatoria con distribución $\cc{N}(2.5,\ 36)$. Calcular:
    \begin{enumerate}[label=\alph*)]
        \item Probabilidad de que la cuasivarianza muestral esté comprendida entre $1.863$ y $2.674$.

            Tenemos una m.a.s. $(X_1, X_2, X_3, X_4, X_5)$, todas ellas idénticamente distribuidas a $X\rightsquigarrow \cc{N}(2.5,\ 36)$. Tomamos la cuasivarianza de dichos datos:
            \begin{equation*}
                S^2 = \dfrac{1}{n-1} \sum_{i=1}^n {(X_i - \mu)}^{2} = \dfrac{1}{4}\sum_{i=1}^{5}{(X_i - 2.5)}^{2}
            \end{equation*}
            Y queremos calcular la probabilidad:
            \begin{equation*}
                P\left[1.863 < S^2 < 2.674\right]
            \end{equation*}
            Para ello, en teoría hemos visto que como tenemos una población normal, se cumple que:
            \begin{equation*}
                \dfrac{(n-1)S^2}{\sigma^2}= \dfrac{4S^2}{36} \rightsquigarrow\chi^2(4) \equiv \chi^2(n-1)
            \end{equation*}
            Por tanto:
            \begin{align*}
                P[1.863 < S^2 < 2.674] &= P\left[\dfrac{4\cdot 1.863}{36}<\dfrac{4S^2}{36} < \dfrac{4\cdot 2.674}{36}\right] \\
                                       &= P\left[0.207 < \dfrac{4S^2}{36} < 0.2971\right]  \\
                                       &= P\left[\dfrac{4S^2}{36} > 0.207\right] - P\left[\dfrac{4S^2}{36}>0.2971\right]
            \end{align*}
            Si consultamos la tabla de la $\chi^2(4)$:
            \begin{equation*}
                P\left[\dfrac{4S^2}{36} > 0.207\right] = 0.995, \qquad 
                P\left[\dfrac{4S^2}{36}>0.2971\right] = 0.99
            \end{equation*}
            Por lo que:
            \begin{align*}
                P[1.863 < S^2 < 2.674] &= P\left[\dfrac{4S^2}{36} > 0.207\right] - P\left[\dfrac{4S^2}{36}>0.2971\right] \approx 0.995 - 0.99 \\
                                       &= 0.005
            \end{align*}
        \item Probabilidad de que la media muestral esté comprendida entre $1.3$ y $3.5$, supuesto que la cuasivarianza muestral está entre $30$ y $40$. 

            En la situación del apartado anterior, ahora tenemos también en consideración la media muestral:
            \begin{equation*}
                \overline{X} = \dfrac{1}{n}\sum_{i=1}^n {(X_i -\mu)}^{2} = \dfrac{1}{5}\sum_{i=1}^{5}{(X_i - 2.5)}^{2}
            \end{equation*}
            Bajo estas condiciones, el Lema de Fisher nos dice que $S^2$ y $\overline{X}$ son independientes, por lo que la probabilidad de que la media muestral esté comprendida entre $1.3$ y $3.5$ suponiendo que la cuasivarianza muestral está comprendida entre $30$ y $40$ es igual a la probabilidad de que la media muestral esté comprendida entre $1.3$ y $3.5$ por la probabilidad de que la cuasivarianza muestral esté comprendida entre $30$ y $40$:

            Sabemos que $\overline{X}\rightsquigarrow\cc{N}\left(\mu, \frac{\sigma^2}{n}\right)$, nos disponemos a calcular dicha probabilidad:
            \begin{align*}
                P[1.3<\overline{X}<3.5] &\stackrel{\text{tipificamos}}{=} P\left[\dfrac{\sqrt{5}(1.3-2.5)}{\sqrt{36}}< Z < \dfrac{\sqrt{5}(3.5-2.5)}{\sqrt{36}}\right] \\
                                        &= P[-0.447214 < Z < 0.372678] \\ 
                                        &= P[Z<0.372678] - P[Z>0.447214] \\
                                        &= P[Z<0.372678] -1 + P[Z<0.447214] \\
                                        &= 0.64431 - 1 + 0.67003 = 0.31434
            \end{align*}
            Para la cuasivarianza, sabemos que $\frac{4S^2}{36} = \frac{(n-1)S^2}{\sigma^2}\rightsquigarrow\chi^2(n-1) \equiv \chi^2(4)$, podemos calcular también esta probabilidad:
            \begin{align*}
                P[30<S^2<40] &= P\left[\dfrac{4\cdot 30}{36}<\dfrac{4S^2}{36}<\dfrac{4\cdot 40}{36}\right] = P\left[3.333 < \dfrac{4S^2}{36}<4.444\right] \\
                             &= P\left[\dfrac{4S^2}{36}>3.333\right] - P\left[\dfrac{4S^2}{36}>4.444\right] \\
                             &= 0.5 - 0.35 = 0.15
            \end{align*}
            Por tanto:
            \begin{align*}
                P[1.3 < \overline{X}<3.5 | 30<S^2<40] &\stackrel{\text{indep.}}{=} P[1.3<\overline{X}<3.5]\cdot P[30<S^2<40] \\ &= 0.31434 - 0.3 = 0.01434
            \end{align*}
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    La longitud craneal en una determinada población humana es una variable aleatoria que sigue una distribución normal con media $185.6$ mm. y desviación típica $12.78$ mm. ¿Cuál es la probabilidad de que una muestra aleatoria simple de tamaño 20 de esa población tenga media mayor que $190$ mm.?\\

    \noindent
    Tenemos una población que sigue una variable aleatoria $X\rightsquigarrow \cc{N}\left(\mu, \sigma^2\right)$ con parámetros $\mu = 185.6$ y $\sigma=12.78$. Tomamos una m.a.s. $(X_1, \ldots, X_n)$ con $n=20$ y tomamos la media muestral:
    \begin{equation*}
        \overline{X} = \dfrac{1}{n}\sum_{i=1}^{n}{(X_i - \mu)}^{2}
    \end{equation*}
    Queremos calcular:
    \begin{equation*}
        P\left[\overline{X}>190\right]
    \end{equation*}
    Para ello, sabemos por lo visto en teoría que $\overline{X}\rightsquigarrow\cc{N}\left(\mu, \frac{\sigma^2}{n}\right)$, por lo que:
    \begin{align*}
        P\left[\overline{X}>190\right] &\stackrel{\text{tipificamos}}{=} P\left[Z > \dfrac{\sqrt{20}(190-185.6)}{12.78}\right] = P[Z > 1.5397] \\ 
                                                                     &= 1-P[Z<1.5397] = 1-0.93822 = 0.06178
    \end{align*}
\end{ejercicio}

\begin{ejercicio}
    ¿De qué tamaño mínimo habría que seleccionar una muestra de una variable con distribución normal $\cc{N}(\mu, 4)$ para poder afirmar, con probabilidad mayor que $0.9$, que la media muestral diferirá de la poblacional menos de $0.1$?\\

    \noindent
    Supuesto que tenemos una m.a.s. $(X_1, \ldots, X_n)$ de tamaño $n\in \mathbb{N}$ de variables aleatorias idénticamente distribuidas a $X\rightsquigarrow\cc{N}(\mu, 4)$, queremos buscar el menor $n$ de forma que:
    \begin{equation*}
        0.9 \leq P\left[|\overline{X}-\mu| < 0.1\right]
    \end{equation*}
    Para ello, usaremos que hemos visto en teoría que $\overline{X}\rightsquigarrow\cc{N}\left(\mu, \frac{\sigma^2}{n}\right)$ con $\sigma=2$:
    \begin{align*}
        0.9 \leq P[|\overline{X}-\mu| < 0.1] &= P[-0.1 < |\overline{X}-\mu| < 0.1] = P\left[\dfrac{-\sqrt{n}0.1}{2} < Z < \dfrac{\sqrt{n}0.1}{2}\right] \\ 
                                             &= P\left[-\sqrt{n}0.05 < Z<\sqrt{n}0.05\right] = 2P\left[Z<\sqrt{n}0.05\right] - 1 
    \end{align*}
    De donde:
    \begin{equation*}
        0.95 = \dfrac{0.9+1}{2} \leq P[Z<\sqrt{n}0.05]
    \end{equation*}
    De donde mirando la tabla de la normal $\cc{N}(0,1)$:
    \begin{equation*}
        \sqrt{n}0.05 \geq 1.65 \Longrightarrow n \geq {\left(\dfrac{1.65}{0.05}\right)}^{2} = 1089
    \end{equation*}
    Por lo que habría que seleccionar como mínimo una muestra de tamaño $1089$ para poder afirmar con probabilidad mayor que $0.9$ que la media muestral difiere de la poblacional menos de $0.1$.
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable con distribución normal.  Calcular la probabilidad de que la cuasivarianza muestral sea menor que un $50\%$ de la varianza poblacional para $n = 16$ y para $n = 1000$.\\

    \noindent
    Dicha muestra aleatoria simple es de variables idénticamente distribuidas a $X\rightsquigarrow\cc{N}(\mu, \sigma^2)$. Bajo estas condiciones, sabemos por lo visto en teoría que:
    \begin{equation*}
        \dfrac{(n-1)S^2}{\sigma^2} \rightsquigarrow\chi^2(n-1)
    \end{equation*}
    Y queremos calcular:
    \begin{align*}
        P[S^2 < 0.5\sigma^2] = P\left[\dfrac{(n-1)S^2}{\sigma^2} < \dfrac{(n-1)0.5\cancel{\sigma^2}}{\cancel{\sigma^2}}\right] = 1-P\left[\dfrac{(n-1)S^2}{\sigma^2} > (n-1)0.5\right]
    \end{align*}
    \begin{itemize}
        \item Para $n=16$, mirando en la tabla de $\chi^2(15)$ tenemos que:
            \begin{equation*}
                P[S^2 < 0.5\sigma^2] = 1-P\left[\dfrac{(n-1)S^2}{\sigma^2} > 7.5\right] = 1-0.95 = 0.05
            \end{equation*}
        \item Para $n=1000$, no disponemos de tabla para $\chi^2(999)$, por lo que aproximaremos $\chi^2(999)$ por $\cc{N}(999, 2\cdot 999)\equiv \cc{N}(999, 1998)$, gracias al Teorema de Lévy. De esta forma, si tomamos:
            \begin{equation*}
                Y = \dfrac{(n-1)S^2}{\sigma^2}\rightsquigarrow\cc{N}(999, 1998)
            \end{equation*}
            Tenemos entonces que:
            \begin{equation*}
                P[999\cdot 0.5] = P[Y>499.5] = P\left[Z>\dfrac{499.5-999}{\sqrt{1998}}\right] = 1-P[Z< -11.17]
            \end{equation*}
            Por lo que:
            \begin{equation*}
                P[S^2 < 0.5\sigma^2] \approx P[Z< -11.17] \approx 0
            \end{equation*}
    \end{itemize}
\end{ejercicio}

\begin{ejercicio}
    Sean $S_1^2$ y $S_2^2$ las cuasivarianzas muestrales de dos muestras independientes de tamaños $n_1 = 5$ y $n_2 = 4$ de dos poblaciones normales con la misma varianza. Calcular la probabilidad de que $\nicefrac{S_1^2}{S_2^2}$ sea menor que $5.34$ o mayor que $9.12$.\\

    \noindent
    Bajo estas hipótesis, sabemos por lo visto en teoría que (como $\sigma_1=\sigma_2$):
    \begin{equation*}
        \dfrac{S_1^2}{S_2^2} \rightsquigarrow F(n_1-1, n_2-1) \equiv F(4,3)
    \end{equation*}
    Por tanto, notando $Y = \frac{S_1^2}{S_2^2}$ por comodidad, calculamos:
    \begin{equation*}
        P[Y < 5.34 \text{\ o\ } Y>9.12] = P[Y<5.34] + P[Y>9.12]
    \end{equation*}
    Observamos la tabla de $F(4,3)$:
    \begin{align*}
        P[Y<5.34] &= 0.9 \\
        P[Y>9.12] &= 1-P[Y<9.12] = 1-0.95 = 0.05
    \end{align*}
    En definitiva:
    \begin{equation*}
        P[Y < 5.34 \text{\ o\ } Y>9.12] = P[Y<5.34] + P[Y>9.12] = 0.95
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}
    Se consideran dos poblaciones de bombillas cuyas longitudes de vida siguen una ley normal con la misma media y desviaciones típicas $425$ y $375$ horas, respectivamente.  Con objeto de realizar un estudio comparativo de ambas poblaciones, se considera una muestra aleatoria simple de $10$ bombillas en la primera población y una de tamaño 6 en la segunda. ¿Cuál es la probabilidad de que la media muestral del primer grupo menos la del segundo sea menor que la observada en dos realizaciones muestrales que dieron $1325$ horas y $1215$ horas, respectivamente?\\

    \noindent
    Como describe el enunciado, tenemos una m.a.s. $(X_1, \ldots, X_n)$ de tamaño $n=10$ de variables aleatorias idénticamente distribuidas a $X\rightsquigarrow\cc{N}\left(\mu, {425}^{2}\right)$; y otra m.a.s. $(Y_1, \ldots, Y_m)$ de tamaño $m=6$ de variables aleatorias idénticamente distribuidas a $Y\rightsquigarrow\cc{N}\left(\mu, {375}^{2}\right)$. Notaremos $\mu_1 = 1325$, $\mu_2 = 1215$. Bajo estas condiciones, sabemos por lo visto en teoría que:
    \begin{equation*}
        \dfrac{\overline{X}-\overline{Y}}{\sqrt{\dfrac{\sigma_1^2}{n_1}+\dfrac{\sigma_2^2}{n_2}}} = \dfrac{\overline{X}-\overline{Y}-(\mu - \mu)}{\sqrt{\dfrac{\sigma_1^2}{n_1}+\dfrac{\sigma_2^2}{n_2}}}  \rightsquigarrow \cc{N}(0,1)
    \end{equation*}
    Luego si notamos a esta última variable como $Z$:
    \begin{align*}
        P[\overline{X}-\overline{Y}<1325-1215] &= P\left[Z < \dfrac{1325-1215}{\sqrt{\dfrac{\sigma_1^2}{n_1}+\dfrac{\sigma_2^2}{n_2}}}\right] = P\left[Z <  \dfrac{1325-1215}{\sqrt{\dfrac{{425}^{2}}{10}+\dfrac{{375}^{2}}{6}}}\right] \\
                                               &= P[Z<0.5399] = 0.7054
    \end{align*}
\end{ejercicio}

\begin{ejercicio}
    Sean $X_1, \ldots, X_n, X_{n+1}$ variables aleatorias independientes e idénticamente distribuidas según una $\cc{N}(\mu, \sigma^2)$, y sean $\overline{X}$ y $S^2$ la media y la cuasivarianza muestral de $(X_1, \ldots, X_n)$. Calcular la distribución de
    \begin{equation*}
        \dfrac{X_{n+1}- \overline{X}}{S}\sqrt{\dfrac{n}{n+1}}
    \end{equation*}

    \noindent
    Bajo dichas hipótesis, tenemos:
    \begin{equation*}
        \left.\begin{array}{r}
            X_{n+1} \rightsquigarrow \cc{N}(\mu, \sigma^2) \\
            \overline{X} \rightsquigarrow \cc{N}\left(\mu, \frac{\sigma^2}{n}\right)
    \end{array}\right\}\Longrightarrow X_{n+1}-\overline{X} \rightsquigarrow\cc{N}\left(\mu-\mu, \sigma^2 + \frac{\sigma^2}{n}\right) \equiv \cc{N}\left(0, \sigma^2\left(1+\frac{1}{n}\right)\right)
    \end{equation*}
    de donde si tipificamos la variable:
    \begin{equation*}
        \dfrac{X_{n+1}-\overline{X}}{\sqrt{\sigma^2\left(1+\frac{1}{n}\right)}} = \dfrac{X_{n+1}-\overline{X}}{\sigma\sqrt{\dfrac{n+1}{n}}} = \dfrac{X_{n+1}-\overline{X}}{\sigma}\sqrt{\dfrac{n}{n+1}}\rightsquigarrow \cc{N}(0,1)
    \end{equation*}
    Además, se ha visto en teoría que:
    \begin{equation*}
        \dfrac{(n-1)S^2}{\sigma^2} \rightsquigarrow\chi^2(n-1)
    \end{equation*}
    FALTA RAZONAR QUE AMBAS VARIABLES SON INDEPENDIENTES. Por lo que si realizamos la construcción de la distribución $t$ de Student:
    \begin{equation*}
        \left.\begin{array}{l}
                U\rightsquigarrow \cc{N}(0,1) \\
                \qquad \text{independientes} \\
                V\rightsquigarrow\chi^2(n)
        \end{array}\right\} \Longrightarrow \dfrac{U}{\sqrt{\nicefrac{V}{n}}} \rightsquigarrow t(n)
    \end{equation*}
    Si lo aplicamos a nuestras variables:
    \begin{equation*}
        \dfrac{\dfrac{X_{n+1}-\overline{X}}{\sigma}\sqrt{\dfrac{n}{n+1}}}{\sqrt{\dfrac{(n-1)S^2}{\sigma^2(n-1)}}} = \dfrac{\dfrac{X_{n+1}-\overline{X}}{\sigma}\sqrt{\dfrac{n}{n+1}}}{\dfrac{S}{\sigma}} = \dfrac{X_{n+1}- \overline{X}}{S}\sqrt{\dfrac{n}{n+1}} \rightsquigarrow t(n-1)
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}
    Sean $(X_1, \ldots, X_n)$, $(Y_1, \ldots, Y_m)$ muestras aleatorias simples independientes de poblaciones $\cc{N}(\mu_1,\sigma^2)$ y $\cc{N}(\mu_2, \sigma^2)$, respectivamente. Sean $\alpha,\beta\in \mathbb{R}$ y $\overline{X},\overline{Y},S_1^2, S_2^2$ las medias y cuasivarianzas de las dos muestras. Calcular la distribución de
    \begin{equation*}
        \dfrac{\alpha\left(\overline{X}-\mu_1\right)+\beta\left(\overline{Y}-\mu_2\right)}{\sqrt{\dfrac{(n-1)S_1^2 + (m-1)S_2^2}{n+m-2}} \sqrt{\dfrac{\alpha^2}{n} + \dfrac{\beta^2}{m}}}
    \end{equation*}

    \noindent
    Sabemos que:
    \begin{equation*}
        \left.\begin{array}{r}
                \overline{X} \rightsquigarrow\cc{N}\left(\mu_1, \frac{\sigma^2}{n}\right) \\
                \overline{Y} \rightsquigarrow\cc{N}\left(\mu_2, \frac{\sigma^2}{m}\right) 
        \end{array}\right\} \Longrightarrow 
        \left.\begin{array}{r}
                \overline{X}-\mu_1 \rightsquigarrow\cc{N}\left(0, \frac{\sigma^2}{n}\right) \\
                \overline{Y}-\mu_2 \rightsquigarrow\cc{N}\left(0, \frac{\sigma^2}{m}\right) 
        \end{array}\right\} \Longrightarrow 
        \left.\begin{array}{r}
                \alpha(\overline{X}-\mu_1 )\rightsquigarrow\cc{N}\left(0, \frac{\alpha^2\sigma^2}{n}\right) \\
                \beta(\overline{Y}-\mu_2 )\rightsquigarrow\cc{N}\left(0, \frac{\beta^2\sigma^2}{m}\right) 
        \end{array}\right\} 
    \end{equation*}
    Por lo que:
    \begin{equation*}
        \alpha(\overline{X}-\mu_1 ) +\beta(\overline{Y}-\mu_2 ) \rightsquigarrow \cc{N}\left(0, \dfrac{\alpha^2\sigma^2}{n} + \dfrac{\beta^2\sigma^2}{m}\right) \equiv \cc{N}\left(0, \sigma^2\left(\frac{\alpha^2}{n}+\frac{\beta^2}{m}\right) \right) 
    \end{equation*}
    Si tipificamos la variable:
    \begin{equation*}
        \dfrac{\alpha(\overline{X}-\mu_1 ) +\beta(\overline{Y}-\mu_2) }{\sigma\sqrt{\frac{\alpha^2}{n}+\frac{\beta^2}{m}}} \rightsquigarrow\cc{N}(0,1)
    \end{equation*}
    Por otra parte, tenemos que:
    \begin{equation*}
        \dfrac{(n-1)S_1^2}{\sigma^2} \rightsquigarrow\chi^2(n-1), \qquad 
        \dfrac{(m-1)S_2^2}{\sigma^2} \rightsquigarrow\chi^2(m-1) 
    \end{equation*}
    Como ambas son independientes por ser $S_1^2$ y $S_2^2$ independientes (ya que las muestras aleatorias simples eran independientes), podemos aplicar la propiedad reproductiva de $\chi^2$, obteniendo que:
    \begin{equation*}
        \dfrac{(n-1)S_1^2}{\sigma^2} + \dfrac{(m-1)S_2^2}{\sigma^2} = \dfrac{(n-1)S_1^2+(m-1)S_2^2}{\sigma^2}\rightsquigarrow\chi^2(n+m-2) 
    \end{equation*}
    Finalmente, la extensión del Lema de Fisher nos dice que $(\overline{X},\overline{Y})$ es independiente de $(S_1^2, S_2^2)$, por lo que las dos variables aleatorias con las que trabajamos son independientes, lo que nos permite aplicar la construcción de la distribución $t$ de Student:
    \begin{equation*}
        \dfrac{\dfrac{\alpha(\overline{X}-\mu_1 ) +\beta(\overline{Y}-\mu_2) }{\cancel{\sigma}\sqrt{\frac{\alpha^2}{n}+\frac{\beta^2}{m}}}}{\sqrt{\dfrac{\dfrac{(n-1)S_1^2+(m-1)S_2^2}{\cancel{\sigma^2}}}{n+m-2}}} = \dfrac{\alpha\left(\overline{X}-\mu_1\right)+\beta\left(\overline{Y}-\mu_2\right)}{\sqrt{\dfrac{(n-1)S_1^2 + (m-1)S_2^2}{n+m-2}} \sqrt{\dfrac{\alpha^2}{n} + \dfrac{\beta^2}{m}}} \rightsquigarrow t(n+m-2)
    \end{equation*}
\end{ejercicio}


