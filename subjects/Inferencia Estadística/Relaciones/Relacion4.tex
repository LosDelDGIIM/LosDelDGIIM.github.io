\newpage
\section{Estimación puntual. Insesgadez y mínima varianza}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra de una variable $X\rightsquigarrow\cc{N}(\mu, \sigma^2)$ con $\mu\in \mathbb{R}$, $\sigma\in \mathbb{R}^+$. Probar que
    \begin{equation*}
        T(X_1, \ldots, X_n) = \left\{\begin{array}{ll}
            1 & \text{si\ } \overline{X}\leq 0 \\
            0 & \text{si\ } \overline{X} > 0
        \end{array}\right. 
    \end{equation*}
    es un estimador insesgado de la función paramétrica $\Phi\left(\frac{-\mu \sqrt{n}}{\sigma}\right)$, siendo $\Phi$ la función de distribución de la $\cc{N}(0,1)$.\\

    \noindent
    Tenemos $T(X_1, \ldots, X_n) = I_{\left]-\infty,0\right]}(\overline{X})$. Como $X\rightsquigarrow \cc{N}(\mu, \sigma^2)$, sabemos por lo visto en el Tema 1 que entonces:
    \begin{equation*}
        \overline{X} \rightsquigarrow \cc{N}\left(\mu, \frac{\sigma^2}{n}\right)
    \end{equation*}

    de donde (escribiendo $T=T(X_1,\ldots,X_n)$):
    \begin{equation*}
    T = I_{\left]-\infty,0\right]}(\overline{X}) \rightsquigarrow B(1,P[\overline{X}\leq 0])
    \end{equation*}

    estamos ya en condiciones de ver que $T$ es insesgado para dicha función:
    \begin{equation*}
        E[T] \AstIg P[\overline{X}\leq 0] \stackrel{\text{tipif.}}{=} P\left[Z \leq \dfrac{-\mu\sqrt{n}}{\sigma}\right] = \Phi\left(\dfrac{-\mu\sqrt{n}}{\sigma}\right) 
    \end{equation*}
    donde en $(\ast)$ usamos que conocemos bien la esperanza de una distribución Bernoulli.
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de $X\rightsquigarrow B(1,p)$ con $p\in \left]0,1\right[$ y sea $T=\sum\limits_{i=1}^{n}X_i$.
    \begin{enumerate}[label=\alph*)]
        \item Probar que si $k\in \mathbb{N}$ y $k\leq n$, el estadístico
            \begin{equation*}
                \dfrac{T(T-1)\cdot \ldots\cdot (T-k+1)}{n(n-1)\cdot \ldots\cdot (n-k+1)}
            \end{equation*}
            es un estimador insesgado de $p^k$. ¿Es este estimador el UMVUE?.
        \item Probar que si $k>n$, no existe ningún estimador insesgado para $p^k$.
        \item ¿Puede afirmarse que $\frac{T}{n}{\left(1-\frac{T}{n}\right)}^{2}$ es insesgado para $p{(1-p)}^{2}$? 
    \end{enumerate}~\\

    Veamos cada apartado:
    \begin{enumerate}[label=\alph*)]
        \item Sea $k\in \mathbb{N}$ con $k\leq n$, definimos:
            \begin{equation*}
                h(T) = \dfrac{T(T-1)\cdot \ldots\cdot (T-k+1)}{n(n-1)\cdot \ldots\cdot (n-k+1)}
            \end{equation*}
            Veamos que $h(T)$ es insesgado para $p^k$. En primer lugar, observemos que por la reproductividad de la binomial $T\rightsquigarrow B(n,p)$:
            \begin{equation*}
                E[h(T)] = \dfrac{E[T(T-1)\cdot \ldots\cdot (T-k+1)]}{n(n-1)\cdot \ldots\cdot (n-k+1)} = \dfrac{(n-k)!}{n!}E[T(T-1)\cdot \ldots\cdot (T-k+1)]
            \end{equation*}
            Calculamos ahora la esperanza:
            \begin{align*}
                E[T(T-1)\cdot \ldots\cdot (T-k+1)] &= \sum_{t=0}^{n}t(t-1)\cdot \ldots\cdot (t-k+1)P[T=t] \\ 
                                                   &= \sum_{t=0}^{n}t(t-1)\cdot \ldots\cdot (t-k+1)\binom{n}{t}p^t{(1-p)}^{n-t} \\
                                                   &= \sum_{t=k}^{n}t(t-1)\cdot \ldots\cdot(t-k+1) \binom{n}{t}p^t {(1-p)}^{n-t} \\
                                                   &= \sum_{t=k}^{n}\dfrac{t!}{(t-k)!}\dfrac{n!}{t!(n-t)!}p^t {(1-p)}^{n-t} \\
                                                   &= \sum_{t=k}^{n}\dfrac{n!}{(t-k)!(n-t)!}p^t {(1-p)}^{n-t}
            \end{align*}
            si desarrollamos ahora los primeros términos, observamos que:
            \begin{align*}
                &E[T(T-1)\cdot \ldots\cdot (T-k+1)] = \sum_{t=k}^{n}\dfrac{n!}{(t-k)!(n-t)!}p^t {(1-p)}^{n-t} \\
               &= \dfrac{n!}{(n-k)!}p^k {(1-p)}^{n-k} + \dfrac{n!}{(n-k-1)!}p^{k+1}{(1-p)}^{n-k-1} + \ldots
            \end{align*}
            donde podemos ver que podemos sacar factor común de la sumatoria ciertos términos:
            \begin{align*}
                &E[T(T-1)\cdot \ldots\cdot (T-k+1)] = \sum_{t=k}^{n}\dfrac{n!}{(t-k)!(n-t)!}p^t {(1-p)}^{n-t} \\
               &= \dfrac{n!\cdot p^k }{(n-k)!}\sum_{t=k}^{n}p^{t-k}{(1-p)}^{n-t}\dfrac{(n-k)!}{(t-k)!(n-t)!} \\
               &= \dfrac{n!\cdot p^k}{(n-k)!}\sum_{t=0}^{n-k} p^t {(1-p)}^{n-k-t}\dfrac{(n-k)!}{t!(n-k-t)!} \\
               &= \dfrac{n!\cdot p^k}{(n-k)!}\sum_{t=0}^{n-k}\binom{n-k}{t} p^t {(1-p)}^{n-k-t} \\
               &= \dfrac{n!\cdot p^k}{(n-k)!}\sum_{t=0}^{n-k}P[S=t] = \dfrac{n!\cdot p^k}{(n-k)!}P[0\leq S \leq n-k] \AstIg \dfrac{n!\cdot p^k}{(n-k)!}
            \end{align*}
            para cierta variable aleatoria $S\rightsquigarrow B(n-k,p)$, donde en $(\ast)$ usamos que $P[0\leq S \leq n-k]=1$. Ahora, vemos que:
            \begin{equation*}
                E[h(T)] = \dfrac{(n-k)!}{n!}\dfrac{n!}{(n-k)!}p^k = p^k
            \end{equation*}
            por lo que $h(T)$ es insesgado para $p^k$. Veamos ahora que $h(T)$ es un estimador. Para ello, observamos primero que:
            \begin{equation*}
                h(T) = \dfrac{T}{n}\cdot \dfrac{T-1}{n-1}\cdot \ldots \cdot \dfrac{T-k+1}{n-k+1} = \prod_{j=0}^{k-1}\dfrac{T-j}{n-j}
            \end{equation*}
            Para cada $j\in \{0,\ldots,k-1\}$, observemos que $T\in \{0,\ldots,n\}$, por lo que $T-j\in \{-j, \ldots, n-j\}$ con $j<k\leq n$, de donde deducimos que:
            \begin{equation*}
                \dfrac{T-j}{n-j} \in [-1, 1] \quad \Longrightarrow \quad  \prod_{j=0}^{k-1}\dfrac{T-j}{n-j} \in [-1,1] 
            \end{equation*}
            Tenemos que ver finalmente que dicho producto es positivo, con lo que habremos probado que $h(T)$ es un estimador. Para ello, si el prodcuto no fuera positivo es por la existencia de $j\in \{0,\ldots,k-1\}$ de forma que $T-j<0$, es decir, tenemos entonces que $T\in \{0,\ldots,j-1\}$ supuesto que $T=l \in \{0,\ldots,j-1\}$, tendremos entonces que:
            \begin{equation*}
                h(T) = \prod_{j=0}^{k-1}\dfrac{T-j}{n-j} = \left(\prod_{j=0}^{l-1}\dfrac{T-j}{n-j} \right) \dfrac{T-l}{n-l} \left(\prod_{j=l+1}^{k-1}\dfrac{T-j}{n-j}\right) = 0
            \end{equation*}
            es decir, siempre que un término del producto sea negativo el producto entero se anula, por lo que siempre el producto es positivo, de donde $h(T)\in [0,1]$, por lo que $h(T)$ es un estimador.

            Finalmente, como:
            \begin{equation*}
                E\left[{(h(T))}^{2}\right] = \sum_{t=0}^{n}{(h(t))}^{2}P[T=t] < \infty
            \end{equation*}
            y teníamos que $T$ era un estimador suficiente y completo (cuando vimos que $\{B(1,p) : p\in \left]0,1\right[\}$ era una familia exponencial), tenemos entonces que $E[h(T)/T] = h(T)$ es el UMVUE de $p^k$.
        \item Sea ahora $k>n$, veamos que no puede existir ningun estimador insesgado para $p^k$. Para ello, por reducción al absurdo, supongamos que $h(T)$ es un estimador insesgado para $p^k$, con lo que:
            \begin{equation*}
                p^k = E[h(T)] = \sum_{t=0}^{n}h(t)P[T=t] = \sum_{t=0}^{n}h(t)\binom{n}{t}p^t {(1-p)}^{n-t}
            \end{equation*}

            de donde:
            \begin{equation*}
                1 = \sum_{t=0}^{n}h(t)\binom{n}{t}p^{t-k} {(1-p)}^{n-t} \qquad \forall p \in \left]0,1\right[
            \end{equation*}
            en particular, tomando $p\to 0$, como $t-k < 0$ para todo $t \in \{0,\ldots,k\}$, tenemos que:
            \begin{equation*}
                1 = \lim_{p\to0} \sum_{t=0}^{n}h(t)\binom{n}{t}p^{t-k} {(1-p)}^{n-t} = \infty
            \end{equation*}
            \underline{contradicción}, con lo que para $k>n$ no puede existir un estimador insesgado para $p^k$.
        \item Buscamos ahora comprobar si $h(T) = \frac{T}{n}{\left(1-\frac{T}{n}\right)}^{2}$ es insesgado para $p{(1-p)}^{2}$.  % // TODO: TERMINAR
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable $X\rightsquigarrow \cc{P}(\lm)$ con $\lm\in \mathbb{R}^+$. Encontrar, si existe, el UMVUE para $\lm^s$, siendo $s\in \mathbb{N}$ arbitrario.\\
    
    \noindent
    Veamos que $T(X_1, \ldots, X_n) = \sum\limits_{i=1}^{n}X_i$ es un estadístico suficiente y completo. Para ello, recordemos que $\{\cc{P}(\lm) : \lm>0\}$ es una familia exponencial:
    \begin{enumerate}
        \item El espacio paramétrico es $\mathbb{R}^+ \subseteq \mathbb{R}$.
        \item El espacio muestral es $\cc{X}=\mathbb{N}\cup \{0\}$, que no depende de $\lm$.
        \item Observamos que:
            \begin{align*}
                P_\lm[X=x] &= e^{-\lm}\dfrac{\lm^x}{x!} = exp\left[\ln\left(e^{-\lm}\dfrac{\lm^x}{x!}\right)\right] = exp\left(-\lm + x\ln\lm -\ln(x!)\right)
            \end{align*}
            por lo que basta tomar:
            \begin{equation*}
                Q(\lm) = \ln\lm, \qquad T(x) = x \qquad D(\lm) = -\lm, \qquad S(x) = -\ln(x!)
            \end{equation*}
    \end{enumerate}
    En consecuencia, por un Teorema visto en teoría, tenemos que el estadístico:
    \begin{equation*}
        T(X_1, \ldots, X_n) = \sum_{i=1}^{n}T(X_i) = \sum_{i=1}^{n}X_i
    \end{equation*}
    es suficiente y completo para $\lm$. Observemos que por la reproductividad de la Poisson tenemos que (notando $T=T(X_1, \ldots, X_n)$): 
    \begin{equation*}
        T\rightsquigarrow \cc{P}\left(\sum_{i=1}^{n}\lm\right) \equiv \cc{P}(n\lm)
    \end{equation*}
    Ahora, para buscar el UMVUE, buscamos una función $h$ medible de forma que:
    \begin{equation*}
        \lm^s = E[h(T)] = \sum_{t\in \mathbb{N}\cup \{0\}}h(t)P[T=t] = \sum_{t\in \mathbb{N}\cup \{0\}} h(t) e^{-n\lm} \dfrac{{(n\lm)}^{t}}{t!} 
    \end{equation*}

    por lo que:
    \begin{equation*}
        \lm^s e^{n\lm} = \sum_{t\in \mathbb{N}\cup \{0\}} h(t)\dfrac{{(n\lm)}^{t}}{t!}
    \end{equation*}

    y si aplicamos el desarrollo en serie de la exponencial, obtenemos:
    \begin{equation*}
        \lm^s \sum_{t\in \mathbb{N}\cup \{0\}} \dfrac{{(n\lm)}^{t}}{t!} = \lm^s e^{n\lm} = \sum_{t\in \mathbb{N}\cup\{0\}} h(t) \dfrac{{(n\lm)}^{t}}{t!} 
    \end{equation*}

    si desarrollamos cada uno de los términos:
    \begin{equation*}
        \lm^s + \lm^{s+1}n + \frac{\lm^{s+2}n^2}{2!} + \ldots = h(0) + h(1)(n\lm) + \ldots + h(s)\dfrac{{(n\lm)}^{s}}{s!} + \ldots
    \end{equation*}

    observamos que tomando:
    \begin{gather*}
        h(0) = \ldots = h(s-1) = 0,\quad  h(s) = \dfrac{s!}{n^s}\\h(s+1) = \dfrac{(s+1)!}{n^s}, \quad \ldots \quad  h(s+k) = \dfrac{(s+k)!}{n^s k!}
    \end{gather*}

    es decir:
    \begin{equation*}
        h(T) = \left\{\begin{array}{ll}
            0 & \text{si\ } T<s \\
            \dfrac{T!}{n^s(T-s)!}& \text{si\ } T\geq s
        \end{array}\right. 
    \end{equation*}
    tenemos que $h(T)$ es insesgado para $\lm^s$. Es claro además que $h(t)\in \mathbb{R}^+$ para cualquier valor de $t$, con lo que $h(T)$ es un estimador de $\lm^s$. Finalmente, observemos que:
    \begin{align*}
        E\left[{(h(T))}^{2}\right] &= \sum_{t\in \mathbb{N}\cup \{0\}} {(h(t))}^{2}P[T=t] = \sum_{t\geq s} {\left(\dfrac{t!}{n^s(t-s)!}\right)}^{2} e^{-n\lm} \dfrac{{(n\lm)}^{t}}{t!} \\ &= \dfrac{1}{n^s e^{n\lm}} \sum_{t\geq s} \dfrac{{(n\lm)}^{t}t!}{{((t-s)!)}^{2}}
    \end{align*}

    como:
    \begin{equation*}
        \dfrac{\dfrac{{(n\lm)}^{t+1}(t+1)!}{{((t+1-s)!)}^{2}}}{\dfrac{{(n\lm)}^{t}t!}{{((t-s)!)}^{2}}} = \dfrac{{(n\lm)}^{t+1}(t+1)!{((t-s)!)}^{2}}{{(n\lm)}^{t}t!{((t+1-s)!)}^{2}} = \dfrac{n\lm(t+1)}{{(t+1-s)}^{2}} \to 0 < 1
    \end{equation*}
    por el Criterio del cociente, tenemos que:
    \begin{equation*}
        \sum_{t\geq s} \dfrac{{(n\lm)}^{t}t!}{{((t-s)!)}^{2}} < \infty \Longrightarrow E\left[{(h(T))}^{2}\right]  =\dfrac{1}{n^s e^{n\lm}}\sum_{t\geq s} \dfrac{{(n\lm)}^{t}t!}{{((t-s)!)}^{2}}   < \infty
    \end{equation*}
    en consecuencia, tenemos que $h(T)$ es un estimador insesgado para $\lm^s$ y de momento de segundo orden finito y es función de un estadístico suficiente y completo, con lo que el Teorema de Lehmann-Scheffé nos dice que:
    \begin{equation*}
        E[h(T)/T] = h(T)
    \end{equation*}
    es un UMVUE para $\lm^s$.
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable con distribución uniforme discreta en los puntos $\{1,\ldots,N\}$, siendo $N$ un número natural arbitrario. Encontrar el UMVUE para $N$.\\

    \noindent
    En el Ejercicio~\ref{ej:3.5} vimos que $T(X_1, \ldots, X_n) = X_{(n)}$ era un estadístico suficiente y completo. Si notamos $T = T(X_1,\ldots, X_n)$, tenemos que:
    \begin{equation*}
        F_T(t) = {(F_X(t))}^{n} \Longrightarrow P[T=t] = P[T\leq t] - P[T\leq t-1] = {(F_X(t))}^{n}-{(F_X(t-1))}^{n}
    \end{equation*}
    como $F_X(t) = \frac{t}{N}$, tenemos:
    \begin{equation*}
        P[T=t] = {(F_X(t))}^{n}-{(F_X(t-1))}^{n} = \dfrac{t^n - {(t-1)}^{n}}{N^n}
    \end{equation*} 
    Buscamos ahora una función $h$ medible de forma que:
    \begin{equation*}
        N = E[h(T)] = \sum_{t=1}^{N}h(t) P[T=t] = \sum_{t=1}^{N}h(t)\dfrac{t^n-{(t-1)}^{n}}{N^n}
    \end{equation*}
    % // TODO: TERMINAR
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ cuya función de densidad es de la forma
    \begin{equation*}
        f_\theta(x) = \dfrac{1}{2\sqrt{x\theta}}, \quad 0<x<\theta
    \end{equation*}
    Calcular, si existe, el UMVUE para $\theta$.\\

    \noindent
    En primer lugar, buscamos un estadístico suficiente y completo:
    \begin{equation*}
        f_\theta(x_1, \ldots, x_n) \stackrel{\text{iid.}}{=} \prod_{i=1}^{n}f_\theta(x_i) = \prod_{i=1}^{n}\dfrac{1}{2\sqrt{x_i\theta}} = \dfrac{1}{{\left(2\sqrt{\theta}\right)}^{n}} \prod_{i=1}^{n} \frac{1}{\sqrt{x_i}} \qquad x_i \in \left]0,\theta\right[
    \end{equation*}
    Si consideramos $0< X_{(1)} \leq X_{(n)} < \theta $, podemos escribir:
    \begin{equation*}
        f_\theta(x_1, \ldots, x_n) = \dfrac{I_{\left]0,+\infty\right[}(X_{(1)}-0) \cdot I_{\left]-\infty,0\right[}(X_{(n)}-\theta)}{{\left(2\sqrt{\theta}\right)}^{n}} \prod_{i=1}^{n}\dfrac{1}{\sqrt{x_i}}
    \end{equation*}
    Tomando:
    \begin{gather*}
        h(x_1, \ldots, x_n) = \prod_{i=1}^{n}\dfrac{1}{\sqrt{x_i}}, \qquad T(X_1, \ldots, X_n) = X_{(n)} \\
        g_\theta(t) = \dfrac{I_{\left]0,+\infty\right[}(X_{(1)}-0) \cdot I_{\left]-\infty,0\right[}(t-\theta)}{{\left(2\sqrt{\theta}\right)}^{n}} 
    \end{gather*}
    por el Teorema de factorización de Neymann-Fisher tenemos que el estadístico $X_{(n)}$ es suficiente para $\theta$. Notando $T=X_{(n)}$ para abreviar, calculamos la distribución de $T$:
    \begin{equation*}
        F_T(t) = {(F_X(t))}^{n} \quad \Longrightarrow \quad  f_T(t) = n{(F_X(t))}^{n-1}f_\theta(t)
    \end{equation*}
    Calculamos la distribución de $X$:
    \begin{equation*}
        F_X(t) = \int_{0}^{t} \dfrac{1}{2\sqrt{x\theta}}~dx = \dfrac{1}{2\sqrt{\theta}}\int_{0}^{t} \dfrac{1}{\sqrt{x}}~dx  = \sqrt{\frac{t}{\theta}} \qquad t \in \left]0,\theta\right[
    \end{equation*}
    Por lo que:
    \begin{equation*}
        f_T(t) = n{(F_X(t))}^{n-1}f_\theta(t) = n{\left(\sqrt{\frac{t}{\theta}}\right)}^{n-1}\frac{1}{2\sqrt{t\theta}} = \dfrac{n{\left(\sqrt{t}\right)}^{n-2}}{2{\left(\sqrt{\theta}\right)}^{n}}\qquad t\in \left]0,\theta\right[
    \end{equation*}
    Sea ahora $h$ una función medible de forma que:
    \begin{equation*}
        0 = E[h(T)] = \int_{0}^{\theta} h(t) f_T(t)~dt = \frac{n}{2{\left(\sqrt{\theta}\right)}^{n}}\int_{0}^{\theta} h(t){\left(\sqrt{t}\right)}^{n-2}~dt  \qquad \forall \theta\in \mathbb{R}^+
    \end{equation*}

    tenemos entonces que:
    \begin{equation*}
        \int_{0}^{\theta} h(t){\left(\sqrt{t}\right)}^{n-2}~dt = 0 \qquad \forall \theta\in \mathbb{R}^+
    \end{equation*}
    Sea ahora $H(t)$ una primitiva de $h(t){\left(\sqrt{t}\right)}^{n-2}$, tenemos entonces que:
    \begin{equation*}
        H(\theta) - H(0) = 0 \qquad \forall \theta\in \mathbb{R}^+
    \end{equation*}

    por lo que derivando respecto a $\theta$:
    \begin{equation*}
        h(\theta) {\left(\sqrt{\theta}\right)}^{n-2} = 0 \qquad \forall \theta\in \mathbb{R}^+ \quad \Longrightarrow \quad  h(\theta) = 0 \qquad \forall \theta\in \mathbb{R}^+
    \end{equation*}
    En conclusión, tenemos que:
    \begin{equation*}
        \mathbb{R}^+ \subseteq \{t:h(t) = 0\}
    \end{equation*}

    por lo que:
    \begin{equation*}
        1 \geq P[h(T) = 0] \geq P[T\in \mathbb{R}^+] = 1 \quad \Longrightarrow \quad P[h(T)=0] = 1
    \end{equation*}
    lo que demustra que $T$ es un estadístico completo. Ahora, tratamos de buscar un estimador insesgado para $\theta$, que será nuestro candidato a UMVUE. Sea $h$ una función medible de forma que:
    \begin{equation*}
        \theta = E[h(T)] = \dfrac{n}{2{\left(\sqrt{\theta}\right)}^{n}}\int_{0}^{\theta} h(t){\left(\sqrt{t}\right)}^{n-2}~dt 
    \end{equation*} % // TODO: TERMINAR
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con función de densidad
    \begin{equation*}
        f_\theta(x) = \dfrac{\theta}{x^2}, \quad x>\theta
    \end{equation*}
    Calcular, si existen, los UMVUE para $\theta$ y para $\nicefrac{1}{\theta}$.\\

    \noindent
    Calculamos en primer lugar un estadístico suficiente y completo para $\theta$:
    \begin{equation*}
        f_\theta(x_1,\ldots, x_n) \stackrel{\text{iid.}}{=} \prod_{i=1}^{n}f_\theta(x_i) =\prod_{i=1}^{n} \dfrac{\theta\cdot I_{\left]0,+\infty\right[}(X_{(1)}-\theta)}{x_i^2} = \theta^n \cdot I_{\left]0,+\infty\right[}(X_{(1)}-\theta)\prod_{i=1}^{n} \dfrac{1}{x_i^2}
    \end{equation*}
    Tomando:
    \begin{gather*}
        h(x_1, \ldots, x_n) = \prod_{i=1}^{n}\dfrac{1}{x_i^2}, \qquad T(X_1, \ldots, X_n) = X_{(1)} \\
        g_\theta(t) = \theta^n \cdot I_{\left]0,+\infty\right[}(t-\theta)
    \end{gather*}
    Podemos aplicar el Teorema de factorización de Neymann-Fisher, obteniendo que $T$ es suficiente para $\theta$. Notando $T = X_{(1)}$ para abreviar, calculamos la distribución de $T$:
    \begin{equation*}
        F_T(t) = 1-{(1-F_X(t))}^{n} \quad \Longrightarrow \quad f_T(t) = n{(1-F_X(t))}^{n-1}f_\theta(t)
    \end{equation*}
    Si calculamos:
    \begin{equation*}
        F_X(t) = \int_{\theta}^{t} \dfrac{\theta}{x^2}~dx  = \theta \int_{\theta}^{t} \dfrac{1}{x^2}~dx = \theta \left[\dfrac{-1}{x}\right]_\theta^t = \theta\left(\dfrac{-1}{t}+\dfrac{1}{\theta}\right) = \dfrac{\theta-t}{t} \qquad  t > \theta
    \end{equation*}
    Tenemos que:
    \begin{equation*}
        f_T(t) = n{\left(1-\dfrac{\theta-t}{t}\right)}^{n-1}\dfrac{\theta}{t^2}
    \end{equation*} % // TODO: TERMINAR
\end{ejercicio}

\begin{ejercicio}
    Sea $X\rightsquigarrow P_\theta$ siendo $P_\theta$ una distribución con función de densidad
    \begin{equation*}
        f_\theta(x) = e^{\theta-x}, \quad x\geq \theta
    \end{equation*}
    Dada una muestra aleatoria simple de tamaño arbitrario, encontrar los UMVUE de $\theta$ y de $e^{\theta}$.\\

    \noindent
    Calculamos en primer lugar un estadístico suficiente y completo para $\theta$:
    \begin{equation*}
        f_\theta(x_1, \ldots, x_n) \stackrel{\text{iid.}}{=} \prod_{i=1}^{n}f_\theta(x_i) = \prod_{i=1}^{n} e^{\theta-x_i} \cdot I_{\left[0,+\infty\right[}(X_{(1)}-\theta)=I_{\left[0,+\infty\right[}(X_{(1)}-\theta) e^{n\theta}\prod_{i=1}^{n} e^{-x_i}
    \end{equation*}
    Tomando:
    \begin{gather*}
        h(x_1, \ldots, x_n) = \prod_{i=1}^{n}e^{-x_i}, \qquad T(X_1, \ldots, X_n) = X_{(1)} \\
        g_\theta(t) = I_{\left[0,+\infty\right[}(t-\theta) \cdot e^{n\theta}
    \end{gather*}
    Tenemos por el Teorema de factorización de Neymann-Fisher que $T=X_{(1)}$ es suficiente para $\theta$. Calculamos la distribución de $T$:
    \begin{equation*}
        F_T(t) = 1-{(1-F_X(t))}^{n} \quad \Longrightarrow \quad f_T(t) = n{(1-F_X(t))}^{n-1}f_\theta(t)
    \end{equation*}
    Si calculamos:
    \begin{equation*}
        F_X(t) = \int_{\theta}^{t} e^{\theta-x}~dx  = e^{\theta}\int_{\theta}^{t} e^{-x}~dx  = e^{\theta}[-e^{-x}]_\theta^t = e^{\theta}\left(e^{-\theta} - e^{-t}\right) = 1- e^{\theta-t}, \quad t\geq \theta
    \end{equation*}
    Tenemos entonces que:
    \begin{equation*}
        f_T(t) = n{\left(e^{\theta-t}\right)}^{n-1} e^{\theta-t} = n{\left(e^{\theta-t}\right)}^{n}
    \end{equation*}
    Sea $h$ una función medible de forma que:
    \begin{equation*}
        0 = E[h(T)] = \int_{\theta}^{+\infty} h(t)n{\left(e^{\theta-t}\right)}^{n}~dt 
    \end{equation*} % // TODO: TERMINAR
\end{ejercicio}

\begin{ejercicio}
    Sea $X$ la variable que describe el número de fracasos antes del primer éxito en una sucesión de pruebas de Bernoulli con probabilidad de éxito $\theta\in \left]0,1\right[$, y sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de $X$.
    \begin{enumerate}[label=\alph*)]
        \item Probar que la familia de distribuciones de $X$ es regular y calcular la función de información asociada a la muestra.
        \item Especificar la clase de funciones paramétricas que admiten estimadores eficientes y los correspondientes estimadores.
        \item Calcular la varianza de cada estimador eficiente y comprobar que coincide con las correspondiente cota de Fréchet-Cramér-Rao.
        \item Calcular, si existen, los UMVUE para $P_\theta[X=0]$ y para $E_\theta[X]$ y decir si son eficientes.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con distribución exponencial.
    \begin{enumerate}[label=\alph*)]
        \item Probar que la familia de distribuciones de $X$ es regular.
        \item Encontrar la clase de funciones paramétricas que admiten estimador eficiente y el estimador correspondiente. Calcular la varianza de estos estimadores.
        \item Basándose en el apartado anterior, encontrar el UMVUE para la media de $X$.
        \item Dar la cota de Fréchet-Cramér-Rao para la varianza de estimadores insesgados y regulares de $\lm^3$. ¿Es alcanzable dicha cota?
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $X$ una variable aleatoria con función de densidad de la forma
    \begin{equation*}
        f_\theta(x) = \theta x^{\theta-1}, \quad 0<x<1
    \end{equation*}
    \begin{enumerate}[label=\alph*)]
        \item Sabiendo que $E_\theta[\ln X] = -\frac{1}{\theta}$ y $Var_\theta[\ln X] = \frac{1}{\theta^2}$, comprobar que esta familia de distribuciones es regular.
        \item Basándose en una muestra aleatoria simple de $X$, dar la clase de funciones paramétricas con estimador eficiente, los estimadores y su varianza.
    \end{enumerate}
\end{ejercicio}
