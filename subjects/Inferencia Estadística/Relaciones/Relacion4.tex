\newpage
\section{Estimación puntual. Insesgadez y mínima varianza}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra de una variable $X\rightsquigarrow\cc{N}(\mu, \sigma^2)$ con $\mu\in \mathbb{R}$, $\sigma\in \mathbb{R}^+$. Probar que
    \begin{equation*}
        T(X_1, \ldots, X_n) = \left\{\begin{array}{ll}
            1 & \text{si\ } \overline{X}\leq 0 \\
            0 & \text{si\ } \overline{X} > 0
        \end{array}\right. 
    \end{equation*}
    es un estimador insesgado de la función paramétrica $\Phi\left(\frac{-\mu \sqrt{n}}{\sigma}\right)$, siendo $\Phi$ la función de distribución de la $\cc{N}(0,1)$.\\

    \noindent
    Tenemos $T(X_1, \ldots, X_n) = I_{\left]-\infty,0\right]}(\overline{X})$. Como $X\rightsquigarrow \cc{N}(\mu, \sigma^2)$, sabemos por lo visto en el Tema 1 que entonces:
    \begin{equation*}
        \overline{X} \rightsquigarrow \cc{N}\left(\mu, \frac{\sigma^2}{n}\right)
    \end{equation*}

    de donde (escribiendo $T=T(X_1,\ldots,X_n)$):
    \begin{equation*}
    T = I_{\left]-\infty,0\right]}(\overline{X}) \rightsquigarrow B(1,P[\overline{X}\leq 0])
    \end{equation*}

    estamos ya en condiciones de ver que $T$ es insesgado para dicha función:
    \begin{equation*}
        E[T] \AstIg P[\overline{X}\leq 0] \stackrel{\text{tipif.}}{=} P\left[Z \leq \dfrac{-\mu\sqrt{n}}{\sigma}\right] = \Phi\left(\dfrac{-\mu\sqrt{n}}{\sigma}\right) 
    \end{equation*}
    donde en $(\ast)$ usamos que conocemos bien la esperanza de una distribución Bernoulli.
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de $X\rightsquigarrow B(1,p)$ con $p\in \left]0,1\right[$ y sea $T=\sum\limits_{i=1}^{n}X_i$.
    \begin{enumerate}[label=\alph*)]
        \item Probar que si $k\in \mathbb{N}$ y $k\leq n$, el estadístico
            \begin{equation*}
                \dfrac{T(T-1)\cdot \ldots\cdot (T-k+1)}{n(n-1)\cdot \ldots\cdot (n-k+1)}
            \end{equation*}
            es un estimador insesgado de $p^k$. ¿Es este estimador el UMVUE?.
        \item Probar que si $k>n$, no existe ningún estimador insesgado para $p^k$.
        \item ¿Puede afirmarse que $\frac{T}{n}{\left(1-\frac{T}{n}\right)}^{2}$ es insesgado para $p{(1-p)}^{2}$? 
    \end{enumerate}~\\

    Veamos cada apartado:
    \begin{enumerate}[label=\alph*)]
        \item Sea $k\in \mathbb{N}$ con $k\leq n$, definimos:
            \begin{equation*}
                h(T) = \dfrac{T(T-1)\cdot \ldots\cdot (T-k+1)}{n(n-1)\cdot \ldots\cdot (n-k+1)}
            \end{equation*}
            Veamos que $h(T)$ es insesgado para $p^k$. En primer lugar, observemos que por la reproductividad de la binomial $T\rightsquigarrow B(n,p)$:
            \begin{equation*}
                E[h(T)] = \dfrac{E[T(T-1)\cdot \ldots\cdot (T-k+1)]}{n(n-1)\cdot \ldots\cdot (n-k+1)} = \dfrac{(n-k)!}{n!}E[T(T-1)\cdot \ldots\cdot (T-k+1)]
            \end{equation*}
            Calculamos ahora la esperanza:
            \begin{align*}
                E[T(T-1)\cdot \ldots\cdot (T-k+1)] &= \sum_{t=0}^{n}t(t-1)\cdot \ldots\cdot (t-k+1)P[T=t] \\ 
                                                   &= \sum_{t=0}^{n}t(t-1)\cdot \ldots\cdot (t-k+1)\binom{n}{t}p^t{(1-p)}^{n-t} \\
                                                   &= \sum_{t=k}^{n}t(t-1)\cdot \ldots\cdot(t-k+1) \binom{n}{t}p^t {(1-p)}^{n-t} \\
                                                   &= \sum_{t=k}^{n}\dfrac{t!}{(t-k)!}\dfrac{n!}{t!(n-t)!}p^t {(1-p)}^{n-t} \\
                                                   &= \sum_{t=k}^{n}\dfrac{n!}{(t-k)!(n-t)!}p^t {(1-p)}^{n-t}
            \end{align*}
            si desarrollamos ahora los primeros términos, observamos que:
            \begin{align*}
                &E[T(T-1)\cdot \ldots\cdot (T-k+1)] = \sum_{t=k}^{n}\dfrac{n!}{(t-k)!(n-t)!}p^t {(1-p)}^{n-t} \\
               &= \dfrac{n!}{(n-k)!}p^k {(1-p)}^{n-k} + \dfrac{n!}{(n-k-1)!}p^{k+1}{(1-p)}^{n-k-1} + \ldots
            \end{align*}
            donde podemos ver que podemos sacar factor común de la sumatoria ciertos términos:
            \begin{align*}
                &E[T(T-1)\cdot \ldots\cdot (T-k+1)] = \sum_{t=k}^{n}\dfrac{n!}{(t-k)!(n-t)!}p^t {(1-p)}^{n-t} \\
               &= \dfrac{n!\cdot p^k }{(n-k)!}\sum_{t=k}^{n}p^{t-k}{(1-p)}^{n-t}\dfrac{(n-k)!}{(t-k)!(n-t)!} \\
               &= \dfrac{n!\cdot p^k}{(n-k)!}\sum_{t=0}^{n-k} p^t {(1-p)}^{n-k-t}\dfrac{(n-k)!}{t!(n-k-t)!} \\
               &= \dfrac{n!\cdot p^k}{(n-k)!}\sum_{t=0}^{n-k}\binom{n-k}{t} p^t {(1-p)}^{n-k-t} \\
               &= \dfrac{n!\cdot p^k}{(n-k)!}\sum_{t=0}^{n-k}P[S=t] = \dfrac{n!\cdot p^k}{(n-k)!}P[0\leq S \leq n-k] \AstIg \dfrac{n!\cdot p^k}{(n-k)!}
            \end{align*}
            para cierta variable aleatoria $S\rightsquigarrow B(n-k,p)$, donde en $(\ast)$ usamos que $P[0\leq S \leq n-k]=1$. Ahora, vemos que:
            \begin{equation*}
                E[h(T)] = \dfrac{(n-k)!}{n!}\dfrac{n!}{(n-k)!}p^k = p^k
            \end{equation*}
            por lo que $h(T)$ es insesgado para $p^k$. Veamos ahora que $h(T)$ es un estimador. Para ello, observamos primero que:
            \begin{equation*}
                h(T) = \dfrac{T}{n}\cdot \dfrac{T-1}{n-1}\cdot \ldots \cdot \dfrac{T-k+1}{n-k+1} = \prod_{j=0}^{k-1}\dfrac{T-j}{n-j}
            \end{equation*}
            Para cada $j\in \{0,\ldots,k-1\}$, observemos que $T\in \{0,\ldots,n\}$, por lo que $T-j\in \{-j, \ldots, n-j\}$ con $j<k\leq n$, de donde deducimos que:
            \begin{equation*}
                \dfrac{T-j}{n-j} \in [-1, 1] \quad \Longrightarrow \quad  \prod_{j=0}^{k-1}\dfrac{T-j}{n-j} \in [-1,1] 
            \end{equation*}
            Tenemos que ver finalmente que dicho producto es positivo, con lo que habremos probado que $h(T)$ es un estimador. Para ello, si el prodcuto no fuera positivo es por la existencia de $j\in \{0,\ldots,k-1\}$ de forma que $T-j<0$, es decir, tenemos entonces que $T\in \{0,\ldots,j-1\}$ supuesto que $T=l \in \{0,\ldots,j-1\}$, tendremos entonces que:
            \begin{equation*}
                h(T) = \prod_{j=0}^{k-1}\dfrac{T-j}{n-j} = \left(\prod_{j=0}^{l-1}\dfrac{T-j}{n-j} \right) \dfrac{T-l}{n-l} \left(\prod_{j=l+1}^{k-1}\dfrac{T-j}{n-j}\right) = 0
            \end{equation*}
            es decir, siempre que un término del producto sea negativo el producto entero se anula, por lo que siempre el producto es positivo, de donde $h(T)\in [0,1]$, por lo que $h(T)$ es un estimador.

            Finalmente, como:
            \begin{equation*}
                E\left[{(h(T))}^{2}\right] = \sum_{t=0}^{n}{(h(t))}^{2}P[T=t] < \infty
            \end{equation*}
            y teníamos que $T$ era un estimador suficiente y completo (cuando vimos que $\{B(1,p) : p\in \left]0,1\right[\}$ era una familia exponencial), tenemos entonces que $E[h(T)/T] = h(T)$ es el UMVUE de $p^k$.
        \item Sea ahora $k>n$, veamos que no puede existir ningun estimador insesgado para $p^k$. Para ello, por reducción al absurdo, supongamos que $h(T)$ es un estimador insesgado para $p^k$, con lo que:
            \begin{equation*}
                p^k = E[h(T)] = \sum_{t=0}^{n}h(t)P[T=t] = \sum_{t=0}^{n}h(t)\binom{n}{t}p^t {(1-p)}^{n-t}
            \end{equation*}

            de donde:
            \begin{equation*}
                1 = \sum_{t=0}^{n}h(t)\binom{n}{t}p^{t-k} {(1-p)}^{n-t} \qquad \forall p \in \left]0,1\right[
            \end{equation*}
            en particular, tomando $p\to 0$, como $t-k < 0$ para todo $t \in \{0,\ldots,k\}$, tenemos que:
            \begin{equation*}
                1 = \lim_{p\to0} \sum_{t=0}^{n}h(t)\binom{n}{t}p^{t-k} {(1-p)}^{n-t} = \infty
            \end{equation*}
            \underline{contradicción}, con lo que para $k>n$ no puede existir un estimador insesgado para $p^k$. También podríamos haber justificado que son dos polinomios de distinto grado, para llegar a contradicción.
        \item Buscamos ahora comprobar si $h(T) = \frac{T}{n}{\left(1-\frac{T}{n}\right)}^{2}$ es insesgado para $p{(1-p)}^{2}$.

            Resulta que esto no puede afirmarse para todos los valores de $n$ y $p$. Como un contraejemplo, para $n=3$ resulta que tenemos:
            \begin{align*}
                E[h(T)] &= \sum_{t=0}^{3}\frac{t}{3}{\left(1-\frac{t}{3}\right)}^{2}\binom{3}{t}p^t{(1-p)}^{3-t} \AstIg \sum_{t=1}^{2}\frac{t}{3}{\left(1-\frac{t}{3}\right)}^{2}\binom{3}{t}p^t{(1-p)}^{3-t} \\
                        &= \frac{1}{3}{\left(1-\frac{1}{3}\right)}^{2}\binom{3}{1}p{(1-p)}^{2}+\frac{2}{3}{\left(1-\dfrac{2}{3}\right)}^{2}\binom{3}{2}p^2(1-p) \\
                        &= p(1-p)\left(\frac{1}{9}(1-p)+\frac{2}{9}p\right) = p(1-p)\frac{(1+p)}{9} \qquad \forall p\in \left]0,1\right[
            \end{align*}
            donde en $(\ast)$ hemos usado que la función $t\mapsto \frac{t}{n}{\left(1-\frac{t}{n}\right)}^{2}$ evaluada en 0 y 3 es igual a cero. Observamos que:
            \begin{equation*}
                E[h(T)] = p(1-p)\frac{(1+p)}{9} = p{(1-p)}^{2} \Longleftrightarrow \dfrac{1+p}{9} = 1-p
            \end{equation*}
            resultado que no es cierto para todo $p\in \left]0,1\right[$ (basta considerar $p=\nicefrac{1}{2}$), por lo que dicho estimardor no es insesgado para $p{(1-p)}^{2}$.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable $X\rightsquigarrow \cc{P}(\lm)$ con $\lm\in \mathbb{R}^+$. Encontrar, si existe, el UMVUE para $\lm^s$, siendo $s\in \mathbb{N}$ arbitrario.\\
    
    \noindent
    Veamos que $T(X_1, \ldots, X_n) = \sum\limits_{i=1}^{n}X_i$ es un estadístico suficiente y completo. Para ello, recordemos que $\{\cc{P}(\lm) : \lm>0\}$ es una familia exponencial:
    \begin{enumerate}
        \item El espacio paramétrico es $\mathbb{R}^+ \subseteq \mathbb{R}$.
        \item El espacio muestral es $\cc{X}=\mathbb{N}\cup \{0\}$, que no depende de $\lm$.
        \item Observamos que:
            \begin{align*}
                P_\lm[X=x] &= e^{-\lm}\dfrac{\lm^x}{x!} = exp\left[\ln\left(e^{-\lm}\dfrac{\lm^x}{x!}\right)\right] = exp\left(-\lm + x\ln\lm -\ln(x!)\right)
            \end{align*}
            por lo que basta tomar:
            \begin{equation*}
                Q(\lm) = \ln\lm, \qquad T(x) = x \qquad D(\lm) = -\lm, \qquad S(x) = -\ln(x!)
            \end{equation*}
    \end{enumerate}
    En consecuencia, por un Teorema visto en teoría, tenemos que el estadístico:
    \begin{equation*}
        T(X_1, \ldots, X_n) = \sum_{i=1}^{n}T(X_i) = \sum_{i=1}^{n}X_i
    \end{equation*}
    es suficiente y completo para $\lm$. Observemos que por la reproductividad de la Poisson tenemos que (notando $T=T(X_1, \ldots, X_n)$): 
    \begin{equation*}
        T\rightsquigarrow \cc{P}\left(\sum_{i=1}^{n}\lm\right) \equiv \cc{P}(n\lm)
    \end{equation*}
    Ahora, para buscar el UMVUE, buscamos una función $h$ medible de forma que:
    \begin{equation*}
        \lm^s = E[h(T)] = \sum_{t\in \mathbb{N}\cup \{0\}}h(t)P[T=t] = \sum_{t\in \mathbb{N}\cup \{0\}} h(t) e^{-n\lm} \dfrac{{(n\lm)}^{t}}{t!} 
    \end{equation*}

    por lo que:
    \begin{equation*}
        \lm^s e^{n\lm} = \sum_{t\in \mathbb{N}\cup \{0\}} h(t)\dfrac{{(n\lm)}^{t}}{t!}
    \end{equation*}

    y si aplicamos el desarrollo en serie de la exponencial, obtenemos:
    \begin{equation*}
        \lm^s \sum_{t\in \mathbb{N}\cup \{0\}} \dfrac{{(n\lm)}^{t}}{t!} = \lm^s e^{n\lm} = \sum_{t\in \mathbb{N}\cup\{0\}} h(t) \dfrac{{(n\lm)}^{t}}{t!} 
    \end{equation*}

    si desarrollamos cada uno de los términos:
    \begin{equation*}
        \lm^s + \lm^{s+1}n + \frac{\lm^{s+2}n^2}{2!} + \ldots = h(0) + h(1)(n\lm) + \ldots + h(s)\dfrac{{(n\lm)}^{s}}{s!} + \ldots
    \end{equation*}

    observamos que tomando:
    \begin{gather*}
        h(0) = \ldots = h(s-1) = 0,\quad  h(s) = \dfrac{s!}{n^s}\\h(s+1) = \dfrac{(s+1)!}{n^s}, \quad \ldots \quad  h(s+k) = \dfrac{(s+k)!}{n^s k!}
    \end{gather*}

    es decir:
    \begin{equation*}
        h(T) = \left\{\begin{array}{ll}
            0 & \text{si\ } T<s \\
            \dfrac{T!}{n^s(T-s)!}& \text{si\ } T\geq s
        \end{array}\right. 
    \end{equation*}
    tenemos que $h(T)$ es insesgado para $\lm^s$. Es claro además que $h(t)\in \mathbb{R}^+$ para cualquier valor de $t$, con lo que $h(T)$ es un estimador de $\lm^s$. Finalmente, observemos que:
    \begin{align*}
        E\left[{(h(T))}^{2}\right] &= \sum_{t\in \mathbb{N}\cup \{0\}} {(h(t))}^{2}P[T=t] = \sum_{t\geq s} {\left(\dfrac{t!}{n^s(t-s)!}\right)}^{2} e^{-n\lm} \dfrac{{(n\lm)}^{t}}{t!} \\ &= \dfrac{1}{n^s e^{n\lm}} \sum_{t\geq s} \dfrac{{(n\lm)}^{t}t!}{{((t-s)!)}^{2}}
    \end{align*}

    como:
    \begin{equation*}
        \dfrac{\dfrac{{(n\lm)}^{t+1}(t+1)!}{{((t+1-s)!)}^{2}}}{\dfrac{{(n\lm)}^{t}t!}{{((t-s)!)}^{2}}} = \dfrac{{(n\lm)}^{t+1}(t+1)!{((t-s)!)}^{2}}{{(n\lm)}^{t}t!{((t+1-s)!)}^{2}} = \dfrac{n\lm(t+1)}{{(t+1-s)}^{2}} \to 0 < 1
    \end{equation*}
    por el Criterio del cociente, tenemos que:
    \begin{equation*}
        \sum_{t\geq s} \dfrac{{(n\lm)}^{t}t!}{{((t-s)!)}^{2}} < \infty \Longrightarrow E\left[{(h(T))}^{2}\right]  =\dfrac{1}{n^s e^{n\lm}}\sum_{t\geq s} \dfrac{{(n\lm)}^{t}t!}{{((t-s)!)}^{2}}   < \infty
    \end{equation*}
    en consecuencia, tenemos que $h(T)$ es un estimador insesgado para $\lm^s$ y de momento de segundo orden finito y es función de un estadístico suficiente y completo, con lo que el Teorema de Lehmann-Scheffé nos dice que:
    \begin{equation*}
        E[h(T)/T] = h(T)
    \end{equation*}
    es un UMVUE para $\lm^s$.
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable con distribución uniforme discreta en los puntos $\{1,\ldots,N\}$, siendo $N$ un número natural arbitrario. Encontrar el UMVUE para $N$.\\

    \noindent
    En el Ejercicio~\ref{ej:3.5} vimos que $T(X_1, \ldots, X_n) = X_{(n)}$ era un estadístico suficiente y completo. Si notamos $T = T(X_1,\ldots, X_n)$, tenemos que:
    \begin{equation*}
        F_T(t) = {(F_X(t))}^{n} \Longrightarrow P[T=t] = P[T\leq t] - P[T\leq t-1] = {(F_X(t))}^{n}-{(F_X(t-1))}^{n}
    \end{equation*}
    como $F_X(t) = \frac{t}{N}$ para $t\in \{1,\ldots,N\}$, tenemos:
    \begin{equation*}
        P[T=t] = {(F_X(t))}^{n}-{(F_X(t-1))}^{n} = \dfrac{t^n - {(t-1)}^{n}}{N^n}
    \end{equation*} 
    Buscamos ahora una función $h$ medible de forma que:
    \begin{equation*}
        N = E[h(T)] = \sum_{t=1}^{N}h(t) P[T=t] = \sum_{t=1}^{N}h(t)\dfrac{t^n-{(t-1)}^{n}}{N^n}
    \end{equation*}
    Si desarrollamos la suma para $h(t) = 1$, observamos un comportamiento telescópico:
    \begin{align*}
        \dfrac{1}{N^n}\sum_{t=1}^{N}(t^n - {(t-1)}^{n}) &= \dfrac{1}{N^n}(N^n - {(N-1)}^{n} + {(N-1)}^{n} - {(N-2)}^{n} + \ldots + 2^n - 1 + 1 - 0) \\
                                                        &= \dfrac{N^n}{N^n} = 1
    \end{align*}
    No hemos obteniedo lo que queríamos, puesto que queríamos que el resultado de la suma fuera $N^{n+1}$. Si tomamos sin embargo:
    \begin{equation*}
        h(t) = \dfrac{t^{n+1}-{(t-1)}^{n+1}}{t^n - {(t-1)}^{n}}
    \end{equation*}
    tenemos entonces que:
    \begin{align*}
        E[h(T)] &= \dfrac{1}{N^n}\sum_{t=1}^{N}h(t) (t^n-{(t-1)}^{n}) = \dfrac{1}{N^n}\sum_{t=1}^{N}t^{n+1}-{(t-1)}^{n+1} \\
                &= \dfrac{1}{N^n}(N^{n+1}-{(N-1)}^{n+1} + {(N-1)}^{n+1} + \ldots + 2^{n+1} - 1 + 1 -0) \\ 
                &= \dfrac{N^{n+1}}{N^n} = N
    \end{align*}
    Por lo que el estadístico $h(T)$ es insesgado para $N$. Nos falta comprobar si es un estimador y si tiene momento de segundo orden finito:
    \begin{itemize}
        \item No es estimador, puesto que $h(\mathbb{N})\nsubseteq \mathbb{N}$. Sin embargo, podemos realizar aproximaciones y quedarnos con el natural más próximo. Es decir, no va a haber un UMVUE pero podemos quedarnos con este estadístico, que es aquello que podemos conseguir más próximo a un UMVUE. % Literalmente dijo esto la profe.
        \item Para el momento de segundo orden, observamos que:
            \begin{equation*}
                E[{(h(T))}^{2}] = \dfrac{1}{N^n}\sum_{t=1}^{N}{(h(t))}^{2}(t^n-{(t-1)}^{n}) < \infty
            \end{equation*}
            al ser una suma finita.
    \end{itemize}
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ cuya función de densidad es de la forma
    \begin{equation*}
        f_\theta(x) = \dfrac{1}{2\sqrt{x\theta}}, \quad 0<x<\theta
    \end{equation*}
    Calcular, si existe, el UMVUE para $\theta$.\\

    \noindent
    En primer lugar, buscamos un estadístico suficiente y completo:
    \begin{equation*}
        f_\theta(x_1, \ldots, x_n) \stackrel{\text{iid.}}{=} \prod_{i=1}^{n}f_\theta(x_i) = \prod_{i=1}^{n}\dfrac{1}{2\sqrt{x_i\theta}} = \dfrac{1}{{\left(2\sqrt{\theta}\right)}^{n}} \prod_{i=1}^{n} \frac{1}{\sqrt{x_i}} \qquad x_i \in \left]0,\theta\right[
    \end{equation*}
    Si consideramos $0< X_{(1)} \leq X_{(n)} < \theta $, podemos escribir:
    \begin{equation*}
        f_\theta(x_1, \ldots, x_n) = \dfrac{I_{\left]0,+\infty\right[}(X_{(1)}-0) \cdot I_{\left]-\infty,0\right[}(X_{(n)}-\theta)}{{\left(2\sqrt{\theta}\right)}^{n}} \prod_{i=1}^{n}\dfrac{1}{\sqrt{x_i}}
    \end{equation*}
    Tomando:
    \begin{gather*}
        h(x_1, \ldots, x_n) = \prod_{i=1}^{n}\dfrac{1}{\sqrt{x_i}}, \qquad T(X_1, \ldots, X_n) = X_{(n)} \\
        g_\theta(t) = \dfrac{I_{\left]0,+\infty\right[}(X_{(1)}-0) \cdot I_{\left]-\infty,0\right[}(t-\theta)}{{\left(2\sqrt{\theta}\right)}^{n}} 
    \end{gather*}
    por el Teorema de factorización de Neymann-Fisher tenemos que el estadístico $X_{(n)}$ es suficiente para $\theta$. Notando $T=X_{(n)}$ para abreviar, calculamos la distribución de $T$:
    \begin{equation*}
        F_T(t) = {(F_X(t))}^{n} \quad \Longrightarrow \quad  f_T(t) = n{(F_X(t))}^{n-1}f_\theta(t)
    \end{equation*}
    Calculamos la distribución de $X$:
    \begin{equation*}
        F_X(t) = \int_{0}^{t} \dfrac{1}{2\sqrt{x\theta}}~dx = \dfrac{1}{2\sqrt{\theta}}\int_{0}^{t} \dfrac{1}{\sqrt{x}}~dx  = \sqrt{\frac{t}{\theta}} \qquad t \in \left]0,\theta\right[
    \end{equation*}
    Por lo que:
    \begin{equation*}
        f_T(t) = n{(F_X(t))}^{n-1}f_\theta(t) = n{\left(\sqrt{\frac{t}{\theta}}\right)}^{n-1}\frac{1}{2\sqrt{t\theta}} = \dfrac{n{\left(\sqrt{t}\right)}^{n-2}}{2{\left(\sqrt{\theta}\right)}^{n}}\qquad t\in \left]0,\theta\right[
    \end{equation*}
    Sea ahora $h$ una función medible de forma que:
    \begin{equation*}
        0 = E[h(T)] = \int_{0}^{\theta} h(t) f_T(t)~dt = \frac{n}{2{\left(\sqrt{\theta}\right)}^{n}}\int_{0}^{\theta} h(t){\left(\sqrt{t}\right)}^{n-2}~dt  \qquad \forall \theta\in \mathbb{R}^+
    \end{equation*}

    tenemos entonces que:
    \begin{equation*}
        \int_{0}^{\theta} h(t){\left(\sqrt{t}\right)}^{n-2}~dt = 0 \qquad \forall \theta\in \mathbb{R}^+
    \end{equation*}
    Sea ahora $H(t)$ una primitiva de $h(t){\left(\sqrt{t}\right)}^{n-2}$, tenemos entonces que:
    \begin{equation*}
        H(\theta) - H(0) = 0 \qquad \forall \theta\in \mathbb{R}^+
    \end{equation*}

    por lo que derivando respecto a $\theta$:
    \begin{equation*}
        h(\theta) {\left(\sqrt{\theta}\right)}^{n-2} = 0 \qquad \forall \theta\in \mathbb{R}^+ \quad \Longrightarrow \quad  h(\theta) = 0 \qquad \forall \theta\in \mathbb{R}^+
    \end{equation*}
    En conclusión, tenemos que:
    \begin{equation*}
        \mathbb{R}^+ \subseteq \{t:h(t) = 0\}
    \end{equation*}

    por lo que:
    \begin{equation*}
        1 \geq P[h(T) = 0] \geq P[T\in \mathbb{R}^+] = 1 \quad \Longrightarrow \quad P[h(T)=0] = 1
    \end{equation*}
    lo que demustra que $T$ es un estadístico completo. Ahora, tratamos de buscar un estimador insesgado para $\theta$, que será nuestro candidato a UMVUE. Buscamos una función medible $h$ de forma que:
    \begin{equation*}
        \theta = E[h(T)] = \dfrac{n}{2{\left(\sqrt{\theta}\right)}^{n}}\int_{0}^{\theta} h(t){\left(\sqrt{t}\right)}^{n-2}~dt 
    \end{equation*} 
    Observemos que:
    \begin{equation*}
        \int_{0}^{\theta}{\left(\sqrt{t}\right)}^{n} ~dt  = \dfrac{2}{n+2}\left[{\left(\sqrt{t}\right)}^{n+2}\right]_0^\theta = \dfrac{2}{n+2}{\left(\sqrt{\theta}\right)}^{n+2} = \dfrac{2}{n+2}{\left(\sqrt{\theta}\right)}^{n}\theta
    \end{equation*}
    Por lo que si tomamos:
    \begin{equation*}
        h(t) = \dfrac{n+2}{n} \cdot t
    \end{equation*}
    tenemos que:
    \begin{align*}
        E[h(T)] &= \int_{0}^{\theta} \dfrac{n+2}{n}t\frac{n}{2}\frac{{\left(\sqrt{t}\right)}^{n-2}}{{\left(\sqrt{\theta}\right)}^{n}}~dt  = \dfrac{n+2}{2{\left(\sqrt{\theta}\right)}^{n}} \int_{0}^{\theta}t{\left(\sqrt{t}\right)}^{n-2} ~dt  \\
                &= \dfrac{n+2}{2{\left(\sqrt{\theta}\right)}^{n}} \int_{0}^{\theta}{\left(\sqrt{t}\right)}^{n} ~dt   = \dfrac{n+2}{2{\left(\sqrt{\theta}\right)}^{n}} \dfrac{2}{n+2}{\left(\sqrt{\theta}\right)}^{n}\theta  = \theta \qquad \forall \theta\in \mathbb{R}^+
    \end{align*}
    Por lo que $h(T)$ es insesgado para $\theta$. Ahora, si $t\in \mathbb{R}^+$, tendremos entonces que:
    \begin{equation*}
        h(t) = \dfrac{n+2}{n}\cdot t \geq t \in \mathbb{R}^+ \quad  \Longrightarrow \quad  h(t) \in \mathbb{R}^+
    \end{equation*}
    por lo que $h(T)$ es además un estimador. Si calculamos ahora:
    \begin{align*}
        E\left[{(h(T))}^{2}\right] = \dfrac{n}{2{\left(\sqrt{\theta}\right)}^{n}} \int_{0}^{\theta} {(h(t))}^{2}{(\sqrt{t})}^{n-2}~dt 
    \end{align*}
    Tenemos la integral en un compacto de una función continua, por lo que sabemos que $E[{(h(T))}^{2}]<\infty$. En conclusión, tenemos por el Teorema de Lehmann-Scheffé que: $E[h(T)/T] = h(T)$ es el UMVUE.
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con función de densidad
    \begin{equation*}
        f_\theta(x) = \dfrac{\theta}{x^2}, \quad x>\theta > 0
    \end{equation*}
    Calcular, si existen, los UMVUE para $\theta$ y para $\nicefrac{1}{\theta}$.\\

    \noindent
    Calculamos en primer lugar un estadístico suficiente y completo para $\theta$:
    \begin{equation*}
        f_\theta(x_1,\ldots, x_n) \stackrel{\text{iid.}}{=} \prod_{i=1}^{n}f_\theta(x_i) =\prod_{i=1}^{n} \dfrac{\theta\cdot I_{\left]0,+\infty\right[}(X_{(1)}-\theta)}{x_i^2} = \theta^n \cdot I_{\left]0,+\infty\right[}(X_{(1)}-\theta)\prod_{i=1}^{n} \dfrac{1}{x_i^2}
    \end{equation*}
    Tomando:
    \begin{gather*}
        h(x_1, \ldots, x_n) = \prod_{i=1}^{n}\dfrac{1}{x_i^2}, \qquad T(X_1, \ldots, X_n) = X_{(1)} \\
        g_\theta(t) = \theta^n \cdot I_{\left]0,+\infty\right[}(t-\theta)
    \end{gather*}
    Podemos aplicar el Teorema de factorización de Neymann-Fisher, obteniendo que $T$ es suficiente para $\theta$. Notando $T = X_{(1)}$ para abreviar, calculamos la distribución de $T$:
    \begin{equation*}
        F_T(t) = 1-{(1-F_X(t))}^{n} \quad \Longrightarrow \quad f_T(t) = n{(1-F_X(t))}^{n-1}f_\theta(t)
    \end{equation*}
    Si calculamos:
    \begin{equation*}
        F_X(t) = \int_{\theta}^{t} \dfrac{\theta}{x^2}~dx  = \theta \int_{\theta}^{t} \dfrac{1}{x^2}~dx = \theta \left[\dfrac{-1}{x}\right]_\theta^t = \theta\left(\dfrac{-1}{t}+\dfrac{1}{\theta}\right) = \dfrac{t-\theta}{t} \qquad  t > \theta
    \end{equation*}
    Tenemos que:
    \begin{equation*}
        f_T(t) = n{\left(1-\dfrac{t-\theta}{t}\right)}^{n-1}\dfrac{\theta}{t^2} = n\cdot \dfrac{\theta^{n}}{t^{n+1}}
    \end{equation*}
    Sea $h$ una función medible de forma que:
    \begin{equation*}
        0 = E[h(T)] = \int_{\theta}^{+\infty} h(t)f_T(t)~dt  = \lim_{n\to\infty}\int_{\theta}^{n} \underbrace{h(t)\cdot n\cdot \frac{\theta^n}{t^{n+1}}}_{(\ast)}~dt  \qquad \forall \theta\in \mathbb{R}^+
    \end{equation*}
    Sea $H(t)$ una primitiva de $(\ast)$, tenemos entonces que:
    \begin{equation*}
         0 = \lim_{n\to\infty}H(n) - H(\theta) \qquad \forall \theta\in \mathbb{R}^+
    \end{equation*}

    con lo que derivando respecto $\theta$:
    \begin{equation*}
        0 = h(\theta) \cdot n \cdot \dfrac{\theta^n}{\theta^{n+1}} = h(\theta)\cdot \dfrac{n}{\theta} \quad \Longrightarrow \quad  h(\theta) = 0 \qquad \forall \theta\in \mathbb{R}^+
    \end{equation*}
    Por lo que:
    \begin{equation*}
        \mathbb{R}^+ \subseteq \{t:h(t) = 0\}
    \end{equation*}

    de donde:
    \begin{equation*}
        1 \geq P[h(T) = 0] \geq P[T\in \mathbb{R}^+] = 1 \quad \Longrightarrow \quad P[h(T)=0] = 1
    \end{equation*}
    Luego tenemos que $T$ es completo. Si calculamos la esperanza de $T$:
    \begin{align*}
        E[T] &= \int_{\theta}^{\infty} t\cdot n\cdot \dfrac{\theta^n}{t^{n+1}}~dt  = \int_{\theta}^{+\infty} \dfrac{n\theta^n}{t^n}~dt  = n\theta^n \int_{\theta}^{+\infty} \dfrac{1}{t^n}~dt  = \dfrac{-n\theta^n}{n-1} \left[\frac{1}{t^{n-1}}\right]_\theta^\infty \\
             &= \dfrac{n\theta^n}{(n-1)\theta^{n-1}} = \dfrac{n\theta}{n-1}
    \end{align*}
    Por lo que tomando:
    \begin{equation*}
        h(t) = \dfrac{n-1}{n}\cdot t
    \end{equation*}

    tenemos que:
    \begin{equation*}
        E[h(T)] = \dfrac{n-1}{n}E[T] = \theta
    \end{equation*}
    Es decir, $h(T)$ es un estadístico insesgado para $\theta$. Veamos que es estimador y que tiene momento de segundo orden:
    \begin{itemize}
        \item Si $t\in \mathbb{R}^+$, tenemos entonces que $h(t) \in \mathbb{R}^+_0$ para $n>1$, por lo que $h(T)$ es estimador.
        \item Calculamos para $n>2$:
            \begin{align*}
                E[{(h(T))}^{2}] &= \int_{\theta}^{+\infty} {(h(t))}^{2}\dfrac{n\theta^n}{t^{n+1}}~dt  = {\left(\dfrac{n-1}{n}\right)}^{2}\int_{\theta}^{+\infty} \dfrac{n\theta^n t^2}{t^{n+1}}~dt  \\ 
                                &= n\theta^n {\left(\dfrac{n-1}{n}\right)}^{2}\int_{\theta}^{+\infty} \dfrac{1}{t^{n-1}}~dt  = n\theta^n {\left(\dfrac{n-1}{n}\right)}^{2} \left[\frac{1}{(-n+2)t^{n-2}}\right]_\theta^{+\infty} \\
                                &= n\theta^n {\left(\dfrac{n-1}{n}\right)}^{2}\dfrac{1}{(n-2)\theta^{n-2}} = {\left(\dfrac{n-1}{n}\right)}^{2} \dfrac{n}{(n-2)}\theta^2 < \infty
            \end{align*}
    \end{itemize}
    Por lo que $h(T) = \frac{n-1}{n}\cdot t$ es UMVUE para $\theta$. Si buscamos ahora un UMVUE para $\nicefrac{1}{\theta}$, observamos que tomando $h(t) = \frac{n+1}{n}\cdot \frac{1}{t}$, tenemos que:
    \begin{align*}
        E[h(T)] &= \int_{\theta}^{+\infty} h(t) \dfrac{n\theta^n}{t^{n+1}}~dt  = (n+1)\theta^n \int_{\theta}^{+\infty} \frac{1}{t^{n+2}}~dt  = (n+1)\theta^n \left[\frac{1}{-(n+1)t^{n+1}}\right]_\theta^{+\infty} \\
                &= \dfrac{\theta^n}{\theta^{n+1}} = \dfrac{1}{\theta}
    \end{align*}
    por lo que $h(T)$ es insesgado para $T$. Observamos además que:
    \begin{itemize}
        \item Si $t\in \mathbb{R}^+$, entonces $h(t)\in \mathbb{R}^+$, por lo que $h(T)$ es un estimador.
        \item Calculamos si $n+3>1$:
            \begin{align*}
                E[{(h(T))}^{2}] &= \int_{\theta}^{+\infty} {(h(t))}^{2}\dfrac{n\theta^n}{t^{n+1}}~dt  = \dfrac{\theta^n{(n+1)}^{2}}{n}\int_{\theta}^{+\infty} \dfrac{1}{t^{n+3}}~dt  \\
                &= \dfrac{\theta^n{(n+1)}^{2}}{n} \left[\frac{1}{-(n+2)t^{n+2}}\right]_\theta^{+\infty} = \dfrac{{(n+1)}^{2}}{n(n+2)\theta^2} < \infty
            \end{align*}
    \end{itemize}
    Aplicando el Teorema de Lehmann-Scheffé, tenemos que $E[h(T)/T] = h(T)$ es UMVUE para $\nicefrac{1}{\theta}$.
\end{ejercicio}

\begin{ejercicio}\label{ej:7_rel4}
    Sea $X\rightsquigarrow P_\theta$ siendo $P_\theta$ una distribución con función de densidad
    \begin{equation*}
        f_\theta(x) = e^{\theta-x}, \quad x\geq \theta
    \end{equation*}
    Dada una muestra aleatoria simple de tamaño arbitrario, encontrar los UMVUE de $\theta$ y de $e^{\theta}$.\\

    \noindent
    Calculamos en primer lugar un estadístico suficiente y completo para $\theta$:
    \begin{equation*}
        f_\theta(x_1, \ldots, x_n) \stackrel{\text{iid.}}{=} \prod_{i=1}^{n}f_\theta(x_i) = \prod_{i=1}^{n} e^{\theta-x_i} \cdot I_{\left[0,+\infty\right[}(X_{(1)}-\theta)=I_{\left[0,+\infty\right[}(X_{(1)}-\theta) e^{n\theta}\prod_{i=1}^{n} e^{-x_i}
    \end{equation*}
    Tomando:
    \begin{gather*}
        h(x_1, \ldots, x_n) = \prod_{i=1}^{n}e^{-x_i}, \qquad T(X_1, \ldots, X_n) = X_{(1)} \\
        g_\theta(t) = I_{\left[0,+\infty\right[}(t-\theta) \cdot e^{n\theta}
    \end{gather*}
    Tenemos por el Teorema de factorización de Neymann-Fisher que $T=X_{(1)}$ es suficiente para $\theta$. Calculamos la distribución de $T$:
    \begin{equation*}
        F_T(t) = 1-{(1-F_X(t))}^{n} \quad \Longrightarrow \quad f_T(t) = n{(1-F_X(t))}^{n-1}f_\theta(t)
    \end{equation*}
    Si calculamos:
    \begin{equation*}
        F_X(t) = \int_{\theta}^{t} e^{\theta-x}~dx  = e^{\theta}\int_{\theta}^{t} e^{-x}~dx  = e^{\theta}[-e^{-x}]_\theta^t = e^{\theta}\left(e^{-\theta} - e^{-t}\right) = 1- e^{\theta-t}, \quad t\geq \theta
    \end{equation*}
    Tenemos entonces que:
    \begin{equation*}
        f_T(t) = n{\left(e^{\theta-t}\right)}^{n-1} e^{\theta-t} = n{\left(e^{\theta-t}\right)}^{n}
    \end{equation*}
    Sea $h$ una función medible de forma que:
    \begin{equation*}
        0 = E[h(T)] = \int_{\theta}^{+\infty} h(t)n{\left(e^{\theta-t}\right)}^{n}~dt  = ne^{n\theta} \int_{\theta}^{+\infty}h(t)e^{-nt} ~dt \qquad \forall \theta\in \mathbb{R}
    \end{equation*}
    Tenemos entonces que:
    \begin{equation*}
        0 = \lim_{n\to\infty}\int_{\theta}^{n} h(t)e^{-nt}~dt  \qquad \forall \theta\in \mathbb{R}
    \end{equation*}
    Sea $H(t)$ una primiriva de $h(t)e^{-nt}$, tenemos que:
    \begin{equation*}
        0 = \lim_{n\to\infty}H(n)-H(\theta)\qquad \forall \theta\in \mathbb{R}
    \end{equation*}

    derivando respecto $\theta$ obtenemos:
    \begin{equation*}
        0 = -h(\theta)e^{-n\theta} \quad \Longrightarrow \quad  h(\theta) = 0 \qquad \forall \theta\in \mathbb{R}
    \end{equation*}
    Por lo que:
    \begin{equation*}
        \mathbb{R}\subseteq \{t:h(t) = 0\}
    \end{equation*}

    de donde deducimos que $T$ es completo:
    \begin{equation*}
        1 \geq P[h(t) = 0] \geq P[T\in \mathbb{R}] = 1 \quad \Longrightarrow \quad P[h(t)=0] = 1
    \end{equation*}
    Buscamos ahora un estadístico insesgado para $\theta$, es decir, buscamos una función $h$ medible de forma que:
    \begin{equation*}
        \theta = E[h(T)] = \int_{\theta}^{+\infty} h(t) n{(e^{\theta-t})}^{n}~dt  = ne^{n\theta}\int_{\theta}^{+\infty} h(t)e^{-nt}~dt 
    \end{equation*}
    Sea $H(t)$ una primitiva de $h(t)e^{-nt}$, tenemos entonces que:
    \begin{equation*}
        \theta = ne^{n\theta}\left(\lim_{n\to\infty}H(n)-H(\theta )\right) \Longrightarrow \frac{\theta}{ne^{n\theta}} = \left(\lim_{n\to\infty}H(n)-H(\theta )\right)
    \end{equation*}
    y si derivamos a ambos lados:
    \begin{equation*}
        \dfrac{e^{-n\theta}}{n} - \theta e^{-n\theta} = -h(\theta)e^{-n\theta} \quad \Longrightarrow \quad  h(\theta) = \theta-\frac{1}{n}
    \end{equation*}
    Por lo que si tomamos:
    \begin{equation*}
        h(T) = T - \dfrac{1}{n}
    \end{equation*}
    Tenemos que $E[T] = \theta$, con lo que $h(T)$ es insesgado para $\theta$. Además, se trata de un estimador (puesto que el espacio paramétrico es $\mathbb{R}$). Finalmente, vemos que:
    \begin{equation*}
        E[{(h(T))}^{2}] = \int_{\theta}^{+\infty} {(h(t))}^{2}n{\left(e^{\theta-t}\right)}^{n}~dt  = ne^{n\theta} \int_{\theta}^{+\infty} \left(t^2 + \frac{1}{n^2}-\frac{2t}{n}\right)e^{-nt}~dt  
    \end{equation*}
    calculamos la integral a parte:
    \begin{align*}
        \int_{\theta}^{+\infty} \left(t^2 + \frac{1}{n^2}-\frac{2t}{n}\right)e^{-nt}~dt &= \left\{\begin{array}{ll}
                                u = {(t-\nicefrac{1}{n})}^{2} & du = 2t -\nicefrac{2}{n} \\
                                dv = e^{-nt} & v = \nicefrac{-e^{-nt}}{n}
                        \end{array}\right\} \\
                                                                                        &= \left[\frac{-{(t-\nicefrac{1}{n})}^{2}e^{-nt}}{n}\right]_\theta^{+\infty} +\frac{1}{n}\int_{\theta}^{+\infty} (2t-\nicefrac{2}{n})e^{-nt}~dt 
    \end{align*}

    tenemos que:
    \begin{equation*}
        \left[\frac{-{(t-\nicefrac{1}{n})}^{2}e^{-nt}}{n}\right]_\theta^{+\infty} < \infty
    \end{equation*}

    y volvemos a calcular a parte:
    \begin{align*}
        \int_{\theta}^{+\infty} (2t-\nicefrac{2}{n})e^{-nt}~dt  &= \left\{\begin{array}{ll}
                u = 2t-\nicefrac{2}{n} & du = 2 \\
                dv = e^{-nt} & v = \nicefrac{-e^{-nt}}{n}
        \end{array}\right\} \\
                                                                &= \left[\frac{-(2t-\nicefrac{2}{n})e^{-nt}}{n}\right]_\theta^{+\infty} +\frac{2}{n} \int_{\theta}^{+\infty} e^{-nt}~dt 
    \end{align*}

    tenemos que:
    \begin{equation*}
        \left[\frac{-(2t-\nicefrac{2}{n})e^{-nt}}{n}\right]_\theta^{+\infty}  < \infty
    \end{equation*}

    y también que:
    \begin{equation*}
        \frac{2}{n}\int_{\theta}^{+\infty}e^{-nt} ~dt = \frac{2}{n}\left[\frac{-e^{-nt}}{n}\right]_\theta^{+\infty} < \infty
    \end{equation*}
    En definitiva, $E[{(h(T))}^{2}] < \infty$, por lo que aplicando el Teorema de Lehmann-Scheffé, obtenemos que $E[h(T)/T] = h(T)$ es UMVUE.
\end{ejercicio}

\subsubsection{Ejercicios de estimadores eficientes}

\begin{ejercicio}
    Sea $X$ la variable que describe el número de fracasos antes del primer éxito en una sucesión de pruebas de Bernoulli con probabilidad de éxito $\theta\in \left]0,1\right[$, y sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de $X$.
    \begin{enumerate}[label=\alph*)]
        \item Probar que la familia de distribuciones de $X$ es regular y calcular la función de información asociada a la muestra.

            Según el enunciado, ``$X$ modela el número de fracasos antes del primer éxito en una sucesión de pruebas de Bernoulli con probabilidad de éxito $\theta\in \left]0,1\right[$'', por lo que $X$ sigue una distribución geométrica con probabilidad de éxito $\theta$, $X\rightsquigarrow G(\theta)$, $\theta \in \left]0,1\right[$. Para ver que la familia de distribuciones de $X$ es regular:
            \begin{enumerate}
                \item[$i)$] El espacio paramétrico es $\Theta = \left]0,1\right[$, un intervalo abierto de $\mathbb{R}$.
                \item[$ii)$] El espacio muestral es $\cc{X}=\mathbb{N}\cup \{0\}$, que no depende de $\theta$.
                \item[$iii)$] Para comprobar la tercera condición, recordamos que la función masa de probabilidad viene dada por:
                    \begin{equation*}
                        P_\theta[X=x] = {(1-\theta)}^{x}\theta
                    \end{equation*}
                    que es derivable respecto $\theta$. Calculamos la derivada del logaritmo de la función masa de probabilidad:
                    \begin{align*}
                        \ln P_\theta[X=x] &= \ln\left({(1-\theta)}^{x}\theta\right) = x\ln(1-\theta) + \ln\theta \\
                        \dfrac{\partial \ln P_\theta[X=x]}{\partial \theta} &= \dfrac{-x}{1-\theta} + \dfrac{1}{\theta} = \dfrac{1-\theta - x\theta}{\theta(1-\theta)} 
                    \end{align*}
                    Y si calculamos su esperanza:
                    \begin{equation*} % // TODO: Aqui paso de x a X por la cara, informarme sobre esto
                        E\left[\dfrac{\partial \ln P_\theta[X=x]}{\partial \theta}\right] = E\left[\dfrac{1-\theta - X\theta}{\theta(1-\theta)} \right] = \dfrac{1-\theta - \theta E[x]}{\theta(1-\theta)} \AstIg \dfrac{1-\theta - (1-\theta)}{\theta(1-\theta)} = 0
                    \end{equation*}
                    donde en $(\ast)$ usamos que $E[X] = \frac{1-\theta}{\theta}$. En definitiva, se cumple también la tercera condición de las familias exponenciales.
            \end{enumerate}
            Calculamos ahora la función de información asociada a la muestra:
            \begin{align*}
                I_X(\theta) &= Var\left(\dfrac{\partial \ln P_\theta[X=x]}{\partial \theta}\right) = Var\left(\dfrac{1-\theta - X\theta}{\theta(1-\theta)}\right) = \dfrac{\theta^2}{\theta^2{(1-\theta)}^{2}}Var(X) \\
                            &\AstIg \dfrac{1-\theta}{{(1-\theta)}^{2}\theta^2}  = \dfrac{1}{\theta^2(1-\theta)}
            \end{align*}
            donde en $(\ast)$ hemos usado que $Var(X) = \frac{1-\theta}{\theta^2}$.
        \item Especificar la clase de funciones paramétricas que admiten estimadores eficientes y los correspondientes estimadores.

            Una vez que sabemos que la familia es exponencial y que $0<I_X(\theta)<\infty$, lo que hacemos es buscar las funciones $a(\theta)$ y $g(\theta)$ que se usan en el enunciado del Teorema de caracterización de los estimadores eficientes. Para ello:
            \begin{align*}
                \dfrac{\partial \ln P_\theta[X_1=x_1, \ldots, X_n = x_n]}{\partial \theta} &= \sum_{i=1}^{n} \dfrac{\partial  \ln P_\theta[X=x_i]}{\partial  \theta} = \sum_{i=1}^{n} \left(\dfrac{1-\theta-x_i\theta}{\theta(1-\theta)}\right) \\
                                                                                           &= \dfrac{n(1-\theta) - \theta\sum\limits_{i=1}^n x_i}{\theta(1-\theta)} = \dfrac{\sum\limits_{i=1}^n x_i - \frac{n(1-\theta)}{\theta}}{\theta-1}
            \end{align*}
            Por lo que tomando:
            \begin{equation*}
                T(X_1, \ldots, X_n) = \sum_{i=1}^{n}X_i, \qquad g(\theta) = \dfrac{n(1-\theta)}{\theta}, \qquad a(\theta) = \frac{1}{\theta-1}
            \end{equation*}
            Tenemos que $a(\theta)\neq 0 \quad \forall \theta\in \left]0,1\right[$, que $g$ es derivable, con:
            \begin{equation*}
                g'(\theta) = \frac{-n}{\theta^2} \neq 0 \qquad \forall \theta\in \left]0,1\right[
            \end{equation*}
            Además, observamos que:
            \begin{equation*}
                a(\theta)g'(\theta) = \frac{-n}{\theta^2(\theta-1)} = \frac{n}{\theta^2(1-\theta)} = nI_X(\theta) = I_{(X_1, \ldots, X_n)}(\theta) \qquad \forall \theta\in \left]0,1\right[
            \end{equation*}
            Así como que $T$ es un estimador, pues: 
            \begin{equation*}
                T(\left]0,1\right[) = \left]0,n\right[ = g(\left]0,1\right[)
            \end{equation*}
            Finalmente, por un Corolario visto en teoría sabemos que las únicas funciones paramétricas que admiten estimadores eficientes son de la forma:
            \begin{equation*}
                a\cdot \frac{n(1-\theta)}{\theta} + b, \qquad a,b\in \mathbb{R}
            \end{equation*}
            y que sus estimadores eficientes son de la forma:
            \begin{equation*}
                a\cdot \sum_{i=1}^{n}X_i + b
            \end{equation*}
        \item Calcular la varianza de cada estimador eficiente y comprobar que coincide con las correspondiente cota de Fréchet-Cramér-Rao.

            Si calculamos ahora la varianza de $T=a\cdot \sum\limits_{i=1}^n X_i$ y la cota de Fréchet-Cramér-Rao:
            \begin{align*}
                Var(T) &= Var\left(a\cdot \sum_{i=1}^{n}X_i+b\right) \AstIg a^2\sum_{i=1}^{n}Var(X_i) = a^2\sum_{i=1}^{n} \left(\frac{1-\theta}{\theta^2}\right) = \frac{a^2n(1-\theta)}{\theta^2} \\
                \frac{{(ag'(\theta))}^{2}}{I_{(X_1,\ldots,X_n)}(\theta)} &= \frac{{\left(\frac{-an}{\theta^2}\right)}^{2}}{\frac{n}{\theta^2(1-\theta)}} = \frac{a^2n(1-\theta)}{\theta^2}
            \end{align*}
        \item Calcular, si existen, los UMVUE para $P_\theta[X=0]$ y para $E_\theta[X]$ y decir si son eficientes. % // TODO: HACER este apartado
            \begin{itemize}
                \item Para $P_\theta[X=0]$:
                    \begin{equation*}
                        P_\theta[X=0] = {(1-\theta)}^{0}\theta = \theta
                    \end{equation*}
                    Como ya hemos demostrado que $T = \sum\limits_{i=1}^{n}X_i$ es un estimador eficiente para $g(\theta)=\frac{n(1-\theta)}{\theta}$, tenemos automáticamente que $T$ es un estadístico suficiente. Tomando ahora $Q(\theta)$ como una primitiva de $a(\theta)$, por ejemplo $Q(\theta) = \ln(|\theta-1|)$, como:
                    \begin{equation*}
                        Q(\left]0,1\right[) = \left]-\infty,0\right[
                    \end{equation*}
                    claramente contiene un abierto de $\mathbb{R}$ concluimos que $T$ es completo. Buscamos ahora una función $h$ medible de forma que $h(T)$ sea insesgado para $\theta$, sabiendo que $T\rightsquigarrow BN(n,\theta)$, con función masa de probabilidad:
                    \begin{equation*}
                        P_\theta[T = t] = \binom{t+n-1}{t} {(1-\theta)}^{t}\theta^n
                    \end{equation*}
                    % \begin{equation*}
                    %     E[h(T)] = \sum_{t=0}^{\infty}h(t){(1-\theta)}^{t}\theta = \theta\sum_{t=0}^{\infty}h(t){(1-\theta)}^{t} = \theta \left(h(0) + \sum_{t=1}^{\infty}h(t){(1-\theta)}^{t}\right)
                    % \end{equation*}
                    % Por lo que tomando:
                    % \begin{equation*}
                    %     h(0) = 1, \qquad h(t) = 0 \quad \forall t\in \mathbb{N}\setminus\{0\}
                    % \end{equation*}
                    % tenemos que $E[h(T)] = \theta$.
                    % \begin{itemize}
                    %     \item Como $h$ solo toma valores $0$ o $1$ tenemos que es un estimador.
                    %     \item Para ver que su momento de segundo orden es finito:
                    %         \begin{equation*}
                    %             E[{(h(T))}^{2}] = \sum_{t=0}^{\infty}{(h(t))}^{2}{(1-\theta)}^{t}\theta = {(h(0))}^{2}\theta = \theta
                    %         \end{equation*}
                    % \end{itemize}
                    % Por el Teorema de Lehmann-Scheffé, $E[h(T)/T] = h(T)$ es UMVUE para $\theta$.
                \item Para $E_\theta[X]$:
                    % \begin{equation*}
                    %     E[X] = \frac{1-\theta}{\theta}
                    % \end{equation*}
                    % Por el apartado anterior tenemos ya que $T=\sum\limits_{i=1}^{n}X_i$ es un estadístico suficiente y completo. Buscamos ahora una función $h$ medible tal que $h(T)$ sea insesgado para $\frac{1-\theta}{\theta}$:
            \end{itemize}
        Finalmente, observamos que como $\theta$ y $\frac{1-\theta}{\theta}$ no están en la clase de funciones paramétricas que admiten estimadores eficientes, estos no pueden ser eficientes.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}\label{ej:9_rel4} % // TODO: HACER
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con distribución exponencial.
    \begin{enumerate}[label=\alph*)]
        \item Probar que la familia de distribuciones de $X$ es regular.

            Supuesto que $X\rightsquigarrow exp(\lm)$ con $\lm\in \mathbb{R}^+$, la función de densidad de $X$ viene dada por:
            \begin{equation*}
                f_\lm(x) = \lm e^{-\lm x} \qquad x\geq 0
            \end{equation*}
            Y se cumple:
            \begin{equation*}
                E[X] = \frac{1}{\lm}, \qquad Var(X) = \frac{1}{\lm^2}
            \end{equation*}
            Comprobamos cada una de las propiedades:
            \begin{enumerate}
                \item[$i)$] El espacio paramétrico es $\Theta=\mathbb{R}^+$, intervalo abierto de $\mathbb{R}$.
                \item[$ii)$] El espacio muestral es $\mathbb{R}^+$, que no depende de $\lm$.
                \item[$iii)$] Para la tercera, observamos que $f_\lm$ es derivable respecto $\lm$, con:
                    \begin{equation*}
                        \ln f_\lm(X) = \ln\left(\lm e^{-\lm X}\right) = \ln\lm - \lm X \quad \Longrightarrow \quad  \dfrac{\partial \ln f_\lm(X)}{\partial \lm} = \frac{1}{\lm} - X
                    \end{equation*}
                    Y si calculamos:
                    \begin{equation*}
                        E\left[\dfrac{\partial \ln f_\lm(X)}{\partial \lm}\right] = E\left[\frac{1}{\lm}-X\right] = \frac{1}{\lm}-E[X] = \frac{1}{\lm}-\frac{1}{\lm} = 0
                    \end{equation*}
            \end{enumerate}
        \item Encontrar la clase de funciones paramétricas que admiten estimador eficiente y el estimador correspondiente. Calcular la varianza de estos estimadores.

            Calculamos primero la función de información de Fisher:
            \begin{equation*}
                I_X(\lm) = Var\left(\dfrac{\partial \ln f_\lm(X)}{\partial \lm}\right) = Var\left(\frac{1}{\lm}-X\right) = Var(X) = \frac{1}{\lm^2}
            \end{equation*}
            que como vemos verifica $0<I_X(\lm) <\infty$. Buscamos ahora las funciones $a$ y $g$:
            \begin{equation*}
                \dfrac{\partial \ln f_\lm^n(x_1, \ldots, x_n)}{\partial \lm} = \sum_{i=1}^{n}\dfrac{\partial \ln f_\lm(x_i)}{\partial \lm} = \sum_{i=1}^{n} \left(\frac{1}{\lm}-x_i\right) = \frac{n}{\lm} - \sum_{i=1}^{n}x_i = -\left(\sum_{i=1}^{n}x_i - \frac{n}{\lm}\right)
            \end{equation*}
            Por lo que tomando:
            \begin{equation*}
                T(X_1, \ldots, X_n) = \sum_{i=1}^{n} X_i, \qquad g(\lm) = \frac{n}{\lm}, \qquad a(\lm) = -1
            \end{equation*}
            Tenemos que $a(\lm) \neq 0 \quad \forall \lm\in \mathbb{R}^+$, que $g$ es derivable con:
            \begin{equation*}
                g'(\lm) = \frac{-n}{\lm^2} \neq 0 \qquad \forall \lm\in \mathbb{R}^+
            \end{equation*}
            y que $T$ es un estimador, pues $T(\mathbb{R}^+) = \mathbb{R}^+ = g(\mathbb{R}^+)$. Finalmente, falta comprobar que:
            \begin{equation*}
                a(\lm)g'(\lm) = \frac{n}{\lm^2} = nI_X(\lm) = I_{(X_1, \ldots, X_n)}(\lm)
            \end{equation*}
            Por lo que podemos deducir ya por el Teorema de caracterización de los estimadores eficientes que $T$ es un estimador eficiente. Más aún, por un Corolario del mismo, tenemos que las únicas funciones paramétricas de dicha familia que admiten estimadores eficientes son de la forma:
            \begin{equation*}
                a\cdot \frac{n}{\lm}+b, \qquad a,b\in \mathbb{R}
            \end{equation*}
            y que dichos estimadores son $a\cdot T+b$. Nos disponemos a calcular la varianza de estos estimadores:
            \begin{equation*}
                Var\left(a\cdot T+b\right) = a^2 Var\left(\sum_{i=1}^{n}X_i\right) = a^2\sum_{i=1}^{n}Var(X) = a^2 \sum_{i=1}^{n} \left(\frac{1}{\lm^2}\right) = \frac{na^2}{\lm^2}
            \end{equation*}
            Veamos que coincide con la cota de Fréchet-Cramér-Rao, para comprobar nuestro trabajo calculando la familia de estimadores eficientes:
            \begin{equation*}
                \dfrac{{(ag'(\lm))}^{2}}{I_{(X_1,\ldots,X_n)}(\lm)} = \dfrac{{\left(\frac{-an}{\lm^2}\right)}^{2}}{\frac{n}{\lm^2}} = \frac{na^2}{\lm^2} = Var(a\cdot T+b)
            \end{equation*}
        \item Basándose en el apartado anterior, encontrar el UMVUE para la media de $X$.

            Como ya sabemos que $T=\sum\limits_{i=1}^{n}X_i$ es un estimador eficiente para $g(\lm)$, tenemos automáticamente que $T$ es suficiente. Para ver que $T$ es completo, sea $Q$ una primitiva de $a$, podemos tomar $Q(\lm) = -\lm$, tenemos que claramente $Q(\mathbb{R}^+)$ contiene un abierto de $\mathbb{R}$, por lo que $T$ es completo. 

            Como en el apartado anterior vimos que $T$ es eficiente para $g(\lm) = \frac{n}{\lm}$, tenemos entonces que $\frac{T}{n}$ es eficiente para $\tilde{g}(\lm) = \frac{g(\lm)}{n} = \frac{1}{\lm}$. Como la correspondencia $t \mapsto \frac{t}{n}$ es biunívoca, $\frac{T}{n}$ también será suficiente y completo (para ver que es completo no hace falta que la corresponencia sea unívoca). Por un Corolario del Teorema de caracterización de estimadores eficientes tenemos que $T$ es UMVUE para $\frac{1}{\lm} = E[X]$.

        \item Dar la cota de Fréchet-Cramér-Rao para la varianza de estimadores insesgados y regulares de $\lm^3$. ¿Es alcanzable dicha cota?

            Si tomamos $g(\lm) = \lm^3$, la cota de Fréchet-Cramér-Rao tiene la forma:
            \begin{equation*}
                \frac{{(g'(\lm))}^{2}}{I_{(X_1,\ldots,X_n)}(\lm)} = \frac{{(3\lm^2)}^{2}}{\frac{n}{\lm^2}} = \dfrac{9\lm^6}{n}
            \end{equation*}
            y como $g(\lm)$ no es de la forma $a\cdot \frac{n}{\lm}+b$, para ciertos $a,b\in \mathbb{R}$, sabemos que esta cota no es alcanzable.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}\label{ej:10_rel4} % // TODO: HACER
    Sea $X$ una variable aleatoria con función de densidad de la forma
    \begin{equation*}
        f_\theta(x) = \theta x^{\theta-1}, \quad 0<x<1
    \end{equation*}
    \begin{enumerate}[label=\alph*)]
        \item Sabiendo que $E_\theta[\ln X] = -\frac{1}{\theta}$ y $Var_\theta[\ln X] = \frac{1}{\theta^2}$, comprobar que esta familia de distribuciones es regular.

            Veamos las propiedades, suponiendo que $\theta > 0$.
            \begin{enumerate}
                \item[$i)$] El espacio paramétrico es $\Theta = \mathbb{R}^+$, intervalo abierto de $\mathbb{R}$.
                \item[$ii)$] El espacio muestral es $\cc{X} = \left]0,1\right[$, que no depende de $\theta$.
                \item[$iii)$] La función de densidad es derivable respecto $\theta$, calculamos la derivada del logaritmo de la función de densidad:
                    \begin{equation*}
                        \ln f_\theta(X) = \ln(\theta X^{\theta-1}) = \ln\theta + (\theta-1)\ln X \quad \Longrightarrow \quad \dfrac{\partial \ln f_\theta(X)}{\partial \theta} = \frac{1}{\theta} + \ln X
                    \end{equation*}
                    Con lo que:
                    \begin{equation*}
                        E\left[\dfrac{\partial \ln f_\theta(X)}{\partial \theta}\right] = E\left[\frac{1}{\theta} + \ln X\right] = \frac{1}{\theta} + E[\ln X] = \frac{1}{\theta} - \frac{1}{\theta} = 0
                    \end{equation*}
            \end{enumerate}
        \item Basándose en una muestra aleatoria simple de $X$, dar la clase de funciones paramétricas con estimador eficiente, los estimadores y su varianza.

            Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de $X$, en primer lugar calculamos la función de información de Fisher:
            \begin{equation*}
                I_X(\theta) = Var\left(\dfrac{\partial \ln f_\theta(X)}{\partial \theta}\right) = Var\left(\frac{1}{\theta} + \ln X\right) = Var\left(\ln X\right) = \frac{1}{\theta^2}
            \end{equation*}
            y observamos que $0<I_X(\theta) < \infty$. Ahora, buscamos las funciones $g$ y $a$:
            \begin{equation*}
                \dfrac{\partial \ln f_\theta^n(x_1, \ldots, x_n)}{\partial \theta} = \sum_{i=1}^{n} \dfrac{\partial  \ln f_\theta(x_i)}{\partial \theta} = \sum_{i=1}^{n} \left(\frac{1}{\theta}+\ln x_i\right) = \frac{n}{\theta} + \sum_{i=1}^{n}\ln x_i 
            \end{equation*}
            Tomando:
            \begin{equation*}
                T(X_1, \ldots, X_n) = \sum_{i=1}^{n}\ln X_i, \qquad g(\theta) = \frac{-n}{\theta}, \qquad a(\theta) = 1
            \end{equation*}

            tenemos que $a(\theta)\neq 0$ para todo $\theta\in \mathbb{R}^+$, que $g$ es derivable con:
            \begin{equation*}
                g'(\theta) = \frac{n}{\theta^2} \neq 0 \qquad \forall \theta\in \mathbb{R}^+
            \end{equation*}

            además de que $T$ es un estimador, puesto que:
            \begin{equation*}
                T({(\left]0,1\right[)}^{n}) \subseteq \left]-\infty,0\right[ = g(\mathbb{R}^+)
            \end{equation*}
            Comprobemos que:
            \begin{equation*}
                a(\theta)g'(\theta) = \frac{n}{\theta^2} = nI_X(\theta) = I_{(X_1, \ldots, X_n)}(\theta)
            \end{equation*}
            Por lo que $T$ es un estimador eficiente para $g(\theta) = \frac{-n}{\theta}$, luego las únicas funciones paramétricas que admiten estimadores eficientes son las de la forma:
            \begin{equation*}
                a\cdot \frac{-n}{\theta} + b, \qquad a,b\in \mathbb{R}
            \end{equation*}
            cuyos estimadores eficientes son:
            \begin{equation*}
                a\cdot T + b
            \end{equation*}
            Calculemos sus varianzas y veamos que coinciden con la cota de Fréchet-Cramér-Rao:
            \begin{align*}
                Var(a\cdot T+b) &= a^2 Var\left(\sum_{i=1}^{n}\ln X_i\right) \stackrel{\text{indep.}}{=} a^2 \sum_{i=1}^{n}Var(\ln X_i) = a^2 \sum_{i=1}^{n} \frac{1}{\theta^2} = \frac{a^2n}{\theta^2} \\
                \frac{{(ag'(\theta))}^{2}}{I_{(X_1,\ldots, X_n)}(\theta)} &= \frac{{(\frac{an}{\theta^2})}^{2}}{\frac{n}{\theta^2}} = \frac{a^2n}{\theta^2}
            \end{align*}
    \end{enumerate}
\end{ejercicio}
