\newpage
\section{Estimación puntual. Insesgadez y mínima varianza}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra de una variable $X\rightsquigarrow\cc{N}(\mu, \sigma^2)$ con $\mu\in \mathbb{R}$, $\sigma\in \mathbb{R}^+$. Probar que
    \begin{equation*}
        T(X_1, \ldots, X_n) = \left\{\begin{array}{ll}
            1 & \text{si\ } \overline{X}\leq 0 \\
            0 & \text{si\ } \overline{X} > 0
        \end{array}\right. 
    \end{equation*}
    es un estimador insesgado de la función paramétrica $\Phi\left(\frac{-\mu \sqrt{n}}{\sigma}\right)$, siendo $\Phi$ la función de distribución de la $\cc{N}(0,1)$.\\

    \noindent
    Tenemos $T(X_1, \ldots, X_n) = I_{\left]-\infty,0\right]}(\overline{X})$. Como $X\rightsquigarrow \cc{N}(\mu, \sigma^2)$, sabemos por lo visto en el Tema 1 que entonces:
    \begin{equation*}
        \overline{X} \rightsquigarrow \cc{N}\left(\mu, \frac{\sigma^2}{n}\right)
    \end{equation*}

    de donde (escribiendo $T=T(X_1,\ldots,X_n)$):
    \begin{equation*}
    T = I_{\left]-\infty,0\right]}(\overline{X}) \rightsquigarrow B(1,P[\overline{X}\leq 0])
    \end{equation*}

    estamos ya en condiciones de ver que $T$ es insesgado para dicha función:
    \begin{equation*}
        E[T] \AstIg P[\overline{X}\leq 0] \stackrel{\text{tipif.}}{=} P\left[Z \leq \dfrac{-\mu\sqrt{n}}{\sigma}\right] = \Phi\left(\dfrac{-\mu\sqrt{n}}{\sigma}\right) 
    \end{equation*}
    donde en $(\ast)$ usamos que conocemos bien la esperanza de una distribución Bernoulli.
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de $X\rightsquigarrow B(1,p)$ con $p\in \left]0,1\right[$ y sea $T=\sum\limits_{i=1}^{n}X_i$.
    \begin{enumerate}[label=\alph*)]
        \item Probar que si $k\in \mathbb{N}$ y $k\leq n$, el estadístico
            \begin{equation*}
                \dfrac{T(T-1)\cdot \ldots\cdot (T-k+1)}{n(n-1)\cdot \ldots\cdot (n-k+1)}
            \end{equation*}
            es un estimador insesgado de $p^k$. ¿Es este estimador el UMVUE?.
        \item Probar que si $k>n$, no existe ningún estimador insesgado para $p^k$.
        \item ¿Puede afirmarse que $\frac{T}{n}{\left(1-\frac{T}{n}\right)}^{2}$ es insesgado para $p{(1-p)}^{2}$? 
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable $X\rightsquigarrow \cc{P}(\lm)$ con $\lm\in \mathbb{R}^+$. Encontrar, si existe, el UMVUE para $\lm^s$, siendo $s\in \mathbb{N}$ arbitrario.\\
    
    \noindent
    Veamos que $T(X_1, \ldots, X_n) = \sum\limits_{i=1}^{n}X_i$ es un estadístico suficiente y completo. Para ello, recordemos que $\{\cc{P}(\lm) : \lm>0\}$ es una familia exponencial:
    \begin{enumerate}
        \item El espacio paramétrico es $\mathbb{R}^+ \subseteq \mathbb{R}$.
        \item El espacio muestral es $\cc{X}=\mathbb{N}\cup \{0\}$, que no depende de $\lm$.
        \item Observamos que:
            \begin{align*}
                P_\lm[X=x] &= e^{-\lm}\dfrac{\lm^x}{x!} = exp\left[\ln\left(e^{-\lm}\dfrac{\lm^x}{x!}\right)\right] = exp\left(-\lm + x\ln\lm -\ln(x!)\right)
            \end{align*}
            por lo que basta tomar:
            \begin{equation*}
                Q(\lm) = \ln\lm, \qquad T(x) = x \qquad D(\lm) = -\lm, \qquad S(x) = -\ln(x!)
            \end{equation*}
    \end{enumerate}
    En consecuencia, por un Teorema visto en teoría, tenemos que el estadístico:
    \begin{equation*}
        T(X_1, \ldots, X_n) = \sum_{i=1}^{n}T(X_i) = \sum_{i=1}^{n}X_i
    \end{equation*}
    es suficiente y completo para $\lm$. Observemos que por la reproductividad de la Poisson tenemos que (notando $T=T(X_1, \ldots, X_n)$): 
    \begin{equation*}
        T\rightsquigarrow \cc{P}\left(\sum_{i=1}^{n}\lm\right) \equiv \cc{P}(n\lm)
    \end{equation*}
    Ahora, para buscar el UMVUE, buscamos una función $h$ medible de forma que:
    \begin{equation*}
        \lm^s = E[h(T)] = \sum_{t\in \mathbb{N}\cup \{0\}}h(t)P[T=t] = \sum_{t\in \mathbb{N}\cup \{0\}} h(t) e^{-n\lm} \dfrac{{(n\lm)}^{t}}{t!} 
    \end{equation*}

    por lo que:
    \begin{equation*}
        \lm^s e^{n\lm} = \sum_{t\in \mathbb{N}\cup \{0\}} h(t)\dfrac{{(n\lm)}^{t}}{t!}
    \end{equation*}

    y si aplicamos el desarrollo en serie de la exponencial, obtenemos:
    \begin{equation*}
        \lm^s \sum_{t\in \mathbb{N}\cup \{0\}} \dfrac{{(n\lm)}^{t}}{t!} = \lm^s e^{n\lm} = \sum_{t\in \mathbb{N}\cup\{0\}} h(t) \dfrac{{(n\lm)}^{t}}{t!} 
    \end{equation*}

    si desarrollamos cada uno de los términos:
    \begin{equation*}
        \lm^s + \lm^{s+1}n + \frac{\lm^{s+2}n^2}{2!} + \ldots = h(0) + h(1)(n\lm) + \ldots + h(s)\dfrac{{(n\lm)}^{s}}{s!} + \ldots
    \end{equation*}

    observamos que tomando:
    \begin{gather*}
        h(0) = \ldots = h(s-1) = 0,\quad  h(s) = \dfrac{s!}{n^s}\\h(s+1) = \dfrac{(s+1)!}{n^s}, \quad \ldots \quad  h(s+k) = \dfrac{(s+k)!}{n^s k!}
    \end{gather*}

    es decir:
    \begin{equation*}
        h(T) = \left\{\begin{array}{ll}
            0 & \text{si\ } T<s \\
            \dfrac{T!}{n^s(T-s)!}& \text{si\ } T\geq s
        \end{array}\right. 
    \end{equation*}
    tenemos que $h(T)$ es insesgado para $\lm^s$. Es claro además que $h(t)\in \mathbb{R}^+$ para cualquier valor de $t$, con lo que $h(T)$ es un estimador de $\lm^s$. Finalmente, observemos que:
    \begin{align*}
        E\left[{(h(T))}^{2}\right] &= \sum_{t\in \mathbb{N}\cup \{0\}} {(h(t))}^{2}P[T=t] = \sum_{t\geq s} {\left(\dfrac{t!}{n^s(t-s)!}\right)}^{2} e^{-n\lm} \dfrac{{(n\lm)}^{t}}{t!} \\ &= \dfrac{1}{n^s e^{n\lm}} \sum_{t\geq s} \dfrac{{(n\lm)}^{t}t!}{{((t-s)!)}^{2}}
    \end{align*}

    como:
    \begin{equation*}
        \dfrac{\dfrac{{(n\lm)}^{t+1}(t+1)!}{{((t+1-s)!)}^{2}}}{\dfrac{{(n\lm)}^{t}t!}{{((t-s)!)}^{2}}} = \dfrac{{(n\lm)}^{t+1}(t+1)!{((t-s)!)}^{2}}{{(n\lm)}^{t}t!{((t+1-s)!)}^{2}} = \dfrac{n\lm(t+1)}{{(t+1-s)}^{2}} \to 0 < 1
    \end{equation*}
    por el Criterio del cociente, tenemos que:
    \begin{equation*}
        \sum_{t\geq s} \dfrac{{(n\lm)}^{t}t!}{{((t-s)!)}^{2}} < \infty \Longrightarrow E\left[{(h(T))}^{2}\right]  =\dfrac{1}{n^s e^{n\lm}}\sum_{t\geq s} \dfrac{{(n\lm)}^{t}t!}{{((t-s)!)}^{2}}   < \infty
    \end{equation*}
    en consecuencia, tenemos que $h(T)$ es un estimador insesgado para $\lm^s$ y de momento de segundo orden finito y es función de un estadístico suficiente y completo, con lo que el Teorema de Lehmann-Scheffé nos dice que:
    \begin{equation*}
        E[h(T)/T] = h(T)
    \end{equation*}
    es un UMVUE para $\lm^s$.
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable con distribución uniforme discreta en los puntos $\{1,\ldots,N\}$, siendo $N$ un número natural arbitrario. Encontrar el UMVUE para $N$.\\

    \noindent
    En el Ejercicio~\ref{ej:3.5} vimos que $T(X_1, \ldots, X_n) = X_{(n)}$ era un estadístico suficiente y completo. Si notamos $T = T(X_1,\ldots, X_n)$, tenemos que:
    \begin{equation*}
        F_T(t) = {(F_X(t))}^{n} \Longrightarrow P[T=t] = P[T\leq t] - P[T\leq t-1] = {(F_X(t))}^{n}-{(F_X(t-1))}^{n}
    \end{equation*}
    como $F_X(t) = \frac{t}{N}$, tenemos:
    \begin{equation*}
        P[T=t] = {(F_X(t))}^{n}-{(F_X(t-1))}^{n} = \dfrac{t^n - {(t-1)}^{n}}{N^n}
    \end{equation*} % // TODO: TERMINAR
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ cuya función de densidad es de la forma
    \begin{equation*}
        f_\theta(x) = \dfrac{1}{2\sqrt{x\theta}}, \quad 0<x<\theta
    \end{equation*}
    Calcular, si existe, el UMVUE para $\theta$.
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con función de densidad
    \begin{equation*}
        f_\theta(x) = \dfrac{\theta}{x^2}, \quad x>\theta
    \end{equation*}
    Calcular, si existen, los UMVUE para $\theta$ y para $\nicefrac{1}{\theta}$.
\end{ejercicio}

\begin{ejercicio}
    Sea $X\rightsquigarrow P_\theta$ siendo $P_\theta$ una distribución con función de densidad
    \begin{equation*}
        f_\theta(x) = e^{\theta-x}, \quad x\geq \theta
    \end{equation*}
    Dada una muestra aleatoria simple de tamaño arbitrario, encontrar los UMVUE de $\theta$ y de $e^{\theta}$.
\end{ejercicio}

\begin{ejercicio}
    Sea $X$ la variable que describe el número de fracasos antes del primer éxito en una sucesión de pruebas de Bernoulli con probabilidad de éxito $\theta\in \left]0,1\right[$, y sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de $X$.
    \begin{enumerate}[label=\alph*)]
        \item Probar que la familia de distribuciones de $X$ es regular y calcular la función de infor- mación asociada a la muestra.
        \item Especificar la clase de funciones paramétricas que admiten estimadores eficientes y los correspondientes estimadores.
        \item Calcular la varianza de cada estimador eficiente y comprobar que coincide con las correspondiente cota de Fréchet-Cramér-Rao.
        \item Calcular, si existen, los UMVUE para $P_\theta[X=0]$ y para $E_\theta[X]$ y decir si son eficientes.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con distribución exponencial.
    \begin{enumerate}[label=\alph*)]
        \item Probar que la familia de distribuciones de $X$ es regular.
        \item Encontrar la clase de funciones paramétricas que admiten estimador eficiente y el estimador correspondiente. Calcular la varianza de estos estimadores.
        \item Basándose en el apartado anterior, encontrar el UMVUE para la media de $X$.
        \item Dar la cota de Fréchet-Cramér-Rao para la varianza de estimadores insesgados y regulares de $\lm^3$. ¿Es alcanzable dicha cota?
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $X$ una variable aleatoria con función de densidad de la forma
    \begin{equation*}
        f_\theta(x) = \theta x^{\theta-1}, \quad 0<x<1
    \end{equation*}
    \begin{enumerate}[label=\alph*)]
        \item Sabiendo que $E_\theta[\ln X] = -\frac{1}{\theta}$ y $Var_\theta[\ln X] = \frac{1}{\theta^2}$, comprobar que esta familia de distribuciones es regular.
        \item Basándose en una muestra aleatoria simple de $X$, dar la clase de funciones paramétricas con estimador eficiente, los estimadores y su varianza.
    \end{enumerate}
\end{ejercicio}
