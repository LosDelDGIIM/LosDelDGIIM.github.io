\newpage
\section{Estimación de máxima verosimilitud y otros métodos}
\begin{ejercicio}
    Sea $X\rightsquigarrow P_\theta$ con $\theta\in \mathbb{R}$ siendo $P_\theta$ una distribución con función de densidad
    \begin{equation*}
        f_\theta(x) = e^{\theta-x}, \qquad x\geq \theta
    \end{equation*}
    Dada una muestra aleatoria simple de tamaño $n$, encontrar los estimadores máximo verosímiles de $\theta$ y de $e^\theta$. Basándose en los resultados del Ejercicio~\ref{ej:7_rel4}, decir si estos estimadores son insesgados.\\

    \noindent
    Sea $(X_1, \ldots, X_n)$ una m.a.s. de $X\rightsquigarrow  P_\theta$, tenemos que:
    \begin{equation*}
        f^n_\theta(x_1, \ldots, x_n) \stackrel{\text{iid.}}{=} \prod_{i=1}^{n}f_\theta(x_i) = \prod_{i=1}^{n} e^{\theta-x} = e^{n\theta - \sum\limits_{i=1}^n x_i} \qquad \forall x_i \geq \theta
    \end{equation*}
    Por lo que si consideramos $x_{(1)} = \min\limits_{i \in \{1,\ldots,n\}}\{x_i\}$:
    \begin{equation*}
        L_{x_1,\ldots, x_n}(\theta) = f^n_\theta(x_1,\ldots,x_n) = \left\{\begin{array}{ll}
                e^{n\theta - \sum\limits_{i=1}^n x_i}  & \text{si\ } \theta \leq x_{(1)}  \\
             0 & \text{en otro caso\ } 
        \end{array}\right. 
    \end{equation*}
    Observamos que $L_{x_1,\ldots,x_n}(\theta)$ es una función creciente en $\left]-\infty,x_{(1)}\right]$, por lo que alcanza su máximo en $x_{(1)}$. Como el espacio paramétrico es $\Theta = \mathbb{R}$, tenemos que $T(X_1, \ldots, X_n) = X_{(1)}$ es un estimador de $\theta$, con lo que es el EMV, al maximizar la función de verosimilitud. Además, el Teorema de Zehna nos dice que:
    \begin{equation*}
        \widehat{e^\theta} = e^{\hat{\theta}} = e^{X_{(1)}}
    \end{equation*}
    Por lo que $e^{X_{(1)}}$ es EMV para $e^{\theta}$. % // TODO: TERMINAR luego
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con distribución exponencial. Basándose en los resultados del Ejercicio~\ref{ej:9_rel4}, encontrar los estimadores máximo verosímiles de la media y de la varianza de $X$.\\

    \noindent
    Sea $X\rightsquigarrow exp(\lm)$ con $\lm\in \mathbb{R}^+$, nos piden encontrar EMVs para:
    \begin{equation*}
        E[X] = \frac{1}{\lm}, \qquad Var(X) = \frac{1}{\lm^2}
    \end{equation*}
    En el Ejercicio~\ref{ej:9_rel4} obtuvimos que las únicas funciones paramétricas de dicha familia de distribuciones que admiten estimadores eficientes son las de la forma:
    \begin{equation*}
        a\cdot \frac{n}{\lm} +b \qquad a,b\in \mathbb{R}, \quad a\neq 0
    \end{equation*}
    y sus estimadores eficientes son:
    \begin{equation*}
        a\cdot \sum_{i=1}^{n}X_i + b
    \end{equation*}
    Tomando $a=\frac{1}{n}, b=0$, obtenemos que el estimador $T=\frac{\sum\limits_{i=1}^nX_i}{n}$ es eficiente para $\nicefrac{1}{\lm}$, por lo que un Teorema visto en teoría nos asegura que $T$ es el único EMV que podemos considerar para $\nicefrac{1}{\lm}$. Para $\nicefrac{1}{\lm^2}$ podemos aplicar el Teorema de Zehna, obteniendo:
    \begin{equation*}
        \widehat{{\left(\frac{1}{\lm^2}\right)}} = \frac{1}{{\left(\hat{\lm}\right)}^{2}} = \frac{n}{{\left(\sum\limits_{i=1}^{n}X_i\right)}^{2}}
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}
    Sea $X$ una variable aleatoria con función de densidad de la forma
    \begin{equation*}
        f_\theta(x) = \theta x^{\theta -1}, \qquad 0<x<1
    \end{equation*}
    \begin{enumerate}[label=\alph*)]
        \item Calcular un estimador máximo verosímil para $\theta$.

            Maximizamos la función de verosimilitud de $\theta$:
            \begin{equation*}
                L_{x_1,\ldots,x_n}(\theta) = f^n_\theta(x_1, \ldots, x_n) \stackrel{\text{iid.}}{=}\prod_{i=1}^{n}f_\theta(x_i) = \prod_{i=1}^{n} \theta x_i^{\theta-1} = \theta^n \prod_{i=1}^{n} x_i^{\theta -1}
            \end{equation*}
            Si aplicamos logaritmo no cambia la abscisa a maximizar:
            \begin{equation*}
                \ln L_{x_1,\ldots,x_n}(\theta) = n\ln \theta + (\theta -1)\sum_{i=1}^{n} \ln x_i
            \end{equation*}
            hayamos los puntos críticos:
            \begin{equation*}
                \dfrac{\partial \ln L_{x_1,\ldots,x_n}(\theta)}{\partial  \theta} = \frac{n}{\theta} + \sum_{i=1}^{n}\ln x_i = \frac{n + \theta \sum\limits_{k=1}^{n}\ln x_i}{\theta} = 0 \Longleftrightarrow \theta = \frac{-n}{\sum\limits_{k=1}^{n}\ln x_i}
            \end{equation*}
            Por lo que tomando:
            \begin{equation*}
                \hat{\theta}(X_1, \ldots, X_n) = \frac{-n}{\sum\limits_{i=1}^{n} \ln X_i}
            \end{equation*}
            tenemos que $\hat{\theta}$ es un estimador de $\theta$, luego es EMV para $\theta$.
        \item Deducir dicho estimador a partir de los resultados del Ejercicio~\ref{ej:10_rel4}.

            En el Ejercicio~\ref{ej:10_rel4} vimos que las únicas funciones paramétricas que admiten un estimador eficiente son las de la forma:
            \begin{equation*}
                a\cdot \frac{-n}{\theta} +b\qquad a,b\in \mathbb{R}, \quad a\neq 0
            \end{equation*}
            y sus estimadores eficientes son:
            \begin{equation*}
                a\cdot \sum_{i=1}^{n}\ln X_i + b
            \end{equation*}
            Tomando $a=\frac{-1}{n},b=0$, tenemos que el estimador:
            \begin{equation*}
                T = \frac{\sum\limits_{i=1}^{n}X_i}{-n}
            \end{equation*}
            es eficiente para $\frac{1}{\theta}$, por lo que $T$ es el EMV de $\theta$. Si aplicamos el Teorema de Zehna, tenemos que:
            \begin{equation*}
                \widehat{\left(\frac{1}{\theta}\right)} = \frac{1}{\hat{\theta}} \Longrightarrow \hat{\theta} = \frac{1}{\widehat{\left(\frac{1}{\theta}\right)} } = \frac{-n}{\sum\limits_{i=1}^{n}\ln X_i}
            \end{equation*}
            Observamos que obtenemos el mismo resultado que en el primer apartado.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra de una variable $X\rightsquigarrow B(k_0,p)$ para cierto $k_0\in \mathbb{N}$ y $p\in \left]0,1\right[$. Estimar, por máxima verosimilitud y por el método de los momentos, el parámetro $p$ y la varianza de $X$.\\

    \noindent
    \textit{Aplicación}: Se lanza $10$ veces un dado cargado y se cuenta el número de veces que sale un $4$. Este experimento se realiza $100$ veces de forma independiente, obteniéndose los siguientes resultados:
    \begin{table}[H]
    \centering
    \begin{tabular}{l|cccc}
        nº de 4 & 0 & 1 & 2 & 3 \\
        \hline
        frecuencia & 84 & 15 & 1 & 0
    \end{tabular}
    \end{table}
    \noindent
    Estimar, a partir de estos datos, la probabilidad de salir un cuatro.\\

    \begin{description}
        \item [Por máxima verosimilitud.] 
            Tenemos:
            \begin{align*}
                P[X_1 =x_1, \ldots, X_n = x_n] &\stackrel{\text{iid.}}{=}\prod_{i=1}^{n} P[X=x_i] = \prod_{i=1}^{n} \binom{k_0}{x_i} p^{x_i}{(1-p)}^{k_0-x_i} \\ &= p^{\sum\limits_{i=1}^{n}x_i}{(1-p)}^{nk_0-\sum\limits_{i=1}^{n}x_i} \prod_{i=1}^{n}\binom{k_0}{n}
            \end{align*}
            Por lo que:
            \begin{equation*}
                \ln L_{x_1,\ldots,x_n}(p) = \ln p \sum_{i=1}^{n}x_i + \ln(1-p)\left(nk_0 - \sum_{i=1}^{n}x_i\right) + \sum_{k=1}^{n}\ln\binom{k_0}{n}
            \end{equation*}
            Luego:
            \begin{align*}
                \dfrac{\partial \ln L_{x_1,\ldots,x_n}(p)}{\partial p} &= \frac{1}{p}\sum_{i=1}^{n}x_i - \frac{1}{1-p}\left(nk_0 - \sum_{i=1}^{n}x_i\right) = \frac{(1-p)\sum\limits_{i=1}^{n}x_i - pnk_0 + p\sum\limits_{i=1}^{n}x_i}{p(1-p)} \\
                                                                       &= \frac{\sum\limits_{i=1}^{n}x_i -pnk_0}{p(1-p)} = 0 \Longleftrightarrow p = \frac{\sum\limits_{i=1}^{n}x_i}{nk_0}
            \end{align*}
            Como $\sum\limits_{i=1}^{n}x_i\leq nk_0$, tenemos entonces que $p\in [0,1]$, por lo que esta fórmula nos dará una estimador, con lo que el EMV es:
            \begin{equation*}
                \hat{p} = \frac{1}{nk_0}\sum_{i=1}^{n}X_i
            \end{equation*}
            Para $Var(X) = k_0p(1-p)$, si aplicamos el Teorema de Zehna tenemos que:
            \begin{equation*}
                \widehat{k_0p(1-p)} = k_0\hat{p}(1-\hat{p}) = \frac{1}{n}\sum_{i=1}^{n}X_i\left(1-\frac{1}{nk_0}\sum_{i=1}^{n}X_i\right)
            \end{equation*}
        \item [Por el método de los momentos.] 
            Por el método de los momentos:
            \begin{equation*}
                \left\{\begin{array}{l}
                        k_0p = E[X] = \overline{X} = \frac{1}{n}\sum\limits_{i=1}^{n}X_i \\
                        k_0p(1-p) = Var(X) = \frac{1}{n}\left(\sum\limits_{k=1}^{n}X_i -\overline{X}\right)
                \end{array}\right.
            \end{equation*}
            Del primero de ducimos que:
            \begin{equation*}
                \hat{p} = \frac{1}{nk_0}\sum_{i=1}^{n}X_i
            \end{equation*}
            Y del segundo que:
            \begin{equation*}
                \widehat{Var(X)} = \frac{1}{n}\left(\sum\limits_{k=1}^{n}X_i -\overline{X}\right)
            \end{equation*}
    \end{description}
    Para estimar a partir de los datos mencionados la probabilidad de salir un cuatro, lo que hacemos primero es considerar:
    \begin{equation*}
        Y \equiv \text{``Número de veces que sale un 4 en 10 tiradas del dado''} \rightsquigarrow B(10,p)
    \end{equation*}
    donde estimaremos el valor de $p$ mediante la estimación obtenida:
    \begin{equation*}
        p = \frac{1}{nk_0}\sum_{i=1}^{n}x_i= \frac{1}{nk_0}\sum_{i=1}^{n}x_i = \frac{1}{1000}(0\cdot 84 + 1\cdot 15 + 2\cdot 1 + 3\cdot 0) = \frac{15+2}{1000} = \frac{17}{1000}
    \end{equation*}
    Por lo que la probabilidad de obtener un 4 es $\frac{17}{1000}$.
\end{ejercicio}

\begin{ejercicio}
    Se lanza un dado hasta que salga un $4$ y se anota el número de lanzamientos necesarios; este experimento se efectúa veinte veces de forma independiente. A partir de los resultados obtenidos, estimar la probabilidad de sacar un $4$ por máxima verosimilitud.
\end{ejercicio}

\begin{ejercicio}
    En $20$ días muy fríos, una granjera pudo arrancar su tractor en el primer, tercer, quinto, primer, segundo, primer, tercer, séptimo, segundo, cuarto, cuarto, octavo, primer, tercer, sexto, quinto, segundo, primer, sexto y segundo intento. Suponiendo que la probabilidad de arrancar en cada intento es constante, y que las observaciones se han obtenido de forma independiente, dar la estimación más verosímil de la probabilidad de que el tractor arranque en el segundo intento.
\end{ejercicio}

\begin{ejercicio}
    Una variable aleatoria discreta toma los valores $0.1$ y $2$ con las siguientes probabilidades
    \begin{equation*}
        P_p[X=0] = p^2, \qquad P_p[X=1] = 2p(1-p), \qquad P_p[X=2] = {(1-p)}^{2}
    \end{equation*}
    siendo $p$ un parámetro desconocido. En una muestra aleatoria simple de tamaño $100$, se ha presentado $22$ veces el $0$, $53$ veces el $1$ y $25$ veces el $2$. Calcular la función de verosimilitud asociada a dicha muestra y dar la estimación más verosímil de $p$.
\end{ejercicio}

\begin{ejercicio}
    En el muestreo de una variable aleatoria con distribución $\cc{N}(\mu,1)$, $\mu \in \mathbb{R}$, se observa que no se obtiene un valor menor que $-1$ hasta la quinta observación. Dar una estimación máximo verosímil de $\mu$.
\end{ejercicio}

\begin{ejercicio}
    En la producción de filamentos eléctricos la medida de interés, $X$, es el tiempo de vida de cada filamento, que tiene una distribución exponencial de parámetro $\theta$. Se eligen $n$ de tales filamentos de forma aleatoria e independiente, pero, por razones de economía, no conviene esperar a que todos se quemen y la observación acaba en el tiempo $T$. Dar el estimador máximo verosímil para la media de $X$ a partir del número de filamentos quemados durante el tiempo de observación.
\end{ejercicio}

\begin{ejercicio}
    Sean $X_1, \ldots, X_n$ observaciones independientes de una variable $X\rightsquigarrow\{\Gamma(p,a) : p,a>0\}$. Estimar ambos parámetros mediante el método de los momentos.\\

    \noindent
    \textit{Aplicación}: Ciertos neumáticos radiales tuvieron vidas útiles de $35200$, $41000$, $44700$, $38600$ y $41500$ kilómetros. Suponiendo que estos datos son observaciones independientes de una variable con distribución exponencial de parámetro $\theta$, dar una estimación de dicho parámetro por el método de los momentos.
\end{ejercicio}
