\newpage
\section{Estimación de máxima verosimilitud y otros métodos}
\begin{ejercicio}
    Sea $X\rightsquigarrow P_\theta$ con $\theta\in \mathbb{R}$ siendo $P_\theta$ una distribución con función de densidad
    \begin{equation*}
        f_\theta(x) = e^{\theta-x}, \qquad x\geq \theta
    \end{equation*}
    Dada una muestra aleatoria simple de tamaño $n$, encontrar los estimadores máximo verosímiles de $\theta$ y de $e^\theta$. Basándose en los resultados del Ejercicio~\ref{ej:7_rel4}, decir si estos estimadores son insesgados.\\

    \noindent
    Sea $(X_1, \ldots, X_n)$ una m.a.s. de $X\rightsquigarrow  P_\theta$, tenemos que:
    \begin{equation*}
        f^n_\theta(x_1, \ldots, x_n) \stackrel{\text{iid.}}{=} \prod_{i=1}^{n}f_\theta(x_i) = \prod_{i=1}^{n} e^{\theta-x} = e^{n\theta - \sum\limits_{i=1}^n x_i} \qquad \forall x_i \geq \theta
    \end{equation*}
    Por lo que si consideramos $x_{(1)} = \min\limits_{i \in \{1,\ldots,n\}}\{x_i\}$:
    \begin{equation*}
        L_{x_1,\ldots, x_n}(\theta) = f^n_\theta(x_1,\ldots,x_n) = \left\{\begin{array}{ll}
                e^{n\theta - \sum\limits_{i=1}^n x_i}  & \text{si\ } \theta \leq x_{(1)}  \\
             0 & \text{en otro caso\ } 
        \end{array}\right. 
    \end{equation*}
    Observamos que $L_{x_1,\ldots,x_n}(\theta)$ es una función creciente en $\left]-\infty,x_{(1)}\right]$, por lo que alcanza su máximo en $x_{(1)}$. Como el espacio paramétrico es $\Theta = \mathbb{R}$, tenemos que $T(X_1, \ldots, X_n) = X_{(1)}$ es un estimador de $\theta$, con lo que es el EMV, al maximizar la función de verosimilitud. Además, el Teorema de Zehna nos dice que:
    \begin{equation*}
        \widehat{e^\theta} = e^{\hat{\theta}} = e^{X_{(1)}}
    \end{equation*}
    Por lo que $e^{X_{(1)}}$ es EMV para $e^{\theta}$. % // TODO: TERMINAR luego
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con distribución exponencial. Basándose en los resultados del Ejercicio~\ref{ej:9_rel4}, encontrar los estimadores máximo verosímiles de la media y de la varianza de $X$.\\

    \noindent
    Sea $X\rightsquigarrow exp(\lm)$ con $\lm\in \mathbb{R}^+$, nos piden encontrar EMVs para:
    \begin{equation*}
        E[X] = \frac{1}{\lm}, \qquad Var(X) = \frac{1}{\lm^2}
    \end{equation*}
    En el Ejercicio~\ref{ej:9_rel4} obtuvimos que las únicas funciones paramétricas de dicha familia de distribuciones que admiten estimadores eficientes son las de la forma:
    \begin{equation*}
        a\cdot \frac{n}{\lm} +b \qquad a,b\in \mathbb{R}, \quad a\neq 0
    \end{equation*}
    y sus estimadores eficientes son:
    \begin{equation*}
        a\cdot \sum_{i=1}^{n}X_i + b
    \end{equation*}
    Tomando $a=\frac{1}{n}, b=0$, obtenemos que el estimador $T=\frac{\sum\limits_{i=1}^nX_i}{n}$ es eficiente para $\nicefrac{1}{\lm}$, por lo que un Teorema visto en teoría nos asegura que $T$ es el único EMV que podemos considerar para $\nicefrac{1}{\lm}$. Para $\nicefrac{1}{\lm^2}$ podemos aplicar el Teorema de Zehna, obteniendo:
    \begin{equation*}
        \widehat{{\left(\frac{1}{\lm^2}\right)}} = \frac{1}{{\left(\hat{\lm}\right)}^{2}} = \frac{n}{{\left(\sum\limits_{i=1}^{n}X_i\right)}^{2}}
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}
    Sea $X$ una variable aleatoria con función de densidad de la forma
    \begin{equation*}
        f_\theta(x) = \theta x^{\theta -1}, \qquad 0<x<1
    \end{equation*}
    \begin{enumerate}[label=\alph*)]
        \item Calcular un estimador máximo verosímil para $\theta$.

            Maximizamos la función de verosimilitud de $\theta$:
            \begin{equation*}
                L_{x_1,\ldots,x_n}(\theta) = f^n_\theta(x_1, \ldots, x_n) \stackrel{\text{iid.}}{=}\prod_{i=1}^{n}f_\theta(x_i) = \prod_{i=1}^{n} \theta x_i^{\theta-1} = \theta^n \prod_{i=1}^{n} x_i^{\theta -1}
            \end{equation*}
            Si aplicamos logaritmo no cambia la abscisa a maximizar:
            \begin{equation*}
                \ln L_{x_1,\ldots,x_n}(\theta) = n\ln \theta + (\theta -1)\sum_{i=1}^{n} \ln x_i
            \end{equation*}
            hayamos los puntos críticos:
            \begin{equation*}
                \dfrac{\partial \ln L_{x_1,\ldots,x_n}(\theta)}{\partial  \theta} = \frac{n}{\theta} + \sum_{i=1}^{n}\ln x_i = \frac{n + \theta \sum\limits_{k=1}^{n}\ln x_i}{\theta} = 0 \Longleftrightarrow \theta = \frac{-n}{\sum\limits_{k=1}^{n}\ln x_i}
            \end{equation*}
            Por lo que tomando:
            \begin{equation*}
                \hat{\theta}(X_1, \ldots, X_n) = \frac{-n}{\sum\limits_{i=1}^{n} \ln X_i}
            \end{equation*}
            tenemos que $\hat{\theta}$ es un estimador de $\theta$, luego es EMV para $\theta$.
        \item Deducir dicho estimador a partir de los resultados del Ejercicio~\ref{ej:10_rel4}.

            En el Ejercicio~\ref{ej:10_rel4} vimos que las únicas funciones paramétricas que admiten un estimador eficiente son las de la forma:
            \begin{equation*}
                a\cdot \frac{-n}{\theta} +b\qquad a,b\in \mathbb{R}, \quad a\neq 0
            \end{equation*}
            y sus estimadores eficientes son:
            \begin{equation*}
                a\cdot \sum_{i=1}^{n}\ln X_i + b
            \end{equation*}
            Tomando $a=\frac{-1}{n},b=0$, tenemos que el estimador:
            \begin{equation*}
                T = \frac{\sum\limits_{i=1}^{n}X_i}{-n}
            \end{equation*}
            es eficiente para $\frac{1}{\theta}$, por lo que $T$ es el EMV de $\theta$. Si aplicamos el Teorema de Zehna, tenemos que:
            \begin{equation*}
                \widehat{\left(\frac{1}{\theta}\right)} = \frac{1}{\hat{\theta}} \Longrightarrow \hat{\theta} = \frac{1}{\widehat{\left(\frac{1}{\theta}\right)} } = \frac{-n}{\sum\limits_{i=1}^{n}\ln X_i}
            \end{equation*}
            Observamos que obtenemos el mismo resultado que en el primer apartado.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $(X_1, \ldots, X_n)$ una muestra de una variable $X\rightsquigarrow B(k_0,p)$ para cierto $k_0\in \mathbb{N}$ y $p\in \left]0,1\right[$. Estimar, por máxima verosimilitud y por el método de los momentos, el parámetro $p$ y la varianza de $X$.\\

    \noindent
    \textit{Aplicación}: Se lanza $10$ veces un dado cargado y se cuenta el número de veces que sale un $4$. Este experimento se realiza $100$ veces de forma independiente, obteniéndose los siguientes resultados:
    \begin{table}[H]
    \centering
    \begin{tabular}{l|cccc}
        nº de 4 & 0 & 1 & 2 & 3 \\
        \hline
        frecuencia & 84 & 15 & 1 & 0
    \end{tabular}
    \end{table}
    \noindent
    Estimar, a partir de estos datos, la probabilidad de salir un cuatro.\\

    \begin{description}
        \item [Por máxima verosimilitud.] 
            Tenemos:
            \begin{align*}
                P[X_1 =x_1, \ldots, X_n = x_n] &\stackrel{\text{iid.}}{=}\prod_{i=1}^{n} P[X=x_i] = \prod_{i=1}^{n} \binom{k_0}{x_i} p^{x_i}{(1-p)}^{k_0-x_i} \\ &= p^{\sum\limits_{i=1}^{n}x_i}{(1-p)}^{nk_0-\sum\limits_{i=1}^{n}x_i} \prod_{i=1}^{n}\binom{k_0}{n}
            \end{align*}
            Por lo que:
            \begin{equation*}
                \ln L_{x_1,\ldots,x_n}(p) = \ln p \sum_{i=1}^{n}x_i + \ln(1-p)\left(nk_0 - \sum_{i=1}^{n}x_i\right) + \sum_{k=1}^{n}\ln\binom{k_0}{n}
            \end{equation*}
            Luego:
            \begin{align*}
                \dfrac{\partial \ln L_{x_1,\ldots,x_n}(p)}{\partial p} &= \frac{1}{p}\sum_{i=1}^{n}x_i - \frac{1}{1-p}\left(nk_0 - \sum_{i=1}^{n}x_i\right) = \frac{(1-p)\sum\limits_{i=1}^{n}x_i - pnk_0 + p\sum\limits_{i=1}^{n}x_i}{p(1-p)} \\
                                                                       &= \frac{\sum\limits_{i=1}^{n}x_i -pnk_0}{p(1-p)} = 0 \Longleftrightarrow p = \frac{\sum\limits_{i=1}^{n}x_i}{nk_0}
            \end{align*}
            Como $\sum\limits_{i=1}^{n}x_i\leq nk_0$, tenemos entonces que $p\in [0,1]$, por lo que esta fórmula nos dará una estimador, con lo que el EMV es:
            \begin{equation*}
                \hat{p} = \frac{1}{nk_0}\sum_{i=1}^{n}X_i
            \end{equation*}
            Para $Var(X) = k_0p(1-p)$, si aplicamos el Teorema de Zehna tenemos que:
            \begin{equation*}
                \widehat{k_0p(1-p)} = k_0\hat{p}(1-\hat{p}) = \frac{1}{n}\sum_{i=1}^{n}X_i\left(1-\frac{1}{nk_0}\sum_{i=1}^{n}X_i\right)
            \end{equation*}
        \item [Por el método de los momentos.] 
            Por el método de los momentos:
            \begin{equation*}
                \left\{\begin{array}{l}
                        k_0p = E[X] = \overline{X} = \frac{1}{n}\sum\limits_{i=1}^{n}X_i \\
                        k_0p(1-p) = Var(X) = \frac{1}{n}\left(\sum\limits_{k=1}^{n}X_i -\overline{X}\right)
                \end{array}\right.
            \end{equation*}
            Del primero de ducimos que:
            \begin{equation*}
                \hat{p} = \frac{1}{nk_0}\sum_{i=1}^{n}X_i
            \end{equation*}
            Y del segundo que:
            \begin{equation*}
                \widehat{Var(X)} = \frac{1}{n}\left(\sum\limits_{k=1}^{n}X_i -\overline{X}\right)
            \end{equation*}
    \end{description}
    Para estimar a partir de los datos mencionados la probabilidad de salir un cuatro, lo que hacemos primero es considerar:
    \begin{equation*}
        Y \equiv \text{``Número de veces que sale un 4 en 10 tiradas del dado''} \rightsquigarrow B(10,p)
    \end{equation*}
    donde estimaremos el valor de $p$ mediante la estimación obtenida:
    \begin{equation*}
        p = \frac{1}{nk_0}\sum_{i=1}^{n}x_i= \frac{1}{nk_0}\sum_{i=1}^{n}x_i = \frac{1}{1000}(0\cdot 84 + 1\cdot 15 + 2\cdot 1 + 3\cdot 0) = \frac{15+2}{1000} = \frac{17}{1000}
    \end{equation*}
    Por lo que la probabilidad de obtener un 4 es $\frac{17}{1000}$.
\end{ejercicio}

\begin{ejercicio}\label{ej:5_rel5}
    Se lanza un dado hasta que salga un $4$ y se anota el número de lanzamientos necesarios; este experimento se efectúa veinte veces de forma independiente. A partir de los resultados obtenidos, estimar la probabilidad de sacar un $4$ por máxima verosimilitud.\\

    \noindent
    Sea:
    \begin{equation*}
        X \equiv \text{``número de lanzamientos antes del lanzamiento en el que sale 4''}
    \end{equation*}
    tenemos que si $p$ es la probabilidad de que salga un 4, entonces $X$ sigue una distribución geométrica de parámetro $p$: $X\rightsquigarrow G(p)$. Como se realiza el experimento 20 veces de forma independientes, disponemos de una muestra $x_1, \ldots, x_n$ donde $n=20$. Estimamos $p$ por máxima verosimilitud:
    \begin{equation*}
        L_{x_1,\ldots,x_{n}}(p) = P_p[X_1 =x_1, \ldots, X_n=x_n] \stackrel{\text{indep.}}{=} \prod_{i=1}^{n} P_p[X=x_i] = \prod_{i=1}^{n} {(1-p)}^{x_i}p = p^{n}{(1-p)}^{\sum\limits_{i=1}^{n}x_i}
    \end{equation*}
    de donde al aplicar logaritmos:
    \begin{equation*}
        \ln L_{x_1,\ldots,x_n}(p) = n\cdot \ln p + \ln(1-p)\sum_{i=1}^{n}x_i
    \end{equation*}
    si derivamos:
    \begin{equation*}
        \dfrac{\partial \ln L_{x_1,\ldots,x_n}(p)}{\partial p} = \frac{n}{p} - \frac{\sum\limits_{i=1}^{n}x_i}{1-p} = \frac{n - np - p\left(\sum\limits_{i=1}^{n}x_i\right)}{p(1-p)} = \frac{n - p\left(\sum\limits_{i=1}^{n}x_i +n\right)}{p(1-p)}
    \end{equation*}
    Obtenemos que:
    \begin{equation*}
        \dfrac{\partial \ln L_{x_1,\ldots,x_n}(p)}{\partial  p} = 0 \Longleftrightarrow p = \frac{n}{\sum\limits_{i=1}^{n}x_i + n}
    \end{equation*}
    Como siempre tenemos que:
    \begin{equation*}
        \frac{n}{\sum\limits_{i=1}^{n}x_i + n} \in [0,1] 
    \end{equation*}
    Tenemos entonces que el EMV es:
    \begin{equation*}
        \hat{p} = \frac{n}{\sum\limits_{i=1}^{n}X_i+n} = \frac{20}{\sum\limits_{i=1}^{20}X_i+20}
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}
    En $20$ días muy fríos, una granjera pudo arrancar su tractor en el primer, tercer, quinto, primer, segundo, primer, tercer, séptimo, segundo, cuarto, cuarto, octavo, primer, tercer, sexto, quinto, segundo, primer, sexto y segundo intento. Suponiendo que la probabilidad de arrancar en cada intento es constante, y que las observaciones se han obtenido de forma independiente, dar la estimación más verosímil de la probabilidad de que el tractor arranque en el segundo intento.\\

    \noindent
    Si reunimos en una tabla la información del enunciado:
    \begin{table}[H]
    \centering
    \begin{tabular}{cccccccc}
        Primer & Segundo & Tercero & Cuarto & Quinto & Sexto & Séptimo & Octavo \\
        \hline
        5 & 4 & 3 & 2 & 2 & 2 & 1 & 1
    \end{tabular}
    \caption{Veces que arrancó en cada intento.}
    \end{table}

    Si consideramos:
    \begin{equation*}
        X \equiv \text{``Número de intentos fallidos de arrancar el tractor''}
    \end{equation*}
    tenemos que $X\rightsquigarrow G(p)$, donde $p$ es la probabilidad de arrancar el tractor en un intento. tenemos a nuestra disposición de una muestra de $X$ $x_1,\ldots, x_n$ de tamaño $n=20$. Si observamos la similitud con el Ejercicio~\ref{ej:5_rel5}, estamos bajo las mismas hipótesis, por lo que al calcular el EMV para $p$ obtendremos:
    \begin{equation*}
        \hat{p} = \frac{n}{\sum\limits_{i=1}^{n}X_i+n} = \frac{20}{\sum\limits_{i=1}^{20}X_i+20}
    \end{equation*}
    Sin embargo, queremos calcularlo para la probabilidad de que el tractor arranque en el segundo intento, es decir, para tener exactamente un intento fallido de arranzar el tractor:
    \begin{equation*}
        P_p[X = 1] = (1-p)p
    \end{equation*}
    Si amplicamos el Teorema de Zehna, podemos obtener el EMV de $P_p[X=1]$ a partir de $\hat{p}$:
    \begin{equation*}
        \widehat{P_p[X=1]} = \widehat{(1-p)p} = (1-\hat{p})\hat{p} = \left(1-\frac{20}{\sum\limits_{i=1}^{20}X_i+20}\right) \frac{20}{\sum\limits_{i=1}^{20}X_i+20}
    \end{equation*}
    Si sustituimos ahora en los valores de la muestra, observamos que:
    \begin{equation*}
        \sum_{i=1}^{n}x_i = 0\cdot 5 + 1\cdot 4 + 2\cdot 3 + 3\cdot 2 + 4\cdot 2 + 5\cdot 2 + 6\cdot 1 + 7\cdot 1 = 47
    \end{equation*}
    Por lo que la estimación máximo verosímil de la probabilida de que el tractor arranque en el segundo intento es igual a:
    \begin{equation*}
        \left(1-\frac{20}{\sum\limits_{i=1}^{20}X_i+20}\right) \frac{20}{\sum\limits_{i=1}^{20}X_i+20} = \left(1-\frac{20}{47+20}\right) \frac{20}{47+20} = \frac{940}{4489} \approx 0.2094
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}
    Una variable aleatoria discreta toma los valores $0$, $1$ y $2$ con las siguientes probabilidades
    \begin{equation*}
        P_p[X=0] = p^2, \qquad P_p[X=1] = 2p(1-p), \qquad P_p[X=2] = {(1-p)}^{2}
    \end{equation*}
    siendo $p$ un parámetro desconocido. En una muestra aleatoria simple de tamaño $100$, se ha presentado $22$ veces el $0$, $53$ veces el $1$ y $25$ veces el $2$. Calcular la función de verosimilitud asociada a dicha muestra y dar la estimación más verosímil de $p$.\\

    \noindent
    Los datos obtenidos son:
    \begin{table}[H]
    \centering
    \begin{tabular}{c|ccc}
        Dato & 0 & 1 & 2 \\
        \hline
        Veces & 22 & 53 & 25
    \end{tabular}
    \end{table}
    Sea $X$ una variable aleatoria cuya función masa de probabilidad nos viene dada, tenemos una muestra $x_1, \ldots, x_n$ de tamaño $n=100$. La función de verosimilitud asociada a la muestra es:
    \begin{align*}
        L_{x_1,\ldots,x_n}(p) &= P_p[X_1=x_1, \ldots, X_n = x_n] \stackrel{\text{iid.}}{=}\prod_{i=1}^{n}P_p[X=x_i]\\ 
                              &= \prod_{i=1}^{22}P_p[X=0] \prod_{i=1}^{53}P_p[X=1]\prod_{i=1}^{25}P_p[X=2] \\
                              &= \prod_{i=1}^{22}p^2 \prod_{i=1}^{53}2p(1-p)\prod_{i=1}^{25}{(1-p)}^{2} \\
                              &= p^{44} \cdot 2^{53}p^{53}{(1-p)}^{53} \cdot {(1-p)}^{50} = 2^{53}p^{97}{(1-p)}^{103}
    \end{align*}
    Si aplicamos logaritos:
    \begin{equation*}
        \ln L_{x_1,\ldots,x_n}(p) = 53\ln 2 + 97 \ln p + 103 \ln(1-p)
    \end{equation*}
    y derivamos:
    \begin{equation*}
        \dfrac{\partial \ln L_{x_1,\ldots,x_n}(p)}{\partial p} = \frac{97}{p} - \frac{103}{1-p} = \frac{97(1-p)-103p}{p(1-p)} = \frac{97 - 200p}{p(1-p)} = 0 \Longleftrightarrow p = \frac{97}{200}
    \end{equation*}
    Por lo que la estimación por máxima verosimilitud es $\frac{97}{200} = 0.485$.
\end{ejercicio}

\begin{ejercicio}
    En el muestreo de una variable aleatoria con distribución $\cc{N}(\mu,1)$, $\mu \in \mathbb{R}$, se observa que no se obtiene un valor menor que $-1$ hasta la quinta observación. Dar una estimación máximo verosímil de $\mu$.\\

    \noindent
    Sea $Y\rightsquigarrow\cc{N}(\mu, 1)$, si tomamos:
    \begin{equation*}
        X\equiv \text{``número de observaciones hasta obtener una menor que -1 (incluida)''}
    \end{equation*}

    tenemos que:
    \begin{equation*}
        P_p[X=x] = {(1-p)}^{x-1}p
    \end{equation*}
    donde $p= P[Y<-1]$. Tenemos una muestra de tamaño 1 ($x=5$) de $X$, con lo que la función de verosimilitud es:
    \begin{equation*}
        L_5(p) = P_p[X=5] = {(1-p)}^{5-1}p = p{(1-p)}^{4}
    \end{equation*}
    aplicando logaritmos:
    \begin{equation*}
        \ln L_5(p) = 4\ln(1-p) + \ln p
    \end{equation*}
    de donde:
    \begin{equation*}
        \dfrac{\partial \ln L_5(p)}{\partial p} = \frac{-4}{1-p}+\frac{1}{p} = \frac{-4p+1-p}{p(1-p)} = \frac{1-5p}{p(1-p)} = 0 \Longleftrightarrow p = \frac{1}{5}
    \end{equation*}
    Si notamos por $\Phi$ la función de distribución de una $\cc{N}(0,1)$, tenemos que:
    \begin{equation*}
        p = P[Y< -1] = P[Z<-1-\mu] = \Phi(-1-\mu) \Longrightarrow -1-\mu = \Phi^{-1}(p) \Longrightarrow \mu = 1-\Phi^{-1}(p)
    \end{equation*}
    de donde aplicando el Teorema de Zehna:
    \begin{equation*}
        \hat{\mu} = \widehat{(1-\Phi^{-1}(p))} = 1-\Phi^{-1}(\hat{p}) = 1-\Phi^{-1}(\nicefrac{1}{5})
    \end{equation*}
    y si miramos en la tabla de la normal $\cc{N}(0,1)$ la abscisa en la que se da probabilidad $\nicefrac{1}{5} = 0.20$ (o mejor, el punto opuesto al cual se alcanza probabilidad de $0.80$)
    \begin{equation*}
        \Phi^{-1}\left(\frac{1}{5}\right) = -0.85 \Longrightarrow \hat{\mu} = 1+0.85 = 1.85
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}
    En la producción de filamentos eléctricos la medida de interés, $X$, es el tiempo de vida de cada filamento, que tiene una distribución exponencial de parámetro $\theta$. Se eligen $n$ de tales filamentos de forma aleatoria e independiente, pero, por razones de economía, no conviene esperar a que todos se quemen y la observación acaba en el tiempo $T$. Dar el estimador máximo verosímil para la media de $X$ a partir del número de filamentos quemados durante el tiempo de observación.\\

    \noindent
    Sea $X\rightsquigarrow exp(\theta)$, si consideramos: 
    \begin{align*}
        Y\equiv &\text{``número de filamentos con tiempo de vida menor que} \\ &\qquad \qquad \text{$T$ en una muestra de tamaño $n$''} 
    \end{align*}
    tenemos que $Y\rightsquigarrow B(n,p)$ donde $p$ es la probabilidad de que el tiempo de vida de un filamento sea menor que $T$, $p = P[X<T]$. Disponemos de una muestra de tamaño $1$ de $Y$, $y$. Tratamos de calcular el EMV de $p$:
    \begin{equation*}
        L_y(p) = P_p[Y=y] = \binom{n}{y}p^y{(1-p)}^{n-y}
    \end{equation*}
    si aplicamos logaritmo:
    \begin{equation*}
        \ln L_y(p) = \ln\binom{n}{y} + y\ln p + (n-y)\ln (1-p)
    \end{equation*}
    y ahora derivamos:
    \begin{equation*}
        \dfrac{\partial \ln L_y(p)}{\partial p} = \frac{y}{p} - \frac{n-y}{1-p} = \frac{y - yp - np + yp}{p(1-p)} = \frac{y-np}{p(1-p)} = 0 \Longleftrightarrow p = \frac{y}{n}
    \end{equation*}
    tenemos que el EMV de $p$ es:
    \begin{equation*}
        \hat{p} = \frac{Y}{n}
    \end{equation*}
    Si observamos la definición de $p$:
    \begin{equation*}
        p = P[X<T] = F_X(T) = 1 - e^{-\theta T} \Longrightarrow e^{-\theta T} = 1-p \Longrightarrow -\theta T = \ln (1-p)
    \end{equation*}

    de donde:
    \begin{equation*}
        \theta = \frac{-\ln(1-p)}{T}
    \end{equation*}
    Sin embargo, lo que nos interesa es calcuar el EMV de $E[X] = \frac{1}{\theta}$, por lo que:
    \begin{equation*}
        E[X] = \frac{1}{\theta} = \frac{-T}{\ln(1-p)}
    \end{equation*}
    Si aplicamos ahora el Teorema de Zehna obtenemos finalmente el EMV de $E[X]$:
    \begin{equation*}
        \widehat{E[X]} = \widehat{\frac{1}{\theta}} = \widehat{\frac{-T}{\ln(1-p)}} = \frac{-T}{\ln(1-\hat{p})} = \frac{-T}{\ln\left(1-\frac{Y}{n}\right)}
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}
    Sean $X_1, \ldots, X_n$ observaciones independientes de una variable $X\rightsquigarrow\{\Gamma(p,a) : p,a>0\}$. Estimar ambos parámetros mediante el método de los momentos.\\

    \noindent
    \textit{Aplicación}: Ciertos neumáticos radiales tuvieron vidas útiles de $35200$, $41000$, $44700$, $38600$ y $41500$ kilómetros. Suponiendo que estos datos son observaciones independientes de una variable con distribución exponencial de parámetro $\theta$, dar una estimación de dicho parámetro por el método de los momentos.\\

    \noindent
    Planteamos el sistema a resolver, sabiendo que si $X\rightsquigarrow \Gamma(p,a)$, entonces:
    \begin{equation*}
        E[X] = \frac{p}{a}, \qquad Var(X) = \frac{p}{a^2}
    \end{equation*}

    por lo que:
    \begin{equation*}
        \left\{\begin{array}{c}
                \nicefrac{p}{a} = E[X] = \overline{X} \\
                \nicefrac{p}{a^2} = Var(X) = A_2 - {\left(\overline{X}\right)}^2
        \end{array}\right.
    \end{equation*}
    de la primera ecuación tenemos que:
    \begin{equation*}
        p = a\overline{X}
    \end{equation*}
    y si sustituimos en la segunda:
    \begin{equation*}
        \frac{\overline{X}}{a} = A_2 - {\left(\overline{X}\right)}^2 \Longrightarrow a = \frac{\overline{X}}{A_2-{\left(\overline{X}\right)}^2}
    \end{equation*}
    Sustituyendo ahora $a$ en la primera:
    \begin{equation*}
        p = \frac{{\left(\overline{X}\right)}^{2}}{A_2-{\left(\overline{X}\right)}^{2}}
    \end{equation*}
    Por lo que las estimaciones a considerar son:
    \begin{equation*}
        \hat{a} = \frac{\overline{X}}{A_2-{\left(\overline{X}\right)}^2}, \qquad \hat{p} = \frac{{\left(\overline{X}\right)}^{2}}{A_2-{\left(\overline{X}\right)}^{2}}
    \end{equation*}~\\

    \noindent
    Sea $X\rightsquigarrow exp(\theta)$, por el método de los momentos tenemos que:
    \begin{equation*}
        \frac{1}{\theta} = E[X] = \overline{X} \Longrightarrow \theta = \frac{1}{\overline{X}}
    \end{equation*}
    Por tanto, para estimar $\theta$ lo primero que hacemos es calcular la media de los valores:
    \begin{equation*}
        \frac{35200 + 41000 + 44700 + 38600 + 41500}{5} = 40200
    \end{equation*}

    y la estimación será:
    \begin{equation*}
        \hat{\theta} = \frac{1}{\overline{x}} = \frac{1}{40200} \approx 2.488\cdot 10^{-5}
    \end{equation*}
\end{ejercicio}
