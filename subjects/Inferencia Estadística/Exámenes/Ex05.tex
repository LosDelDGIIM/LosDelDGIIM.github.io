\documentclass[12pt]{article}

\input{../../_assets/preambulo.tex}

\newcommand{\E}{\mathrm{E}}
\newcommand{\V}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rmas}{\R^{+}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\I}{\mathrm{Im}}
\newcommand{\Rmenos}{\R^{-}}

\begin{document}

    % 1. Foto de fondo
    % 2. Título
    % 3. Encabezado Izquierdo
    % 4. Color de fondo
    % 5. Coord x del titulo
    % 6. Coord y del titulo
    % 7. Fecha

    
    \input{../../_assets/portada}
    \portadaExamen{ffccA4.jpg}{Inferencia \\ Estadística\\Examen V}{Inferencia Estadística. Examen V}{MidnightBlue}{-8}{28}{2026}{José Manuel Sánchez Varbas}

    \begin{description}
        \item[Asignatura] Inferencia Estadística.
        \item[Curso Académico] 2021-22.
        \item[Grado] Grado en Matemáticas y Grado en Ingeniería Informática y Matemáticas.
        \item[Descripción] Examen Ordinario.
        \item[Fecha] 21 de Enero de 2022.  
    \end{description}
    \newpage


    % ------------------------------------

    \begin{ejercicio}[2.5 puntos]
        Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable con función de densidad:
        \begin{equation*}
            f_\theta(x) = \frac{\sqrt{\theta-1}}{2}x^{\nicefrac{-3}{2}}, \qquad x>\theta-1
        \end{equation*}
        \begin{enumerate}[label=\alph*)]
            \item Determinar para qué valores de $n$ existe el UMVUE para $\theta$, y encontrarlo en tales casos. Justificar la existencia en los casos que corresponda.
            \item Calcular la función de verosimilitud y encontrar el estimador máximo verosímil de $\theta$. ¿Es insesgado? (justificar la respuesta).
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}[2.25 puntos]~
        \begin{enumerate}[label=\alph*)]
            \item Sea $X$ una variable aleatoria continua con distribución en una familia regular en el sentido de Fréchet-Cramér-Rao, cuyas funciones de densidad son de la forma:
                \begin{equation*}
                    f_\theta(x) = \exp[Q(\theta)T(x) + D(\theta)+S(x)], \qquad x\in \cc{X}, \quad \theta\in \Theta
                \end{equation*}
                siendo $T(X)$ un estadístico regular, insesgado en $\theta^3$, tal que $Var_\theta[T(X)] = \theta^2$. Calcular las funciones $Q(\theta)$ y $D(\theta)$, sabiendo que $Q(1)=0$ y $D(0)=1$.
            \item Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable $X$ con función masa de probabilidad:
                \begin{gather*}
                    P_p[X=x] = p^2(x-1){(1-p)}^{x-2}, \quad x=2,3,\ldots; \quad 0<p<1 \\ \left(\E_p[X] = \frac{2}{p}, \quad \V_p[X]= \frac{2(1-p)}{p^2}\right)
                \end{gather*}
                \begin{enumerate}[label=\alph*)]
                    \item Sabiendo que la familia de distribuciones es regular, calcular la función de información asociada a la muestra. Encontrar la clase de funciones paramétricas que admiten estimador eficiente y los estimadores correspondientes. 
                    \item Calcular la cota para la varianza de estimadores insesgados en $p^2$, regulares, y justificar si se alcanza o no dicha cota.
                \end{enumerate}
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}[2 puntos]
        Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable $X$ con distribución $\cc{N}(\mu,\sigma^2)$, siendo ambos parámetros desconocidos.
        \begin{enumerate}[label=\alph*)]
            \item Especificar la variable usada para hacer inferencia sobre $\sigma^2$. Deducir su distribución, detallando y justificando exhaustivamente cada paso.
            \item Deducir, detallando el método usado, el intervalo de confianza de menor longitud esperada uniformemente para $\mu$ a nivel de confianza $1-\alpha$.
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}[1.75 puntos]
        El retraso medio diario (en minutos) de los trabajadores de cierta empresa tiene una distribución $U(0,\theta)$. 
        Para contrastar la hipótesis de que el máximo retraso medio diario es de 5 minutos frente a que es de 4 minutos, 
        se midió dicha variable durante 10 días elegidos al azar, observando que el máximo retraso medio en esos 10 días 
        fue de 3 minutos. Obtener los tests más potentes de tamaños $0.05$ y $0.15$ para el problema de contraste planteado. 
        Decidir, a la vista de lo observado, si debe rechazarse la hipótesis nula en cada uno de los casos.
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[1.5 puntos]~
        \begin{enumerate}
            \item[a)] Test de Kolmogorov--Smirnov. Comparación con $\chi^2$.
            
            \item[b)] Se recogen observaciones de la facturación (en miles de euros) de un cierto supermercado durante $25$ días.
            Se quiere contrastar si la facturación se distribuye uniformemente.
            \begin{center}
                \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
                    10.1 & 7.3 & 4.2 & 3.1 & 8.4 & 9.2 & 1.1 & 1 & 8.1 & 6.4 & 10.5 & 1.2 \\
                    \hline
                \end{tabular}
                \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
                    1.8 & 2.6 & 2.2 & 3.9 & 2.9 & 6.5 & 8.1 & 8.3 & 3.8 & 4.2 & 7.3 & 8.9 & 9.6
                \end{tabular}
            \end{center}

        \end{enumerate}
    \end{ejercicio}

    \newpage
    \noindent
    \textbf{Solución.}
    \setcounter{ejercicio}{0}

    \begin{ejercicio}[2.5 puntos]
        Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable con función de densidad:
        \begin{equation*}
            f_\theta(x) = \frac{\sqrt{\theta-1}}{2}x^{\nicefrac{-3}{2}}, \qquad x>\theta-1
        \end{equation*}
        \begin{enumerate}[label=\alph*)]
            \item Determinar para qué valores de $n$ existe el UMVUE para $\theta$, y encontrarlo en tales casos. Justificar la existencia en los casos que corresponda. \\
            
            Buscamos obtener el UMVUE mediante el método alternativo visto en teoría. Para ello, en primer lugar hay que encontrar un estadístico suficiente y completo $T$, y luego una función del estadístico $h(T)$
            (denotaremos indistintamente $T \stackrel{not}{\equiv} T(X_1, \ldots, X_n)$, para una m.a.s. $(X_1, \ldots, X_n)$ con $n \in \N$ fijo) insesgada en $g(\theta) = \theta$, estimadora y con momento de segundo orden finito. Entonces $h(T)$ será el UMVUE. \\

            El estadístico suficiente se calcula por medio del Teorema de Factorización de Neyman-Fisher. La función conjunta es la siguiente
            $$f^n_\theta(x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^{n} f_\theta(x_i)$$
            Vemos que 
            $$x_i > \theta - 1 \quad \forall i = 1, \ldots, n \iff I_{\Rmas}(x_i - (\theta - 1)) = 1 \quad \forall i = 1, \ldots, n \iff I_{\Rmas}(x_{(1)} - (\theta - 1)) = 1$$ de donde se deduce que 
            $$f^n_\theta(x_1, \ldots, x_n) = \prod_{i=1}^{n} \frac{\sqrt{\theta-1}}{2}x_i^{\nicefrac{-3}{2}} I_{\Rmas}(x_{(1)} - (\theta - 1)) = 
            \dfrac{(\theta-1)^{n/2}}{2^n} \prod_{i=1}^{n} x_i^{\nicefrac{-3}{2}} I_{\Rmas}(x_{(1)} - (\theta - 1))$$
            Tomando $T(X_1, \ldots, X_n) = X_{(1)}$ y 
            $$h(x_1, \ldots, x_n) = \dfrac{1}{2^n}\prod_{i=1}^{n} x_i^{\nicefrac{-3}{2}}, \quad g_\theta(t) = 
            (\theta-1)^{n/2} I_{\Rmas}(t - (\theta - 1))$$
            Se cumple que 
            $$f^n_\theta(x_1, \ldots, x_n) = h(x_1, \ldots, x_n) g_\theta(T(x_1, \ldots, x_n)) \quad \forall (x_1, \ldots, x_n) \in \X^n$$
            donde $h$ es independiente del parámetro $\theta$ y $g_\theta$ depende de la muestra solo a través del estadístico, luego,
            por el Teorema de Factorización de Neyman-Fisher, el estadístico $T$ es suficiente. 

            \newpage

            Ahora, hay que comprobar que este estadístico es completo, lo cual se hará por definición. Sabemos por teoría que la distribución del mínimo es 
            $$F_T(t) = 1 - (1 - F_X(t))^n \Longrightarrow f_T(t) = n(1 - F_X(t))^{n-1} f_\theta(t)$$
            Hallamos ahora la función de distribución de $X$:
            $$F_X(t) = \int_{\theta - 1}^{t} f_\theta(x) dx = \int_{\theta - 1}^{t} \frac{\sqrt{\theta-1}}{2}x^{\nicefrac{-3}{2}} dx = 
            \frac{\sqrt{\theta-1}}{2} \int_{\theta - 1}^{t} x^{\nicefrac{-3}{2}} dx = \frac{\sqrt{\theta-1}}{2} \cdot \left[ \dfrac{x^{-1/2}}{-1/2} \right]_{\theta - 1}^{t} = $$
            $$-\sqrt{\theta-1} (t^{-1/2} - (\theta-1)^{-1/2}) = 1 - \sqrt{\dfrac{\theta-1}{t}} \quad t > \theta-1$$

            La función de densidad del estadístico será entonces
            $$f_T(t) = n(1 - F_X(t))^{n-1} f_\theta(t) = n \left(\sqrt{\dfrac{\theta-1}{t}}\right)^{n-1} \frac{\sqrt{\theta-1}}{2}t^{\nicefrac{-3}{2}} = $$
            $$\dfrac{n}{2} \dfrac{(\sqrt{\theta-1})^{n}}{(\sqrt{t})^{n+2}} = \dfrac{n}{2} (\theta-1)^{n/2} t^{-(n+2)/2} \quad t > \theta-1$$

            Sea $h$ una función medible verificando $$0 = \E[h(T)] \stackrel{def}{=} \int_{\theta-1}^{+\infty} h(t) f_T(t) dt = 
            \int_{\theta-1}^{+\infty} h(t) \dfrac{n}{2} (\theta-1)^{n/2} t^{-(n+2)/2} dt =$$
            $$\dfrac{n}{2} (\theta-1)^{n/2} \int_{\theta-1}^{+\infty} h(t) t^{-(n+2)/2} dt$$
            como $\dfrac{n}{2} (\theta-1)^{n/2} \neq 0 \quad \forall n \in \N, \quad \forall \theta>1$, debe ser
            $$\int_{\theta-1}^{+\infty} h(t) t^{-(n+2)/2} dt = 0$$
            Por el Teorema Fundamental del Cálculo, podemos considerar una primitiva $H(t)$ del integrando $h(t) t^{-(n+2)/2}$, y esta cumple,
            por la Regla de Barrow, que $\lim\limits_{m \to +\infty} H(m) - H(\theta-1) = 0 \quad \forall \theta > 1$. Derivando respecto 
            de $\theta$, se obtiene que
            $$-\dfrac{d}{d \theta} H(\theta-1) = 0 \iff -h(\theta-1) (\theta-1)^{-(n+2)/2} = 0 \stackrel{\theta > 1}{\iff}$$
            $$h(\theta-1) = 0$$
            Equivalentemente 
            $$\forall \theta \in \Theta = \left] 1, + \infty \right[ \quad h(\theta-1) = 0 \iff h(t) = 0 \quad \forall t > 0$$ (tomando $t = \theta-1 \in \left]0, +\infty \right[$).
            Por tanto $$\left]0, +\infty \right[ \subseteq \{t : h(t) = 0\}$$ y consecuentemente $$1 \geqslant P[h(T) = 0] \geqslant P[T > 0] = 1
            \Longrightarrow P[h(T) = 0] = 1$$

            y entonces por definición concluimos que $T$ es un estadístico completo. Tenemos entonces en este punto
            que $T$ es un estadístico suficiente y completo. 

            \newpage

            Ahora hay que buscar un estimador insesgado en $g(\theta)$ y de segundo orden finito. Sea $h$ (independiente de la anterior) función medible tal que
            $$\theta = g(\theta) = \E[h(T)] = \dfrac{n}{2} (\theta-1)^{n/2} \int_{\theta-1}^{+\infty} h(t) t^{-(n+2)/2} dt \iff$$
            $$\int_{\theta-1}^{+\infty} h(t) t^{-(n+2)/2} dt = \dfrac{2 \theta}{n} (\theta-1)^{-n/2}$$

            Derivamos respecto de $\theta$ a ambos lados e igualamos. El miembro izquierdo ya lo tenemos por el apartado anterior:
            $$-h(\theta-1) (\theta-1)^{-(n+2)/2}$$
            y el derecho es 
            $$\dfrac{2}{n}\left( (\theta-1)^{-n/2} - \theta \dfrac{n}{2} (\theta-1)^{-n/2-1} \right) = \dfrac{2}{n}(\theta-1)^{-n/2} - \theta (\theta-1)^{-n/2-1}$$
            Despejamos $h(\theta-1)$:
            $$-h(\theta-1) (\theta-1)^{-(n+2)/2} = \dfrac{2}{n}(\theta-1)^{-n/2} - \theta (\theta-1)^{-n/2-1} \iff $$
            $$h(\theta-1) = \dfrac{\dfrac{2}{n}(\theta-1)^{-n/2} - \theta (\theta-1)^{-n/2-1}}{-(\theta-1)^{-(n+2)/2}} = 
            -\dfrac{2}{n}(\theta-1) + \theta = \dfrac{-2(\theta-1) + n \theta}{n} = $$
            $$\dfrac{n \theta - 2 \theta + 2}{n} = \dfrac{(n-2)\theta + 2}{n}$$
            de donde
            $$h(t) = \dfrac{(n-2)(t+1) + 2}{n} = \dfrac{nt + n - 2t - 2 + 2}{n} = \dfrac{n-2}{n} t + 1$$
            Por construcción $h(T)$ es insesgada en $g(\theta)$. Vemos que $h(T)$ también es estimador de $g(\theta)$, 
            pues $\Theta = \left] 1, +\infty \right[$, y $g(\theta) = \theta \Longrightarrow g(\Theta) = \left]1,+\infty \right[$. 
            Como $T = X_{(1)} > 0$ y $\frac{n-2}{n} > 0$ si $n \geq 3$, entonces $h(T) = \frac{n-2}{n}T + 1 > 1$ si $n \geq 3$. Queda comprobar que tiene momento de segundo orden finito. \\

            Ello se cumplirá en caso de que $E[h(T)^2] < + \infty$:
            $$E[h(T)^2] \stackrel{def}{=} \int_{\theta-1}^{+\infty} h(t)^2 f_T(t) dt = $$
            $$\int_{\theta-1}^{+\infty} \left( 1 + \left( \dfrac{n-2}{n} \right)^2 t^2 + \dfrac{2(n-2)}{n}t \right) \dfrac{n}{2} (\theta-1)^{n/2} t^{-(n+2)/2} dt = $$
            $$\dfrac{n}{2} (\theta-1)^{n/2} \int_{\theta-1}^{+\infty} \left( 1 + \left( \dfrac{n-2}{n} \right)^2 t^2 + \dfrac{2(n-2)}{n}t \right) t^{-(n+2)/2} dt = $$
            $$\dfrac{n}{2} (\theta-1)^{n/2} \left( \int_{\theta-1}^{+\infty} t^{-(n+2)/2} dt + 
            \left( \dfrac{n-2}{n} \right)^2 \int_{\theta-1}^{+\infty} t^2 t^{-(n+2)/2} dt + \dfrac{2(n-2)}{n} \int_{\theta-1}^{+\infty} t t^{-(n+2)/2} dt \right) = $$
            $$\dfrac{n}{2} (\theta-1)^{n/2} \left( \left[ - \dfrac{2}{n} t^{-n/2} \right]_{\theta-1}^{+\infty} + 
            \left( \dfrac{n-2}{n} \right)^2 \left[ \dfrac{2}{4-n} t^{(4-n)/2} \right]_{\theta-1}^{+\infty} +
            \dfrac{2(n-2)}{n} \left[ \dfrac{1}{1-n/2} t^{1-n/2} \right]_{\theta-1}^{+\infty} \right)$$

            Estudiamos ahora cada primitiva para saber cuándo converge:
            \begin{itemize}
                \item $\left[ - \dfrac{2}{n} t^{-n/2} \right]_{\theta-1}^{+\infty} = - \dfrac{2}{n} \left(\lim\limits_{m \to +\infty} m^{-n/2} - (\theta-1)^{-n/2} \right)
                < +\infty \iff $
                $$\dfrac{-n}{2} < 0 \iff -n < 0 \iff n > 0$$ y ya se tenía esta condición para que $h(T)$ fuera estimador.
                \item $\left( \dfrac{n-2}{n} \right)^2 \left[ \dfrac{2}{4-n} t^{(4-n)/2} \right]_{\theta-1}^{+\infty} = $
                $$\dfrac{2(n-2)^2}{(4-n)n^2} \left(\lim\limits_{m \to +\infty} m^{(4-n)/2} - (\theta-1)^{(4-n)/2} \right) < +\infty \iff $$
                $$\dfrac{4-n}{2} < 0 \iff 4-n < 0 \iff n > 4$$ Por lo que de momento debe ser $n > 4 \iff n \geq 5$ para que el momento de segundo orden sea finito.
                \item $\dfrac{2(n-2)}{n} \left[ \dfrac{1}{1-n/2} t^{1-n/2} \right]_{\theta-1}^{+\infty} = $
                $$\dfrac{4(n-2)}{(2-n)n} \left(\lim\limits_{m \to +\infty} m^{(2-n)/2} - (\theta-1)^{(2-n)/2} \right) < +\infty \iff $$
                $$\dfrac{2-n}{2} < 0 \iff 2-n < 0 \iff n > 2$$ y ya se tenía esta condición implícitamente por la anterior.
            \end{itemize}

            Concluimos que $E[h(T)^2] < + \infty \iff n > 4$, ya que si $n \leq 4$, el momento de segundo orden no es finito.
            Por tanto, por el Teorema de Lehmann-Scheffé, $E[h(T)/T] = h(T)$ es el UMVUE para $g(\theta)$, y existe siempre y
            cuando $n>4 \iff n \geq 5$.

            \newpage

            \item Calcular la función de verosimilitud y encontrar el estimador máximo verosímil de $\theta$. ¿Es insesgado? (justificar la respuesta). \\
            
            Como 
            \begin{equation*}
                f_\theta(x) = \frac{\sqrt{\theta-1}}{2}x^{\nicefrac{-3}{2}}, \qquad x>\theta-1
            \end{equation*}
            y la función conjunta $f_\theta^n(x_1, \ldots, x_n)$ para $(x_1, \ldots, x_n) \in \X^n = (]\theta-1, +\infty[)^n$ ya la teníamos del apartado anterior
            $$\dfrac{(\theta-1)^{n/2}}{2^n} \prod_{i=1}^{n} x_i^{\nicefrac{-3}{2}}$$
            por definición la función de verosimilitud de $\theta$ será entonces
            $$L_{x_1, \ldots, x_n}(\theta) = f_\theta^n(x_1, \ldots, x_n) = \dfrac{(\theta-1)^{n/2}}{2^n} \prod_{i=1}^{n} x_i^{\nicefrac{-3}{2}} $$

            Para hallar el EMV, usaremos las ecuaciones de verosimilitud:
            $$\ln f_\theta^n(x_1, \ldots, x_n) = \dfrac{n}{2} \ln(\theta-1) - n \ln(2) - \dfrac{3}{2} \sum_{i=1}^{n} \ln(x_i) \Longrightarrow$$
            $$\dfrac{\partial \ln f_\theta^n(x_1, \ldots, x_n)}{\partial \theta} = \dfrac{n}{2 (\theta-1)} = 0 \iff n = 0$$
            lo cual no tiene sentido en este problema ya que $n \geq 1$. 
            Vemos entonces que la función de verosimilitud es estrictamente monótona respecto de $\theta$. De hecho, como $\theta - 1 > 0$, es estrictamente creciente.
            El dominio de esta función de $\theta$ teniendo en cuenta que $x_{(1)} > \theta-1 \iff 1 < \theta < x_{(1)} + 1$, es $]1, x_{(1)} + 1[$, y como
            es estrictamente creciente respecto de $\theta$, deducimos que el EMV de $\theta$ será el supremo del intervalo, es decir, $\hat{\theta} = X_{(1)} + 1$. \\

            El EMV será insesgado, por definición, si y solo si $\E[\hat{\theta}] = \theta$. Obtenemos así $\E[\hat{\theta}]$:
            $$\E[\hat{\theta}] = \E[X_{(1)} + 1] = \E[X_{(1)}] + 1 = \E[T] + 1$$

            Recordando del apartado anterior que 
            $$\E[h(T)] = \dfrac{n}{2} (\theta-1)^{n/2} \int_{\theta-1}^{+\infty} h(t) t^{-(n+2)/2} dt$$

            y tomando $h(T) = T$:
            $$\E[T] = \dfrac{n}{2} (\theta-1)^{n/2} \int_{\theta-1}^{+\infty} t t^{-(n+2)/2} dt \stackrel{(*)}{=} 
            \dfrac{n}{2} (\theta-1)^{n/2} \left( \dfrac{2}{2-n} \left(-(\theta-1)^{(2-n)/2} \right) \right) = $$
            $$\dfrac{n}{\cancel{2}} (\theta-1)^{n/2} \dfrac{\cancel{2}}{n-2} (\theta-1)^{(2-n)/2} = 
            \dfrac{n}{n-2} (\theta-1) = \dfrac{n}{n-2} \theta - \dfrac{n}{n-2}$$

            donde en $(*)$ se ha usado la convergencia de la última integral del apartado anterior, para $n > 2$. Así
            $$\E[\hat{\theta}] = \E[T] + 1 = \dfrac{n}{n-2} \theta - \dfrac{n}{n-2} + 1 = 
            \dfrac{n}{n-2} \theta - \dfrac{2}{n-2}$$

            y como 
            $$\E[\hat{\theta}] = \dfrac{n}{n-2} \theta - \dfrac{2}{n-2} \neq \theta \quad \forall n > 2$$

            se tiene que el EMV de $\theta$, $\hat{\theta}$, no es insesgado en $\theta$.

        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[2.25 puntos]~
        \begin{enumerate}[label=\alph*)]
            \item Sea $X$ una variable aleatoria continua con distribución en una familia regular en el sentido de Fréchet-Cramér-Rao, cuyas funciones de densidad son de la forma:
                \begin{equation*}
                    f_\theta(x) = \exp[Q(\theta)T(x) + D(\theta)+S(x)], \qquad x\in \cc{X}, \quad \theta\in \Theta
                \end{equation*}
                siendo $T(X)$ un estadístico regular, insesgado en $\theta^3$, tal que $Var_\theta[T(X)] = \theta^2$. Calcular las funciones $Q(\theta)$ y $D(\theta)$, sabiendo que $Q(1)=0$ y $D(0)=1$. \\

                Primero obtenemos relaciones con las condiciones que nos da el enunciado. 
                Calculamos:
                $$\ln f_\theta(x) = Q(\theta) T(x) + D(\theta) + S(x) \Longrightarrow 
                \dfrac{\partial \ln f_\theta(X)}{\partial \theta} = T(X) Q'(\theta) + D'(\theta)$$
                y usamos que la familia es regular
                $$\E_\theta \left[ \dfrac{\partial \ln f_\theta(X)}{\partial \theta} \right] = 0 \iff 
                \E_\theta \left[ T(X) Q'(\theta) + D'(\theta) \right] = 0 \iff $$
                $$Q'(\theta) \E_\theta \left[ T(X) \right] + D'(\theta) = 0 \iff$$
                \begin{equation}\label{eq:rel1ej2a}
                    \theta^3 \stackrel{(*)}{=} \E_\theta[T(X)] = -\dfrac{D'(\theta)}{Q'(\theta)} \iff D'(\theta) = - \theta^3 Q'(\theta)
                \end{equation}
                donde en $(*)$ se ha utilizado que $T$ es insesgado en $\theta^3$. Ahora, por ser $T$ regular, se tiene que
                $$\dfrac{\partial}{\partial \theta} E_\theta[T(X_1, \ldots, X_n)] = E_\theta \left[T(X_1, \ldots, X_n) \dfrac{\partial \ln f_\theta^n (X_1, \ldots, X_n)}
                {\partial \theta} \right]$$
                en particular para $n=1$
                $$\dfrac{\partial}{\partial \theta} E_\theta[T(X)] = E_\theta \left[T(X) \dfrac{\partial \ln f_\theta (X)}
                {\partial \theta} \right]$$

                Nuevamente, como $T$ es insesgado en $\theta^3$, podemos obtener el miembro izquierdo 
                de la igualdad
                $$\dfrac{\partial}{\partial \theta} E_\theta[T(X)] = 
                \dfrac{\partial}{\partial \theta} (\theta^3) = 3 \theta^2$$

                y el miembro derecho, usando lo ya calculado al principio, es
                $$\E_\theta \left[T(X) \dfrac{\partial \ln f_\theta (X)}
                {\partial \theta} \right] = 
                \E_\theta \left[T(X) \left( T(X) Q'(\theta) + D'(\theta) \right) \right] = 
                \E_\theta \left[T(X)^2 Q'(\theta) + T(X)D'(\theta) \right] = $$
                $$Q'(\theta) \E_\theta[T(X)^2] + D'(\theta) \E_\theta[T(X)] \stackrel{(*)}{=}
                Q'(\theta) \E_\theta[T(X)^2] + \theta^3 D'(\theta) \stackrel{(**)}{=} $$
                $$Q'(\theta) (\V_\theta[T(X)] + (\E_\theta[T(X)])^2) + \theta^3 D'(\theta) = 
                (\theta^2 + \theta^6) Q'(\theta) + \theta^3 D'(\theta)$$

                donde en $(*)$ se ha tenido en cuenta la insesgadez de $T$ en $\theta^3$ y 
                en $(**)$ que $\V_\theta[X] = \E_\theta[X^2] - (\E_\theta[X])^2$. 
                
                \newpage
                
                Juntando ambos miembros
                $$\dfrac{\partial}{\partial \theta} E_\theta[T(X)] = E_\theta \left[T(X) \dfrac{\partial \ln f_\theta (X)}
                {\partial \theta} \right] \iff$$
                \begin{equation}\label{eq:rel2ej2a}
                    3 \theta^2 = (\theta^2 + \theta^6) Q'(\theta) + \theta^3 D'(\theta)
                \end{equation}

                Podemos construir un sistema de ecuaciones con $(\ref{eq:rel1ej2a})$ y $(\ref{eq:rel2ej2a}$), y obtener $Q(\theta)$ y $D(\theta)$:
                $$\begin{cases}
                    D'(\theta) = - \theta^3 Q'(\theta) \\
                    3 \theta^2 = (\theta^2 + \theta^6) Q'(\theta) + \theta^3 D'(\theta)
                \end{cases}$$

                Sustituimos la primera expresión en la segunda
                $$3 \theta^2 = (\theta^2 + \theta^6) Q'(\theta) + \theta^3 D'(\theta) = (\theta^2 + \theta^6) Q'(\theta) + \theta^3 (- \theta^3 Q'(\theta)) = $$
                $$\theta^2 Q'(\theta) + \theta^6 Q'(\theta) - \theta^6 Q'(\theta) = \theta^2 Q'(\theta) \iff$$
                $$Q'(\theta) = 3$$

                y ahora integramos respecto de $\theta$ para hallar $Q(\theta)$:
                $$Q'(\theta) = 3 \Longrightarrow Q(\theta) + c_1 = \int Q'(\theta) d \theta = \int 3 d \theta = 3 \theta + c_2 \Longrightarrow
                Q(\theta) = 3 \theta + c_2 - c_1$$
                llamando $c_2 - c_1 \equiv c$, determinamos su valor con la condición inicial $Q(1) = 0$.
                $$3 + c = Q(1) = 0 \Longrightarrow c = -3 \Longrightarrow Q(\theta) = 3 \theta - 3 = 3(\theta - 1)$$
                Como hemos visto que $Q'(\theta) = 3$, sustituimos esta expresión en $(\ref{eq:rel1ej2a})$ y obtenemos $D(\theta)$ igual que con $Q(\theta)$:
                $$D'(\theta) = - \theta^3 Q'(\theta) = - 3\theta^3 \Longrightarrow$$
                $$D(\theta) + k_1 = \int D'(\theta) d \theta = - 3 \int \theta^3 d \theta = - 3 \dfrac{\theta^4}{4} + k_2 = \dfrac{-3\theta^4}{4} + k_2 \Longrightarrow D(\theta) = \dfrac{-3\theta^4}{4} + k_2 - k_1$$
                llamando $k_2 - k_1 \equiv k$, determinamos su valor con la condición inicial $D(0) = 1$.
                $$k = D(0) = 1 \Longrightarrow D(\theta) = 1 - \dfrac{3\theta^4}{4}$$
                En resumen, las funciones $Q(\theta)$ y $D(\theta)$ son:
                $$Q(\theta) = 3(\theta - 1) \quad D(\theta) = 1 - \dfrac{3\theta^4}{4}$$

            \item Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable $X$ con función masa de probabilidad:
                \begin{gather*}
                    P_p[X=x] = p^2(x-1){(1-p)}^{x-2}, \quad x=2,3,\ldots; \quad 0<p<1 \\ \left(\E_p[X] = \frac{2}{p}, \quad \V_p[X]= \frac{2(1-p)}{p^2}\right)
                \end{gather*}
                \begin{enumerate}[label=\alph*)]
                    \item Sabiendo que la familia de distribuciones es regular, calcular la función de información asociada a la muestra. 
                    Encontrar la clase de funciones paramétricas que admiten estimador eficiente y los estimadores correspondientes. \\

                    Tenemos que $p = \theta$, y que $f_\theta(x) = P_p[X=x]$. Para lo primero que se pide, 
                    sabemos que $$I_{X_1, \ldots, X_n}(\theta) = n I_X(\theta) = n \V_\theta \left[ \dfrac{\partial \ln f_\theta(X)}{\partial \theta} \right]$$

                    Calculamos

                    $$\ln f_\theta(x) = 2 \ln(\theta) + \ln(x-1) + (x-2) \ln(1-\theta) \Longrightarrow $$
                    $$\dfrac{\partial \ln f_\theta(x)}{\partial \theta} = \dfrac{2}{\theta} - \dfrac{x-2}{1-\theta} = 
                    \dfrac{2(1-\theta) - (x-2) \theta}{\theta(1-\theta)} = \dfrac{2- 2\theta - x \theta + 2\theta}{\theta(1-\theta)} = 
                    \dfrac{2 - x \theta}{\theta(1-\theta)} = $$
                    $$\dfrac{2}{\theta(1-\theta)} - \dfrac{1}{1-\theta}x$$

                    y 

                    $$\V_\theta \left[ \dfrac{\partial \ln f_\theta(X)}{\partial \theta} \right] = \V_\theta \left[ \dfrac{2}{\theta(1-\theta)} - \dfrac{1}{1-\theta}X \right] = $$
                    $$\left( \dfrac{1}{1-\theta} \right)^2 \V_\theta[X] = \dfrac{1}{(1-\theta)^2} \frac{2(1-\theta)}{\theta^2} = \dfrac{2}{\theta^2(1-\theta)}$$

                    de donde

                    $$I_{X_1, \ldots, X_n}(\theta) = n \V_\theta \left[ \dfrac{\partial \ln f_\theta(X)}{\partial \theta} \right] = 
                    n \dfrac{2}{\theta^2(1-\theta)} = \dfrac{2n}{\theta^2(1-\theta)}$$

                    Ahora, para la segunda parte, buscamos aplicar el Teorema de Caracterización de Estimadores Eficientes. Para ello, obtenemos la función conjunta de la m.a.s. de $X$

                    $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep}{=} \prod_{i=1}^n f_\theta(x_i)$$

                    Se supondrá a partir de ahora que $x_i \in \N \setminus \{1\}$ y $0<\theta<1$. De lo contrario, $f_\theta(x_i) = 0 \quad \forall i = 1, \ldots, n$.
                    $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep}{=} \prod_{i=1}^n f_\theta(x_i) = \prod_{i=1}^n 
                    \theta^2(x_i-1){(1-\theta)}^{x_i-2} = \theta^{2n} (1-\theta)^{\sum_{i=1}^{n} (x_i - 2)} \prod_{i=1}^n (x_i-1)$$
                    $$\ln f_\theta^n(x_1, \ldots, x_n) = 2n \ln(\theta) + \left(\sum_{i=1}^{n} (x_i - 2) \right) \ln(1-\theta) + \sum_{i=1}^{n} \ln(x_i-1) = $$
                    $$2n \ln(\theta) + \left(\sum_{i=1}^{n} x_i \right) \ln(1-\theta) - 2n \ln(1-\theta) + \sum_{i=1}^{n} \ln(x_i-1)$$
                    $$\dfrac{\partial \ln f_\theta^n(x_1, \ldots, x_n)}{\partial \theta} = \dfrac{2n}{\theta} - \dfrac{\displaystyle\sum_{i=1}^{n} x_i}{1-\theta} + 
                    \dfrac{2n}{1-\theta} = \dfrac{2n}{\theta(1-\theta)} - \dfrac{\displaystyle\sum_{i=1}^{n} x_i}{1-\theta} = $$
                    $$-\dfrac{1}{1-\theta} \left(\displaystyle\sum_{i=1}^{n} x_i - \dfrac{2n}{\theta} \right) = 
                    -\dfrac{n}{1-\theta} \left(\dfrac{1}{n}\displaystyle\sum_{i=1}^{n} x_i - \dfrac{2}{\theta} \right) $$

                    Ahora, supongamos que $T(X_1, \ldots, X_n)$ es un estimador de $g(\theta)$ función paramétrica derivable y estrictamente monótona
                    ($g'(\theta) \neq 0 \quad \forall \theta \in \Theta = \left]0, 1 \right[ $). Como el enunciado nos dice que la familia es regular, y 
                    $0 < I_X(\theta) = \nicefrac{2}{\theta^2(1-\theta)} < +\infty \quad \forall \theta \in \Theta$, $T$ es eficiente si y solo si $\forall \theta \in \Theta
                    \quad \exists a(\theta) \neq 0$ tal que 
                    $$P_\theta \left[ \dfrac{\partial \ln f_\theta^n(X_1, \ldots, X_n)}{\partial \theta} = a(\theta)[T(X_1, \ldots, X_n) - g(\theta)]\right] = 1$$
                    y
                    $$I_{(X_1, \ldots, X_n)}(\theta) = a(\theta) g'(\theta)$$
                    Como 
                    $$\dfrac{\partial \ln f_\theta^n(X_1, \ldots, X_n)}{\partial \theta} = a(\theta)[T(X_1, \ldots, X_n) - g(\theta)] \iff$$
                    $$-\dfrac{n}{1-\theta} \left(\dfrac{1}{n}\displaystyle\sum_{i=1}^{n} x_i - \dfrac{2}{\theta} \right) = a(\theta) [T(X_1, \ldots, X_n) - g(\theta)]$$
                    claramente por comparación se obtiene que 
                    $$T(X_1, \ldots, X_n) = \dfrac{1}{n} \sum_{i=1}^{n} X_i, \quad g(\theta) = \dfrac{2}{\theta}, \quad a(\theta) = -\dfrac{n}{1-\theta}$$
                    Es claro que $T(X_1, \ldots, X_n) = \frac{1}{n} \sum_{i=1}^n X_i$ es un estimador, pues $\Theta = \left]0, 1 \right[$, $g(\Theta) = \left]2, +\infty \right[$, 
                    y $T(x_1, \ldots, x_n) \geq 2 \quad \forall (x_1, \ldots, x_n) \in \X^n = (\N \setminus \{1\})^n$. Además, tanto 
                    $a(\theta)$ como $g(\theta)$ verifican todas las condiciones del teorema, pues $$g'(\theta) = 2 \cdot (\theta^{-1})' = 2 \cdot (- \theta^{-2}) = -\dfrac{2}{\theta^2} < 0$$ porque $0 < \theta < 1$
                    y $a(\theta) = -\dfrac{n}{1-\theta} < 0 \quad \forall n \in \N$, en particular, $a(\theta) \neq 0 \quad \forall \theta \in \Theta$, y usando la aditividad de la función de información de Fisher, obtenemos
                    $$\dfrac{2n}{\theta^2(1-\theta)} = I_{(X_1, \ldots, X_n)}(\theta) = a(\theta) g'(\theta) = -\dfrac{n}{1-\theta} \left( - \dfrac{2}{\theta^2} \right) = \dfrac{2n}{\theta^2(1-\theta)}$$

                    \newpage

                    Por un corolario visto en teoría, como $T(X_1, \ldots, X_n)$ es un estimador eficiente para $g(\theta)$, con $g'(\theta) \neq 0$,
                    sabemos que las únicas funciones paramétricas que admiten estimadores eficientes son las de la forma $a g(\theta) + b$ y los correspondientes
                    estimadores eficientes son $aT + b$, con probabilidad $1$, bajo todas las distribuciones de la familia.

                    \item Calcular la cota para la varianza de estimadores insesgados en $p^2$, regulares, y justificar si se alcanza o no dicha cota. \\
                    
                    En este apartado $g(\theta) = \theta^2$. Ya se han comprobado las hipótesis del Teorema de Cota de FCR en el apartado anterior, y 
                    supuesto que los estimadores insesgados sean además de segundo orden (si no, la varianza no será finita), se tiene que
                    la cota inferior de la varianza es, tomando como uno de estos estimadores $T(X_1, \ldots, X_n)$:
                    $$\V_\theta[T(X_1, \ldots, X_n)] \geqslant \dfrac{(g'(\theta))^2}{I_{(X_1, \ldots, X_n)}(\theta)} \quad \forall \theta \in \Theta$$
                    Ahora, $g'(\theta) = 2\theta$, y ya hemos visto que 
                    $$I_{(X_1, \ldots, X_n)}(\theta) = \dfrac{2n}{\theta^2(1-\theta)}$$
                    luego la cota inferior para la varianza de todo estos estimadores será
                    $$\V_\theta[T(X_1, \ldots, X_n)] \geqslant \dfrac{(2\theta)^2 \theta^2 (1-\theta)}{2n} = \dfrac{2 \theta^4 (1-\theta)}{n}$$ 

                    La cota no se alcanza por reducción al absurdo. Si lo hiciera, entonces tendríamos que $g (\theta) = \theta^2$ admitiría
                    un estimador regular, insesgado y cuya varianza alcanza la cota de FCR para cualquier valor $\theta \in \Theta$, 
                    es decir, un estimador eficiente (dado que $\{F_\theta : \theta \in \Theta = \Rmas\}$ es una familia regular, $0 < I_X(\theta) = \nicefrac{2}{\theta^2(1-\theta)} < + \infty
                    \quad \forall \theta \in \Theta$ y $g(\theta) = \theta^2$ es una función paramétrica derivable). Sin embargo, 
                    hemos visto en el apartado anterior que las únicas funciones paramétricas que admiten estimador eficiente en esta familia son las 
                    de la forma $a \dfrac{2}{\theta} + b$, $a, b \in \R$. Como $\nexists a,b \in \R : \theta^2 = a \dfrac{2}{\theta} + b$, hemos llegado a contradicción.  

                \end{enumerate}
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[2 puntos]
        Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable $X$ con distribución $\cc{N}(\mu,\sigma^2)$, siendo ambos parámetros desconocidos.
        \begin{enumerate}[label=\alph*)]
            \item Especificar la variable usada para hacer inferencia sobre $\sigma^2$. Deducir su distribución, detallando y justificando exhaustivamente cada paso. \\
            
                Estamos en el caso en que se quiere hacer inferencia sobre $\sigma^2$ teniendo en cuenta que la media $\mu$ es desconocida. Por teoría
                sabemos que la variable usada es $$\dfrac{(n-1)S^2}{\sigma^2}$$

                Veamos qué distribución sigue esta variable.
                \begin{proof}
                    $$\sum_{i=1}^{n} (X_i - \mu)^2 = \sum_{i=1}^{n} (\overbrace{X_i - \overline{X}}^{a} + 
                    \overbrace{\overline{X} - \mu}^{b})^2$$
                    y aplicamos el cuadrado de un binomio $(a+b)^2 = a^2 + b^2 + 2ab$:
                    $$\sum_{i=1}^{n} (X_i - \mu)^2 = \sum_{i=1}^{n} ((X_i - \overline{X})^2 + (\overline{X} - \mu)^2
                    + 2 (X_i - \overline{X})(\overline{X} - \mu)) = $$
                    $$\sum_{i=1}^{n} (X_i - \overline{X})^2 + \sum_{i=1}^{n} (\overline{X} - \mu)^2 + 
                    \cancelto{0}{\sum_{i=1}^{n} 2 (X_i - \overline{X})(\overline{X} - \mu)} = $$
                    $$(n-1)S^2 + n (\overline{X} - \mu)^2$$
                    Dividimos a cada lado de los extremos de la cadena de igualdades por $\sigma^2 > 0$:
                    $$\underbrace{\dfrac{\sum_{i=1}^{n} (X_i - \mu)^2}{\sigma^2}}_{A} = 
                    \underbrace{\dfrac{(n-1)S^2}{\sigma^2}}_B + 
                    \underbrace{\dfrac{n (\overline{X} - \mu)^2}{\sigma^2}}_C$$
                    Hallamos la distribución de $A$, $C$, y la que nos interesa, $B$: 
                    \begin{itemize}
                        \item $A$. El enunciado nos dice que $X \rightsquigarrow \cc{N}(\mu, \sigma^2)$, luego, 
                        como $(X_1, \ldots, X_{n})$ es una m.a.s. de $X$, se tiene que $$X_i \rightsquigarrow \cc{N}(\mu, \sigma^2) \quad
                        \forall i = 1, \ldots, n$$ Tipificando para cada $i = 1, \ldots, n$:
                        $$\dfrac{X_i - \mu}{\sigma} \rightsquigarrow \cc{N}(0,1) \stackrel{(*)}{\Longrightarrow} 
                        \dfrac{(X_i - \mu)^2}{\sigma^2} \rightsquigarrow \chi^2(1)$$

                        donde en $(*)$ se ha aplicado una propiedad de la $\chi^2(n)$. Nuevamente, como $(X_1, \ldots, X_{n})$ es una m.a.s. de $X$,
                        las variables de la m.a.s. son independientes, y se puede usar la reproductividad de la $\chi^2(n)$:
                        $$\dfrac{\displaystyle\sum_{i=1}^{n}(X_i - \mu)^2}{\sigma^2} \rightsquigarrow \chi^2(n)$$

                        \item $C \rightsquigarrow \chi^2(1)$, porque sabemos que $\overline{X} \rightsquigarrow \cc{N}(\mu, \sigma^2/n)$,
                        y tipificando:
                        $$\dfrac{\overline{X} - \mu}{\sigma / \sqrt{n}} \rightsquigarrow \cc{N}(0,1)$$
                        y aplicando la misma propiedad $(*)$ que antes
                        $$\dfrac{\overline{X} - \mu}{\sigma / \sqrt{n}} \rightsquigarrow \cc{N}(0,1) \stackrel{(*)}{\Longrightarrow}
                        \dfrac{(\overline{X} - \mu)^2}{\sigma^2 / n} = \dfrac{n(\overline{X} - \mu)^2}{\sigma^2} \rightsquigarrow \chi^2(1)$$
                    \end{itemize}
                    Ahora, como $A = B + C$, y $B$ y $C$ son independientes porque son funciones medibles de variables independientes
                    ($B$ es función medible de $S^2$ y $C$ función medible de $\overline{X}$, independientes por el Lema de Fisher),
                    podemos utilizar el teorema de multiplicación de esperanzas:
                    $$M_A(t) = \E[e^{tA}] = \E[e^{t(B+C)}] = \E[e^{tB} e^{tC}] \stackrel{TME}{=} \E[e^{tB}]
                    \E[e^{tC}] = M_B(t) M_C(t)$$
                    La función generatriz de momentos de la $\chi^2$ es conocida, $$M(t) = \dfrac{1}{(1-2t)^{n/2}}, 
                    \quad t < 1/2$$
                    por lo que basta despejar $M_B(t)$, con $A \rightsquigarrow \chi^2(n)$ y $C \rightsquigarrow \chi^2(1)$:
                    $$\dfrac{1}{(1-2t)^{n/2}} = M_A(t) = M_B(t) M_C(t) = M_B(t) \dfrac{1}{(1-2t)^{1/2}} \iff $$
                    $$M_B(t) = \dfrac{(1-2t)^{1/2}}{(1-2t)^{n/2}} = (1-2t)^{1/2-n/2} = (1-2t)^{(1-n)/2} = 
                    \dfrac{1}{(1-2t)^{(n-1)/2}} \quad t < 1/2$$
                    y por el Teorema de Unicidad de la Función Generatriz de Momentos, $$\dfrac{(n-1)S^2}{\sigma^2} = B 
                    \rightsquigarrow \chi^2(n-1)$$
                    lo que concluye la prueba.
                \end{proof}

            \newpage

            \item Deducir, detallando el método usado, el intervalo de confianza de menor longitud esperada uniformemente para $\mu$ a nivel de confianza $1-\alpha$. \\
            
            Usaremos el método de la cantidad pivotal. Sabemos que un pivote para $\mu$, siendo $\sigma^2$ desconocida, es, considerando $\theta = \mu$ y $\Theta = \R$, $T : \X^n \times \Theta \to \R$, dado por
            $$T(X_1, \ldots, X_n; \theta) = \dfrac{\overline{X} - \theta}{S / \sqrt{n}} \rightsquigarrow t(n-1)$$
            variable aleatoria cuya distribución, $t(n-1)$, no depende de $\theta$.
            Además:
            \begin{enumerate}
                \item $T$ es estrictamente monótona en $\theta$, pues
                $$\dfrac{\partial T}{\partial \theta} = \dfrac{\partial}{\partial \theta} \left( \dfrac{\overline{X} - \theta}{S / \sqrt{n}} \right) = - \dfrac{1}{S / \sqrt{n}} < 0, \quad \forall \theta \in \Theta$$
                \item $T = \lm$ tiene solución en $\theta$, para todo $\lm \in \Lambda = \R$, con $\Lambda$ la imagen de $T$, pues
                $$T = \lm \iff \dfrac{\overline{X} - \theta}{S / \sqrt{n}} = \lm \iff \overline{X} - \theta = \lm(S / \sqrt{n}) \iff \theta = \overline{X} - \lm(S / \sqrt{n}) \in \Theta = \R$$ 
            \end{enumerate}
            Por un teorema visto en teoría, sabemos que se puede construir un intervalo de confianza para $\theta$ a cualquier nivel
            de confianza $1-\alpha$, con $0 < \alpha < 1$. Por el método de la cantidad pivotal, buscamos $\lm_1 < \lm_2$ 
            verificando $$P_\theta(\lm_1 < T < \lm_2) = 1 - \alpha$$
            Es decir,
            $$\lm_1 < T < \lm_2 \iff \lm_1 < \dfrac{\overline{X} - \theta}{S / \sqrt{n}} < \lm_2 \iff \lm_1(S / \sqrt{n}) < \overline{X} - \theta < \lm_2(S / \sqrt{n}) \iff$$
            $$\lm_1(S / \sqrt{n}) - \overline{X} < - \theta < \lm_2(S / \sqrt{n}) - \overline{X} \iff$$
            $$\overline{X} - \lm_2(S / \sqrt{n}) < \theta < \overline{X} - \lm_1(S / \sqrt{n})$$
            y el intervalo tendrá longitud $L = (\overline{X} - \lm_1(S / \sqrt{n})) - (\overline{X} - \lm_2(S / \sqrt{n})) = (S / \sqrt{n})(\lm_2 - \lm_1)$, que es constante, luego coincidirá 
            con la longitud esperada $\E_\theta[L] = L \quad \forall \theta \in \Theta$. \\
            
            Sea ahora $F$ la función de distribución de la distribución $t$ de Student $t(n-1)$, 
            y sea $f$ la correspondiente función de densidad. La restricción es 
            $$1 - \alpha = P_\theta(\lm_1 < T < \lm_2) = F(\lm_2) - F(\lm_1)$$
            Minimizamos $\lm_2 - \lm_1$ con el método de los multiplicadores de Lagrange visto en teoría
            $$H(\lm_1, \lm_2) = \lm_2 - \lm_1 - \lm[F(\lm_2) - F(\lm_1) - (1-\alpha)]$$
            donde no se ha considerado la parte constante de la longitud media por no afectar al procedimiento de minimización.
            Buscamos aquellos $\lm_1, \lm_2$ que minimicen $H(\lm_1, \lm_2)$, luego obtenemos sus derivadas parciales
            $$\dfrac{\partial H}{\partial \lm_1} = - 1 + \lm f(\lm_1) \quad 
            \dfrac{\partial H}{\partial \lm_2} = 1 - \lm f(\lm_2)$$
            Ahora, igualamos ambas parciales a $0$ y despejamos $\lambda$:
            \begin{equation*}
                \left.\begin{array}{l}
                    0 = \dfrac{\partial H}{\partial \lm_1} = -1+\lm f(\lm_1) \\\\
                    0 = \dfrac{\partial H}{\partial \lm_2} = 1-\lm f(\lm_2)
                \end{array}\right\} \Longrightarrow \left.\begin{array}{l}
                    \lm = \dfrac{1}{f(\lm_1)} \\\\
                    \lm = \dfrac{1}{f(\lm_2)}
                \end{array}\right\} \Longrightarrow f(\lm_1) = f(\lm_2)
            \end{equation*}
            Como la distribución $t$ de Student tiene las mismas propiedades de simetría 
            con respecto al origen que la $\cc{N}(0, 1)$, las únicas posibilidades son o bien 
            $\lm_1 = \lm_2$ o bien $\lm_1 = -\lm_2$. La primera de las dos opciones se descarta ya que no verifica la restricción ($F(\lm_2) - F(\lm_1) = 0 \neq 1-\alpha$), 
            por lo que tiene que ser $\lm_1 = -\lm_2$. Teniendo nuevamente en cuenta la restricción $F(\lm_2) - F(\lm_1) = 1-\alpha$, estos quedan determinados por:
            \begin{equation*}
                \lm_2 = t_{n-1; \nicefrac{\alpha}{2}}, \qquad \lm_1 = -\lm_2 = -t_{n-1; \nicefrac{\alpha}{2}}
            \end{equation*}
            siendo $P[T > t_{n-1; \nicefrac{\alpha}{2}}] = \alpha/2$ y $T \rightsquigarrow t(n-1)$.
            En conclusión, el intervalo de confianza para $\theta$ de mínima longitud esperada uniformemente a nivel de confianza $1-\alpha$ es:
            \begin{equation*}
                \left] \overline{X} - \lm_2(S / \sqrt{n}), \overline{X} - \lm_1(S / \sqrt{n}) \right[ = \left] \overline{X} - t_{n-1; \nicefrac{\alpha}{2}}(S / \sqrt{n}), \overline{X} + t_{n-1; \nicefrac{\alpha}{2}}(S / \sqrt{n}) \right[
            \end{equation*}

        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[1.75 puntos]
        El retraso medio diario (en minutos) de los trabajadores de cierta empresa tiene una distribución $U(0,\theta)$. 
        Para contrastar la hipótesis de que el máximo retraso medio diario es de 5 minutos frente a que es de 4 minutos, 
        se midió dicha variable durante 10 días elegidos al azar, observando que el máximo retraso medio en esos 10 días 
        fue de 3 minutos. Obtener los tests más potentes de tamaños $0.05$ y $0.15$ para el problema de contraste planteado. 
        Decidir, a la vista de lo observado, si debe rechazarse la hipótesis nula en cada uno de los casos. \\

        Sea $X \equiv \text{``Retraso medio diario de los trabajadores de la empresa''}$.
        Nos dicen que $X \rightsquigarrow U(0,\theta)$. Al considerar $(X_1, \ldots, X_n)$ m.a.s. de $X$ con $n=10$, se observó que $X_{(n)} = 3$. 
        El contraste que se quiere resolver es de la forma
        $$\begin{cases}
            H_0:\theta = \theta_0 \\
            H_1:\theta = \theta_1
        \end{cases}$$
        con $4 = \theta_1 < \theta_0 = 5$. Como el contraste es de hipótesis simple frente a hipótesis simple, sabemos por el Lema de Neyman-Pearson que el 
        Test de Neyman-Pearson será el más potente de tamaño $\alpha$, de la forma
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \lambda(X_1, \ldots, X_n) > k \\
            \gamma &\text{si } \lambda(X_1, \ldots, X_n) = k \\
            0 &\text{si } \lambda(X_1, \ldots, X_n) < k
        \end{cases}$$
        para ciertos $\gamma \in [0,1]$, $k \in \R$ y 
        $$\lambda(X_1, \ldots, X_n) = \dfrac{f_1^n(X_1, \ldots, X_n)}{f_0^n(X_1, \ldots, X_n)}$$

        Definimos el espacio muestral y el espacio paramétrico, en ambos casos dependiente del parámetro
        $\theta$. Si estamos en $H_0$, entonces $\X_0 = [0, \theta_0]$, y $\Theta_0 = 
        \{\theta_0\}$. Análogamente, si estamos en $H_1$, entonces $\X_1 = [0, \theta_1]$, y $\Theta_1 = 
        \{\theta_1\}$. Tenemos entonces que $$\Theta = \Theta_0 \cup \Theta_1 = \{\theta_0, \theta_1\}$$ y como
        $\theta_1 < \theta_0$, entonces $[0, \theta_1] \subset [0, \theta_0]$ y
        $$\X = \X_0 \cup \X_1 = [0, \theta_0] \cup [0, \theta_1] = [0, \theta_0]$$
        Consecuentemente $$\X^n = \{(x_1, \ldots, x_n) \in \R^n : 0 \leq x_i \leq \theta_0 \quad \forall i = 1, \ldots, n\} = $$
        $$\{(x_1, \ldots, x_n) \in \R^n : 0 \leq x_{(1)} \quad \land \quad x_{(n)} \leq \theta_0\}$$

        Podemos considerar entonces $(x_1, \ldots, x_n) \in \X^n$, y obtener la función conjunta $f_\theta^n(x_1, \ldots, x_n)$, que es 
        $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^n \frac{1}{\theta}$$ 
        Y vemos que $$x \leq \theta \iff I_{\R_0^{-}}(x - \theta)) = 1$$
        luego $$x_i \leq \theta \quad \forall i = 1, \ldots, n \iff x_{(n)} \leq \theta \iff I_{\R_0^{-}}(x_{(n)} - \theta) = 1$$
        Por tanto $$f_\theta^n(x_1, \ldots, x_n) = \prod_{i=1}^n \frac{1}{\theta} I_{\R_0^{-}}(x_{(n)} - \theta) = $$
        $$\theta^{-n} I_{\R_0^{-}}(x_{(n)} - \theta)$$
        Se tiene entonces que
        $$f_0^n(x_1, \ldots, x_n) = \theta_0^{-n} I_{\R_0^{-}}(x_{(n)} - \theta_0) = \theta_0^{-n} \quad \forall (x_1, \ldots, x_n) \in \X^n$$
        $$f_1^n(x_1, \ldots, x_n) = \theta_1^{-n} I_{\R_0^{-}}(x_{(n)} - \theta_1) \quad \forall (x_1, \ldots, x_n) \in \X^n$$
        y podemos obtener $\lambda(x_1, \ldots, x_n)$, con $(x_1, \ldots, x_n) \in \X^n$: 
        $$\lambda(x_1, \ldots, x_n) = \dfrac{f_1^n(x_1, \ldots, x_n)}{f_0^n(x_1, \ldots, x_n)} = $$$$
        \dfrac{\theta_1^{-n}}{\theta_0^{-n}} I_{\R_0^{-}}(x_{(n)} - \theta_1) = 
        \dfrac{\begin{cases} \theta_1^{-n} &\text{si } x_{(n)} \leq \theta_1 \\ 0 &\text{si } x_{(n)} > \theta_1 \end{cases}}
        {\theta_0^{-n}}$$

        Ahora, si $x_{(n)} \leq \theta_1$, simplificamos

        $$\dfrac{\theta_1^{-n}}{\theta_0^{-n}} = \dfrac{\theta_0^{n}}{\theta_1^{n}} = \left( \dfrac{\theta_0}{\theta_1} \right)^{n}$$

        de donde
        $$\lambda(x_1, \ldots, x_n) = \dfrac{f_1^n(x_1, \ldots, x_n)}{f_0^n(x_1, \ldots, x_n)} = 
        \begin{cases} 
            \left( \dfrac{\theta_0}{\theta_1} \right)^{n} &\text{si } x_{(n)} \leq \theta_1 \\ 
            0 &\text{si } x_{(n)} > \theta_1 
        \end{cases} \quad \forall (x_1, \ldots, x_n) \in \X^n$$

        Gráficamente, como $\theta_1 < \theta_0$:

        \begin{center}
            \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

            \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
                %uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300

                %Straight Lines [id:da9701795441758359] 
                \draw    (70,101) -- (301,102) ;
                %Straight Lines [id:da9876084884828463] 
                \draw    (141,89) -- (141,112) ;
                %Straight Lines [id:da963921157698696] 
                \draw    (210,89) -- (210,112) ;
                %Straight Lines [id:da07701744813997313] 
                \draw    (140.5,41) -- (297.67,41.33) ;
                \draw [shift={(299.67,41.33)}, rotate = 180.12] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da3774624119169432] 
                \draw    (140.5,41) -- (140.5,21.67) ;
                %Straight Lines [id:da9681405401847125] 
                \draw    (140.5,71.5) -- (140.5,51.17) ;
                %Straight Lines [id:da1310029333570385] 
                \draw    (140.5,70.5) -- (72.5,71.23) ;
                \draw [shift={(70.5,71.25)}, rotate = 359.39] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da899862814242277] 
                \draw    (210.5,171.5) -- (210.5,151.17) ;
                %Straight Lines [id:da009971342095128954] 
                \draw    (210.5,170.5) -- (73.5,170.5) ;
                \draw [shift={(71.5,170.5)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da4678439533610015] 
                \draw    (210.5,211.5) -- (210.5,191.17) ;
                %Straight Lines [id:da8507207636884839] 
                \draw    (210.5,210.5) -- (297.5,211.48) ;
                \draw [shift={(299.5,211.5)}, rotate = 180.64] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da9954362463616986] 
                \draw  [dash pattern={on 0.84pt off 2.51pt}]  (140.5,8.5) -- (140.5,211.5) ;
                %Straight Lines [id:da20565414403393933] 
                \draw  [dash pattern={on 0.84pt off 2.51pt}]  (210,8.5) -- (210,211.5) ;

                % Text Node
                \draw (333,59) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \theta _{1} \ < \ \theta _{0}$};
                % Text Node
                \draw (122,103.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \theta _{1}$};
                % Text Node
                \draw (215,104) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \theta _{0}$};
                % Text Node
                \draw (307,92) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle X_{( n)}$};
                % Text Node
                \draw (225,16) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{1} =0$};
                % Text Node
                \draw (83.33,47) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{1}  >0$};
                % Text Node
                \draw (153.33,147) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{0}  >0$};
                % Text Node
                \draw (223.33,188) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{0} =0$};
            \end{tikzpicture}
        \end{center}

        La semirrecta en que $f_0 = 0$, que se corresponde con la condición $x_{(n)} > \theta_0$, no nos interesa 
        ya que $(x_1, \ldots, x_n) \notin \X^n$. Gráficamente, la situación en la que estamos es la siguiente:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}
                % Ejes
                \draw[-Stealth] (-0.2,0) -- (6,0) node[right] {$X_{(n)}$};
                \draw[-Stealth] (0,-0.2) -- (0,3.5) node[left] {$\lm$};

                % Recta con altura 1
                \draw[thick] (0,2.65) -- (6,2.65);

                % Proyección sobre la recta
                \coordinate (P) at (1.5,1.65); % Ajustado para caer sobre la recta dibujada
                \draw[thick, blue] (P) -- (0,1.65);

                \draw[dashed] (1.5,0) -- (1.5,1.65);
                \coordinate (Q) at (1.5,0); % Ajustado para caer sobre la recta dibujada
                \draw (1.5,0) node[below] {$\theta_1$};
                \draw (3.5,0) node[below] {$\theta_0$};
                \coordinate (R) at (3.5,0); % Ajustado para caer sobre la recta dibujada

                \draw[thick, blue] (Q) -- (3.5,0);

                % Etiqueta k en el eje y
                \draw (0,1.65) node[left] {$\left( \dfrac{\theta_0}{\theta_1} \right)^{n}$};
                \draw (0,2.65) node[left] {$1$};

                % Punto en la recta
                \fill (P) circle (2pt);
                % Punto en la recta
                \draw (Q) circle (2pt);

                \fill (R) circle (2pt);
            \end{tikzpicture}
        \end{figure}

        Tenemos que $k \in \left\{0, \left( \dfrac{\theta_0}{\theta_1} \right)^{n} \right\}$, luego distinguimos entre estos dos casos. \\

        Si \boxed{k=0}, entonces el test será:
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \lambda(X_1, \ldots, X_n) > k \iff X_{(n)} \leq \theta_1 \\
            \gamma &\text{si } \lambda(X_1, \ldots, X_n) = k \iff X_{(n)} > \theta_1 \\
            0 &\text{si } \lambda(X_1, \ldots, X_n) < k \text{ nunca}
        \end{cases}$$

        es decir:

        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } X_{(n)} \leq \theta_1 \\
            \gamma &\text{si } X_{(n)} > \theta_1
        \end{cases}$$

        Determinamos $\gamma$ imponiendo tamaño $\alpha$:

        $$\alpha \stackrel{def} = \sup_{\theta \in \Theta_0} \beta_{\varphi}(\theta) =  \sup_{\theta \in \Theta_0} E_\theta[\varphi(X_1, \ldots, X_n)] = E_{\theta_0}[\varphi(X_1, \ldots, X_n)] = $$
        $$1 \cdot P_{\theta_0}[X_{(n)} \leq \theta_1] + \gamma P_{\theta_0}[X_{(n)} > \theta_1] $$

        Para calcular las probabilidades, obtenemos la función de distribución $F_X(t)$:
        $$F_X(t) \stackrel{def}{=} \int_{0}^{t} \dfrac{1}{\theta} dx = \dfrac{1}{\theta} \int_{0}^{t} dx = \dfrac{t}{\theta}$$

        Sabemos por teoría que, para $T = X_{(n)}$, la distribución del máximo verifica $$P_{\theta}[T<t] = F_T(t) = (F_X(t))^n = \left( \dfrac{t}{\theta} \right)^{n}$$

        Por lo tanto, $$P_{\theta_0}[T \leq \theta_1] = \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$$

        y

        $$P_{\theta_0}[T > \theta_1] = 1 - P_{\theta_0}[T \leq \theta_1] = 1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$$

        de donde
        $$\alpha = 1 \cdot P_{\theta_0}[X_{(n)} \leq \theta_1] + 
        \gamma P_{\theta_0}[X_{(n)} > \theta_1] = \left( \dfrac{\theta_1}{\theta_0} \right)^{n} + \gamma \left( 1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n} \right) \iff $$
        $$0 \leq \gamma = \dfrac{\alpha - \left( \dfrac{\theta_1}{\theta_0} \right)^{n}}{1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n}} \leq 1 \iff \alpha \geq \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$$

        y el test resultante es:

        $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                1 &\text{si } X_{(n)} \leq \theta_1 \\
                \dfrac{\alpha - \left( \dfrac{\theta_1}{\theta_0} \right)^{n}}{1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n}} &\text{si } X_{(n)} > \theta_1
            \end{cases}$$      

        Si \boxed{k = \left(\dfrac{\theta_0}{\theta_1} \right)^{n}}, el test será, mirando nuevamente la misma gráfica

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}
                % Ejes
                \draw[-Stealth] (-0.2,0) -- (6,0) node[right] {$X_{(n)}$};
                \draw[-Stealth] (0,-0.2) -- (0,3.5) node[left] {$\lm$};

                % Recta con altura 1
                \draw[thick] (0,2.65) -- (6,2.65);

                % Proyección sobre la recta
                \coordinate (P) at (1.5,1.65); % Ajustado para caer sobre la recta dibujada
                \draw[thick, blue] (P) -- (0,1.65);

                \draw[dashed] (1.5,0) -- (1.5,1.65);
                \coordinate (Q) at (1.5,0); % Ajustado para caer sobre la recta dibujada
                \draw (1.5,0) node[below] {$\theta_1$};
                \draw (3.5,0) node[below] {$\theta_0$};
                \coordinate (R) at (3.5,0); % Ajustado para caer sobre la recta dibujada

                \draw[thick, blue] (Q) -- (3.5,0);

                % Etiqueta k en el eje y
                \draw (0,1.65) node[left] {$\left( \dfrac{\theta_0}{\theta_1} \right)^{n}$};
                \draw (0,2.65) node[left] {$1$};

                % Punto en la recta
                \fill (P) circle (2pt);
                % Punto en la recta
                \draw (Q) circle (2pt);

                \fill (R) circle (2pt);
            \end{tikzpicture}
        \end{figure}

        el que sigue:

        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \lambda(X_1, \ldots, X_n) > k \text{ nunca} \\
            \gamma &\text{si } \lambda(X_1, \ldots, X_n) = k \iff X_{(n)} \leq \theta_1 \\
            0 &\text{si } \lambda(X_1, \ldots, X_n) < k \iff X_{(n)} > \theta_1
        \end{cases}$$

        es decir:
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            \gamma &\text{si } X_{(n)} \leq \theta_1 \\
            0 &\text{si } X_{(n)} > \theta_1
        \end{cases}$$

        Determinamos $\gamma$ igual que antes imponiendo tamaño $\alpha$:

        $$\alpha \stackrel{def} = \sup_{\theta \in \Theta_0} \beta_{\varphi}(\theta) =  \sup_{\theta \in \Theta_0} E_\theta[\varphi(X_1, \ldots, X_n)] = E_{\theta_0}[\varphi(X_1, \ldots, X_n)] = $$
        $$\gamma \cdot P_{\theta_0}[X_{(n)} \leq \theta_1] + 0 \cdot P_{\theta_0}[X_{(n)} > \theta_1] = \gamma \cdot P_{\theta_0}[X_{(n)} \leq \theta_1] $$

        Ya sabemos del caso anterior que 
        $$P_{\theta_0}[T \leq \theta_1] = \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$$

        luego
        $$\alpha = \gamma \cdot P_{\theta_0}[X_{(n)} \leq \theta_1] = \gamma \left( \dfrac{\theta_1}{\theta_0} \right)^{n} \iff $$
        $$0 \leq \gamma = \dfrac{\alpha}{\left( \dfrac{\theta_1}{\theta_0} \right)^{n}} = \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{n}  \leq 1 \iff \alpha \leq \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$$

        y el test resultante es:

        $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{n} &\text{si } X_{(n)} \leq \theta_1 \\
                0 &\text{si } X_{(n)} > \theta_1
            \end{cases}$$      

        Sustituimos ahora los datos del enunciado, $\theta_0 = 5$, $\theta_1 = 4$, $n=10$, y $X_{(n)} = 3$. Distinguimos según el tamaño pedido:
        \begin{itemize}
            \item $\alpha = 0.05$. Como 
            $$0.05 = \alpha \leq \left( \dfrac{\theta_1}{\theta_0} \right)^{n} = \left( \dfrac{4}{5} \right)^{10} \approx 0,1074$$
            el test más potente en este caso es
            $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{n} &\text{si } X_{(n)} \leq \theta_1 \\
                0 &\text{si } X_{(n)} > \theta_1
            \end{cases}$$  
            y sustituyendo
            $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                0.05 \left( \dfrac{5}{4} \right)^{10} &\text{si } X_{(n)} \leq 4 \\
                0 &\text{si } X_{(n)} > 4
            \end{cases}$$  
            como $3 = X_{(n)} \leq 4$, se rechaza $H_0$ con probabilidad:
            $$0.05 \left( \dfrac{5}{4} \right)^{10} \approx 0,4657$$
            \item $\alpha = 0.15$. Como 
            $$0.15 = \alpha \geq \left( \dfrac{\theta_1}{\theta_0} \right)^{n} = \left( \dfrac{4}{5} \right)^{10} \approx 0,1074$$
            el test más potente en este caso es
            $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                1 &\text{si } X_{(n)} \leq \theta_1 \\
                \dfrac{\alpha - \left( \dfrac{\theta_1}{\theta_0} \right)^{n}}{1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n}} &\text{si } X_{(n)} > \theta_1
            \end{cases}$$    
            y sustituyendo
            $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                1 &\text{si } X_{(n)} \leq 4 \\
                \dfrac{0.15 - \left( \dfrac{5}{4} \right)^{10}}{1 - \left( \dfrac{5}{4} \right)^{10}} &\text{si } X_{(n)} > 4
            \end{cases}$$  
            como $3 = X_{(n)} \leq 4$, se rechaza $H_0$ con probabilidad $1$.
        \end{itemize}

    \end{ejercicio}

    \newpage

    \begin{ejercicio}[1.5 puntos]~
        \begin{enumerate}
            \item[a)] Test de Kolmogorov--Smirnov. Comparación con $\chi^2$. \\

            Comenzamos describiendo el Test de Kolmogorov--Smirnov. Sea una función de distribución específica $F_0$, y sea $(X_1, \ldots, X_n)$ una m.a.s. 
            de una v.a. $X$ continua que se distribuye según una función de distribución $F$ que es completamente desconocida. 
            El constraste a resolver es
            $$\begin{cases}
                H_0 : F = F_0 \\
                H_1 : F \neq F_0
            \end{cases}$$
            El estadístico que se usa para resolver el problema es el estadístico de Kolmogorov-Smirnov

            $$D(X_1, \ldots, X_n) = \sup_{x \in \R} |F_{X_1, \ldots, X_n}^{*}(x) - F_0(x)|$$

            Intuitivamente, este mide la máxima distancia vertical entre la función de distribución empírica $F_{X_1, \ldots, X_n}^{*}$ y la función de distribución que se está contrastando como teórica $F_0$.
            Más teóricamente, este test se basa en el teorema de Glivenko-Cantelli: \\

            \begin{teo}[de Glivenko-Cantelli]
                Sea $\{X_n\}_{n \in \N}$ una sucesión de v.a.i.i.d. con función de distribución común $F$. Si $F_{X_1, \ldots, X_n}^{*}$
                es la función de distribución muestral asociada a la m.a.s. $(X_1, \ldots, X_n)$, se verifica que $F_{X_1, \ldots, X_n}^{*}$
                converge casi seguramente y uniformemente a la función de distribución de $X$, $F$.
                $$P \left\{ \lim\limits_{n \to +\infty} \sup_{x \in \R} |F_{X_1, \ldots, X_n}^{*}(x) - F(x)| = 0 \right\} = 1$$
            \end{teo}

            Por otro lado, el test $\chi^2$ de Pearson se basa en comparar frecuencias observadas y esperadas. Para ello, requiere una agrupación en $k$ grupos distintos (al menos $5$ en la práctica).
            Resuelve el mismo contraste que el test de Kolmogorov--Smirnov si la hipótesis nula es simple. Si fuera compuesta, deben estimarse los parámetros de los que depende $F_0$ (normalmente
            por máxima verosimilitud) y luego aplicar el test $\chi^2$ de Pearson a esa adaptación. El estadístico de contraste es
            $$\chi^2 = \sum_{i=1}^{k} \dfrac{(N_i - E_i)^2}{E_i}$$
            La aproximación que hace este test es asintótica y en la práctica se exigen frecuencias esperadas $E_i \geq 5$ para cada $i \in \{1, \ldots, k\}$. \\

            \newpage

            Algunas ventajas del test de Kolmogorov--Smirnov respecto al test $\chi^2$ de Pearson son las siguientes:
                
            \begin{itemize}
                \item El test de Kolmogorov-Smirnov no necesita hacer particiones de los datos, mientras que el test $\chi^2$ de Pearson sí.
                \item Para v.a. continuas, es más apropiado usar el test de Kolmogorov-Smirnov que el test $\chi^2$ de Pearson.
                \item Bajo $H_0$, si $F_0$ es continua, la distribución del estadístico de Kolmogorov-Smirnov no depende de $F_0$, mientras
                que el test $\chi^2$ de Pearson es asintótico.
            \end{itemize}

            De igual manera, algunas ventajas del test $\chi^2$ de Pearson respecto al test de Kolmogorov--Smirnov son las siguientes:

            \begin{itemize}
                \item El test $\chi^2$ puede aplicarse tanto a variables aleatorias discretas como continuas (estas últimas tras agrupar en clases), 
                mientras que el test de Kolmogorov--Smirnov está pensado fundamentalmente para distribuciones continuas.
                \item El test $\chi^2$ permite dar mayor peso a determinadas regiones (por ejemplo, las colas) mediante una elección adecuada de las particiones, 
                mientras que el test de Kolmogorov--Smirnov se basa únicamente en la distancia máxima y no distingue zonas. 
            \end{itemize}

            
            \item[b)] Se recogen observaciones de la facturación (en miles de euros) de un cierto supermercado durante $25$ días.
            Se quiere contrastar si la facturación se distribuye uniformemente.
            \begin{center}
                \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
                    10.1 & 7.3 & 4.2 & 3.1 & 8.4 & 9.2 & 1.1 & 1 & 8.1 & 6.4 & 10.5 & 1.2 \\
                    \hline
                \end{tabular}
                \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
                    1.8 & 2.6 & 2.2 & 3.9 & 2.9 & 6.5 & 8.1 & 8.3 & 3.8 & 4.2 & 7.3 & 8.9 & 9.6
                \end{tabular}
            \end{center}

            En este caso, aunque la variable aleatoria es continua, dado que tenemos frecuencias, es más apropiado usar el test $\chi^2$ de Pearson.
            Sea $$X \equiv \text{``Facturación del supermercado en un día''}$$ El contraste a resolver es
            $$\begin{cases}
                H_0 : X \rightsquigarrow U(a,b) \\
                H_1 : X \not\rightsquigarrow U(a,b)
            \end{cases} \quad a,b \in \R : a < b$$
            Vemos que la hipótesis nula es compuesta, luego primeramente debemos estimar el valor de los parámetros $a$ y $b$. 
            Sabemos que el EMV de $a$ es $\hat{a} = X_{(1)} = 1$ y el EMV de $b$ es $\hat{b} = X_{(n)} = 10.5$. El constraste adaptado sería 
            $$\begin{cases}
                H_0 : X \rightsquigarrow U(1,10{.}5) \\
                H_1 : X \not\rightsquigarrow U(1,10{.}5)
            \end{cases}$$
            Denotemos por $N_1, \ldots, N_k$ las frecuencias observadas en las $k$ clases consideradas, y por
            $$\hat{p}_i = P_{\hat{a}, \hat{b}}(X \in A_i), \quad i = 1, \ldots, k$$
            las probabilidades teóricas bajo $H_0$ con los parámetros $a$ y $b$ estimados por $\hat{a}$ y $\hat{b}$, respectivamente. \\

            El estadístico de contraste viene dado por

            $$\hat{\chi}(N_1, \ldots, N_k) = \sum_{i=1}^k \dfrac{(N_i - n \hat{p}_i)^2}{n \hat{p}_i}$$

            Como el parámetro se ha estimado a partir de los mismos datos, la distribución asintótica bajo $H_0$ es 
            $$\hat{\chi}(N_1, \ldots, N_k) \rightsquigarrow_{H_0} \chi^2(k - q - 1)$$
            con $q=2$ el número de parámetros estimados.

            Por teoría, para poder aplicar el test hay que verificar que $$E^*_{i} = n\hat{p}_i \geqslant 5 \quad \forall i = 1, \ldots, k, \quad n = 25, \quad \hat{p}_i = \int_{c_{i-1}}^{c_i} \dfrac{1}{\hat{b} - \hat{a}}dx = 
            \dfrac{c_{i} - c_{i-1}}{\hat{b} - \hat{a}}$$
            donde $A_i = [c_{i-1}, c_i[$, y $\hat{a} \leq c_{i} \leq c_{j-1} < c_j \leq \hat{b} \quad \forall i<j = 1, \ldots, k$. Como 
            $$E_i = n\hat{p}_i = n \dfrac{c_{i} - c_{i-1}}{\hat{b} - \hat{a}} = 25 \dfrac{c_{i} - c_{i-1}}{10.5 - 1} = 25 \dfrac{c_{i} - c_{i-1}}{9.5}$$
            podemos agrupar por medio de los percentiles. Se tomarán $k$ clases con $\hat{p}_i = 1 / k$, de tal manera que $E_i = n / k$, para cada $i = 1, \ldots, k$. 
            Como $n = 25$, será $E_i \geq 5$ si $k \leq 5$. Para que sean al menos $5$ grupos, debe ser $k=5$, y
            $$E_i = 5 \quad \forall i = 1, \ldots, k$$

            Los extremos de los grupos (intervalos), en este caso los quintiles, vienen dados por $c_i = 1 + 9.5 \frac{i}{5}$:
            
            \begin{center}
                \begin{tabular}{c|c|c|c|c}
                    $c_1$ & $c_2$ & $c_3$ & $c_4$ & $c_5$ \\
                    \hline
                    1 & 2.9 & 4.8 & 6.7 & 8.6
                \end{tabular}
            \end{center}

            Tenemos entonces la siguiente partición de $k=5$ intervalos $$A_i = [c_{i-1}, c_{i}[ \quad i = 1, \ldots, k-1 \quad A_k = [c_{k}, 10.5]$$

            Ahora hay que contar las frecuencias observadas $N_i$ de cada quintil, con \\ $i = 1, \ldots, k$. Vemos que
            \begin{center}
                \begin{tabular}{c|c|c|c|c}
                    $N_1$ & $N_2$ & $N_3$ & $N_4$ & $N_5$ \\
                    \hline
                    6 & 6 & 2 & 6 & 5
                \end{tabular}
            \end{center}

            y obtenemos $$\chi^2_{exp} = \sum_{i=1}^{5} \dfrac{(N_i - E_i)^2}{E_i} = \dfrac{(6 - 5)^2}{5} + \dfrac{(6 - 5)^2}{5} +
            \dfrac{(2 - 5)^2}{5} + \dfrac{(4 - 5)^2}{5} + $$$$\dfrac{(5 - 5)^2}{5} = \dfrac{12}{5} = 2.4$$

            El test asintótico de tamaño $\alpha$ es

            $$\varphi(X_1, \ldots, X_n) = \begin{cases}
                1 &\text{si } \hat{\chi}^2(N_1, \ldots, N_k) \geqslant \chi^2_{k-1;\alpha} \\
                0 &\text{si } \hat{\chi}^2(N_1, \ldots, N_k) < \chi^2_{k-1;\alpha}
            \end{cases}$$

            con $$p-valor = P_{H_0}[\hat{\chi}^2(N_1, \ldots, N_k) \geqslant \chi^2_{exp}] \approx_{n \to +\infty} P[\chi^2(k-q-1) \geqslant \chi^2_{exp}]$$

            y $\chi^2_{exp}$ el valor del estadístico obtenido con la muestra observada. Usando que $k-q-1=2$, obtenemos
            $$p-valor \approx P[\chi^2(2) \geqslant 2.4] \approx 0.3$$

            Como el $p-valor$ es grande (respecto a los niveles habituales de significación), no se rechaza $H_0$, por lo que 
            puede suponerse que la facturación del supermercado en un día se distribuye uniformemente.

        \end{enumerate}
    \end{ejercicio}

\end{document}
