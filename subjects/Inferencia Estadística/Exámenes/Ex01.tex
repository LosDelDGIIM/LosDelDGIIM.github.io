\documentclass[12pt]{article}

\input{../../_assets/preambulo.tex}

\newcommand{\E}{\mathrm{E}}
\newcommand{\V}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rmas}{\R^{+}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\I}{\mathrm{Im}}
\newcommand{\Rmenos}{\R^{-}}
\newcommand{\norma}[1]{\lVert #1 \rVert}

\begin{document}

    % 1. Foto de fondo
    % 2. Título
    % 3. Encabezado Izquierdo
    % 4. Color de fondo
    % 5. Coord x del titulo
    % 6. Coord y del titulo
    % 7. Fecha

    
    \input{../../_assets/portada}
    \portadaExamen{ffccA4.jpg}{Inferencia \\ Estadística\\Examen I}{Inferencia Estadística. Examen I}{MidnightBlue}{-8}{28}{2026}{José Manuel Sánchez Varbas}

    \begin{description}
        \item[Asignatura] Inferencia Estadística.
        \item[Curso Académico] 2025-26.
        \item[Grado] Grado en Matemáticas y Grado en Ingeniería Informática y Matemáticas.
        \item[Descripción] Examen Ordinario.
        \item[Fecha] 9 de Enero de 2026.  
    \end{description}
    \newpage


    % ------------------------------------

    \begin{ejercicio}[1.9 puntos]
        Sea $X$ una variable aleatoria con función de densidad:
        \begin{equation*}
            f_\theta(x) = \frac{x}{\sqrt{\theta-1}\sqrt{x^2-1}}, \qquad 1<x\leq \sqrt{\theta}
        \end{equation*}
        se pide calcular, si existe, un UMVUE para la función paramétrica $g(\theta) = {(\theta-1)}^{-1}$, y justificar detalladamente la no existencia del mismo cuando no exista.
    \end{ejercicio}

    \begin{ejercicio}[2 puntos]
        Sea $X$ una variable aleatoria con distribución en la familia $\{P_\theta:\theta \in \Theta\}$ que se sabe que es regular y cuyas funciones de densidad vienen dadas por:
        \begin{equation*}
            f_\theta(x) = exp[k_1 \ln\theta - k_2 x \theta + S(x)], \qquad x>0, \quad \theta,k_1,k_2\in \mathbb{R}^+
        \end{equation*}
        sabiendo que $Var_\theta(X) = {(E_\theta[X])}^{2}$.
        \begin{enumerate}[label=\alph*)]
            \item ¿Para qué valores de $n$ se puede asegurar que cualquier estimador regular insesgado en $g(\theta) = \ln\theta^2$ tiene varianza mayor o igual que $0.2$ para cualquier valor del parámetro $\theta$?
            \item Para $n=1$, si $U(X)$ es un estimador insesgado de $g(\theta)=\ln\theta^2$ regular, se pide calcular la covarianza de $U(X)$ y de $X$.
            \item ¿Para qué valores de $k_1$ y $k_2$ existen funciones paramétricas con estimadores eficientes?
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}[1.85 puntos]
        Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable $X\rightsquigarrow \{P_\theta:\theta \in \mathbb{R}\}$ y $S\equiv S(X_1,\ldots,X_n)$ un estimador de $\theta$:
        \begin{enumerate}[label=\alph*)]
            \item Si $S-\theta\rightsquigarrow \cc{N}(0,\sigma_0^2)\quad \forall \theta\in \mathbb{R}$. Partiendo de la función de verosimilitud de $\theta$ asociada a una realización de $S$, calcular la función de verosimilitud asociada a la función $\lm = \theta^2-1$ y deducir a partir de ella el estimador máximo verosímil de $\lm$.
            \item Si $S-\theta \rightsquigarrow t(n)\quad \forall \theta \in \mathbb{R}$. Encontrar el intervalo de confianza para $\theta$ de mínima longitud esperada a nivel de confianza $1-\alpha$ basado en $S$.
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}[2 puntos]
        Sea $X$ una variable aleatoria con función de densidad
        \begin{equation*}
            f_\theta(x) = \frac{3{(x-1)}^{2}}{\theta^3}, \qquad 1<x<\theta +1
        \end{equation*}
        se pide obtener el test más potente de tamaño $\alpha$ que permita resolver el contraste $H_0:\theta = \theta_0$, $H_1:\theta=\theta_1$ donde $\theta_1<\theta_0$.\\

        \noindent
        Calcule la potencia de cada test. Para $n=2$ y $\theta_0=9$, obtener el mayor valor de $\theta_1$ para que la potencia del test de Neyman-Pearson de tamaño $0.01$ sea mayor o igual que $0.64$.
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[1.25 puntos]
        Si se tiene un modelo lineal de Gauss-Markov $Y=X\beta + \varepsilon$:
        \begin{enumerate}[label=\alph*)]
            \item Si el modelo es de rango máximo, dar el estimador de mínimos cuadrados de $\beta$ y calcular la media del vector de residuos del modelo estimado, solo con las condiciones iniciales del modelo.
            \item Definir el concepto de función estimable y enunciar el Teorema de Gauss-Markov.
            \item Describir la hipótesis lineal general y bajo hipótesis de normalidad, dar el test de razón de verosimilitudes de tamaño $\alpha$ que permite resolver el contraste, especificando detalladamente el estadístico de contraste.
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}[1 punto]
        Se ha medido el número de particulas de $100$ muestras radioactivas en un intervalo de tiempo prefijado e igual a todas las muestras, obteniendo los siguientes datos:
        \begin{table}[H]
        \centering
        \begin{tabular}{c|ccccccc}
            Número de partículas & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
            \hline
            Número de muestras & 29 & 25 & 20 & 14 & 8 & 3 & 1
        \end{tabular}
        \end{table}
        \noindent
        Se pretende contrastar a nivel de significación $0.05$ si la distribución de los datos se corresponde con la de una Poisson.
    \end{ejercicio}

    \newpage
    \setcounter{ejercicio}{0} % Reiniciar contador de ejercicios
    \noindent
    \textbf{Solución.}

    \begin{ejercicio}[1.9 puntos]
        Sea $X$ una variable aleatoria con función de densidad:
        \begin{equation*}
            f_\theta(x) = \frac{x}{\sqrt{\theta-1}\sqrt{x^2-1}}, \qquad 1<x\leq \sqrt{\theta}
        \end{equation*}
        se pide calcular, si existe, un UMVUE para la función paramétrica $g(\theta) = {(\theta-1)}^{-1}$, y justificar detalladamente la no existencia del mismo cuando no exista. \\

        Buscamos obtener el UMVUE mediante el método alternativo visto en teoría. Para ello, en primer lugar hay que encontrar un estadístico suficiente y completo $T$, y luego una función del estadístico $h(T)$
        (denotaremos indistintamente $T \stackrel{not}{\equiv} T(X_1, \ldots, X_n)$, para una m.a.s. $(X_1, \ldots, X_n)$ con $n \in \N$ fijo) insesgada en $g(\theta) = (\theta-1)^{-1}$, estimadora y con momento de segundo orden finito. Entonces $h(T)$ será el UMVUE. \\

        El estadístico suficiente se calcula por medio del Teorema de Factorización de Neyman-Fisher. La función conjunta es la siguiente
        $$f^n_\theta(x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^{n} f_\theta(x_i)$$
        Suponemos en este punto que $x_{(1)} > 1$ (de lo contrario, $f_\theta(x_i) = 0 \quad \forall i = 1, \ldots, n$), y vemos que 
        $$x_i \leq \sqrt{\theta} \quad \forall i = 1, \ldots, n \iff I_{\R_0^{-}}(x_i - \sqrt{\theta}) = 1 \quad \forall i = 1, \ldots, n \iff I_{\R_0^{-}}(x_{(n)} - \sqrt{\theta}) = 1$$ de donde se deduce que 
        $$f^n_\theta(x_1, \ldots, x_n) = \prod_{i=1}^{n} \frac{x_i}{\sqrt{\theta-1}\sqrt{x_i^2-1}} I_{\R_0^{-}}(x_{(n)} - \sqrt{\theta}) = 
        (\theta-1)^{-n/2} \prod_{i=1}^{n} \frac{x_i}{\sqrt{x_i^2-1}} I_{\R_0^{-}}(x_{(n)} - \sqrt{\theta})$$
        Tomando $T(X_1, \ldots, X_n) = X_{(n)}$ y 
        $$h(x_1, \ldots, x_n) = \prod_{i=1}^{n} \frac{x_i}{\sqrt{x_i^2-1}}, \quad g_\theta(t) = 
        (\theta-1)^{-n/2} I_{\R_0^{-}}(t - \sqrt{\theta})$$
        Se cumple que 
        $$f^n_\theta(x_1, \ldots, x_n) = h(x_1, \ldots, x_n) g_\theta(T(x_1, \ldots, x_n)) \quad \forall (x_1, \ldots, x_n) \in \X^n$$
        donde $h$ es independiente del parámetro $\theta$ y $g_\theta$ depende de la muestra solo a través del estadístico, luego,
        por el Teorema de Factorización de Neyman-Fisher, el estadístico $T$ es suficiente. 

        \newpage

        Ahora, hay que comprobar que este estadístico es completo, lo cual se hará por definición. Sabemos por teoría que la distribución del máximo es 
        $$F_T(t) = (F_X(t))^n \Longrightarrow f_T(t) = n(F_X(t))^{n-1} f_\theta(t)$$
        Hallamos ahora la función de distribución de $X$:
        $$F_X(t) = \int_{1}^{t} f_\theta(x) dx = \int_{1}^{t} \frac{x}{\sqrt{\theta-1}\sqrt{x^2-1}} dx = 
        \dfrac{1}{\sqrt{\theta-1}} \int_{1}^{t} \frac{x}{\sqrt{x^2-1}} dx = \dfrac{1}{\sqrt{\theta-1}} \cdot \left[ \sqrt{x^2-1} \right]_{1}^{t} = $$
        $$\dfrac{\sqrt{t^2-1}}{\sqrt{\theta-1}} \quad 1 < t \leqslant \sqrt{\theta}$$

        La función de densidad del estadístico será entonces
        $$f_T(t) = n(F_X(t))^{n-1} f_\theta(t) = n \left(\dfrac{\sqrt{t^2-1}}{\sqrt{\theta-1}}\right)^{n-1} \frac{t}{\sqrt{\theta-1}\sqrt{t^2-1}} = $$
        $$\dfrac{(\sqrt{t^2-1})^{n-1}}{(\sqrt{\theta-1})^{n-1}} \frac{nt}{\sqrt{\theta-1}\sqrt{t^2-1}} = \dfrac{nt(\sqrt{t^2-1})^{n-2}}{(\sqrt{\theta-1})^{n}} \quad 1 < t \leq \sqrt{\theta}$$

        Sea $h$ una función medible verificando $$0 = \E[h(T)] \stackrel{def}{=} \int_{1}^{\sqrt{\theta}} h(t) f_T(t) dt = 
        \int_{1}^{\sqrt{\theta}} h(t) \dfrac{nt(\sqrt{t^2-1})^{n-2}}{(\sqrt{\theta-1})^{n}} dt =$$
        $$\dfrac{n}{(\sqrt{\theta-1})^{n}} \int_{1}^{\sqrt{\theta}} h(t) t(\sqrt{t^2-1})^{n-2} dt$$
        como $\dfrac{n}{(\sqrt{\theta-1})^{n}} \neq 0 \quad \forall n \in \N, \quad \forall \theta>1$, debe ser
        $$\int_{1}^{\sqrt{\theta}} h(t) t(\sqrt{t^2-1})^{n-2} dt = 0$$
        Por el Teorema Fundamental del Cálculo, podemos considerar una primitiva $H(t)$ del integrando $h(t) t(\sqrt{t^2-1})^{n-2}$, y esta cumple,
        por la Regla de Barrow, que $H(\sqrt{\theta}) - H(1) = 0 \quad \forall \theta > 1$. Derivando respecto 
        de $\theta$, se obtiene que
        $$\dfrac{d}{d \theta} H(\sqrt{\theta}) = 0 \iff h(\sqrt{\theta}) \sqrt{\theta}(\sqrt{\theta-1})^{n-2} \dfrac{1}{2 \sqrt{\theta}} = 0 \stackrel{\theta > 1}{\iff}$$
        $$\dfrac{1}{2} h(\sqrt{\theta}) (\sqrt{\theta-1})^{n-2} = 0 \stackrel{(*)}{\iff} h(\sqrt{\theta}) = 0$$
        donde en $(*)$ se ha usado que $(\sqrt{\theta-1})^{n-2} / 2 \neq 0$ por ser $\theta > 1$. Equivalentemente, 
        $$\forall \theta \in \Theta = \left] 1, + \infty \right[ \quad h(\sqrt{\theta}) = 0 \iff h(t) = 0 \quad \forall t > 1$$ (tomando $t = \sqrt{\theta} \in \left]1, +\infty \right[$).
        Por tanto $$\left]1, +\infty \right[ \subseteq \{t : h(t) = 0\}$$ y consecuentemente $$1 \geqslant P[h(T) = 0] \geqslant P[T > 1] = 1
        \Longrightarrow P[h(T) = 0] = 1$$

        y entonces por definición concluimos que $T$ es un estadístico completo. Tenemos entonces en este punto
        que $T$ es un estadístico suficiente y completo. 

        \newpage

        Ahora hay que buscar un estimador insesgado en $g(\theta)$ y de segundo orden finito. Sea $h$ (independiente de la anterior) función medible tal que
        $$(\theta-1)^{-1} = g(\theta) = \E[h(T)] = \dfrac{n}{(\sqrt{\theta-1})^{n}} \int_{1}^{\sqrt{\theta}} h(t) t(\sqrt{t^2-1})^{n-2} dt \iff$$
        $$\int_{1}^{\sqrt{\theta}} h(t) t(\sqrt{t^2-1})^{n-2} dt = \dfrac{(\sqrt{\theta-1})^{n}}{n} (\theta-1)^{-1} = \dfrac{(\theta-1)^{\frac{n}{2}-1}}{n} $$

        Derivamos respecto de $\theta$ a ambos lados e igualamos. El miembro izquierdo ya lo tenemos por el apartado anterior:
        $$\dfrac{1}{2} h(\sqrt{\theta}) (\sqrt{\theta-1})^{n-2}$$
        y el derecho es 
        $$\dfrac{1}{n}\left( \dfrac{n}{2} - 1 \right)(\theta-1)^{\frac{n}{2}-2} = \left( \dfrac{1}{2} - \dfrac{1}{n} \right) (\theta-1)^{\frac{n}{2}-2} = \dfrac{n-2}{2n} (\theta-1)^{\frac{n}{2}-2}$$
        Despejamos $h(\sqrt{\theta})$:
        $$\dfrac{1}{2} h(\sqrt{\theta}) (\sqrt{\theta-1})^{n-2} = \dfrac{n-2}{2n} (\theta-1)^{\frac{n}{2}-2} \iff $$
        $$h(\sqrt{\theta}) = \dfrac{n-2}{n} \dfrac{(\theta-1)^{\frac{n}{2}-2}}{(\sqrt{\theta-1})^{n-2}} = 
        \dfrac{n-2}{n} \dfrac{(\theta-1)^{\frac{n}{2}-2}}{(\theta-1)^{(n-2)/2}} = \dfrac{n-2}{n} \dfrac{1}{\theta-1}$$
        de donde
        $$h(t) = \dfrac{n-2}{n} \dfrac{1}{t^2-1}$$
        Por construcción $h(T)$ es insesgada en $g(\theta)$. Vemos que $h(T)$ también es estimador de $g(\theta)$, 
        pues $\Theta = \left] 1, +\infty \right[$, y $g(\theta) = \frac{1}{\theta-1} \Longrightarrow g(\Theta) = \left]0,+\infty \right[$. 
        Como $T = X_{(n)} > 1$, entonces $T^2 - 1 > 0$, y $\frac{n-2}{n} > 0$ si $n \geq 3$, luego $h(T) > 0$ si $n \geq 3$. Queda comprobar que tiene momento de segundo orden finito. \\

        Ello se cumplirá en caso de que $E[h(T)^2] < + \infty$:
        $$E[h(T)^2] \stackrel{def}{=} \int_{1}^{\sqrt{\theta}} h(t)^2 f_T(t) dt = \int_{1}^{\sqrt{\theta}} 
        \left( \dfrac{n-2}{n} \right)^2 \dfrac{1}{(t^2-1)^2} \dfrac{nt(\sqrt{t^2-1})^{n-2}}{(\sqrt{\theta-1})^{n}} dt = $$
        $$\dfrac{(n-2)^2}{n(\theta-1)^{n/2}} \int_{1}^{\sqrt{\theta}} t(t^2-1)^{(n-6)/2}dt = 
        \dfrac{(n-2)^2}{n(\theta-1)^{n/2}} \dfrac{1}{(n-4)} \left[(t^2-1)^{(n-4)/2} \right]_1^{\sqrt{\theta}} = $$
        $$\dfrac{(n-2)^2}{n(n-4)} \dfrac{(\theta-1)^{(n-4)/2}}{(\theta-1)^{n/2}} = \dfrac{(n-2)^2}{n(n-4)} \dfrac{1}{(\theta-1)^2}$$

        Y vemos que $E[h(T)^2] < + \infty \iff n > 4$, ya que si $n \leq 4$, el momento de segundo orden no es finito.
        Por tanto, por el Teorema de Lehmann-Scheffé, $E[h(T)/T] = h(T)$ es el UMVUE para $g(\theta)$, y existe siempre y
        cuando $n>4 \iff n \geq 5$.
    \end{ejercicio}

    \begin{ejercicio}[2 puntos]
        Sea $X$ una variable aleatoria con distribución en la familia $\{P_\theta:\theta \in \Theta\}$ que se sabe que es regular y cuyas funciones de densidad vienen dadas por:
        \begin{equation*}
            f_\theta(x) = exp[k_1 \ln\theta - k_2 x \theta + S(x)], \qquad x>0, \quad \theta,k_1,k_2\in \mathbb{R}^+
        \end{equation*}
        sabiendo que $Var_\theta(X) = {(E_\theta[X])}^{2}$.
        \begin{enumerate}[label=\alph*)]
            \item ¿Para qué valores de $n$ se puede asegurar que cualquier estimador regular insesgado en $g(\theta) = \ln\theta^2$ tiene varianza mayor o igual que $0.2$ para cualquier valor del parámetro $\theta$? \\
            
            Nos piden calcular la cota de Frechét-Crámer-Rao, definida para cualquier estimador regular, insesgado en $g(\theta)$ y, suponemos, de segundo orden, $T$, como sigue:
            $$\V(T(X_1, \ldots, X_n)) \geq \dfrac{[g'(\theta)]^2}{I_{X_1, \ldots, X_n}(\theta)}$$
            donde $0 < I_X(\theta) < + \infty$. Ya tenemos el numerador $$g(\theta) = \ln(\theta^2) = 2 \ln \theta \Longrightarrow g'(\theta) = \dfrac{2}{\theta} \Longrightarrow [g'(\theta)]^2 = \dfrac{4}{\theta^2}$$
            Y por la aditividad de la función de información de Fisher, sabemos que $I_{X_1, \ldots, X_n}(\theta) = n I_X(\theta)$. Sacamos $I_{X}(\theta)$. \\

            Para usar la condición de regularidad, se obtiene
            $$\ln f_\theta(x) = k_1 \ln\theta - k_2 x \theta + S(x) \Longrightarrow \dfrac{\partial \ln f_\theta(X)}{\partial \theta} = \dfrac{k_1}{\theta} - k_2 x$$
            Usando que la familia es regular
            $$\E_\theta \left[ \dfrac{\partial \ln f_\theta(X)}{\partial \theta} \right] = 0 \iff 
            \E_\theta \left[ \dfrac{k_1}{\theta} - k_2 X \right] = 0 \iff
            \E_\theta \left[ \dfrac{k_1}{\theta} \right] - \E_\theta [k_2 X] = 0 \iff$$
            $$\dfrac{k_1}{\theta} = k_2 \E_\theta[X] \iff E_\theta[X] = \dfrac{k_1}{k_2 \theta}$$

            Sabemos por teoría que 
            $$I_X(\theta) = \V_\theta \left[ \dfrac{\partial \ln f_\theta(X)}{\partial \theta} \right]$$
            y
            $$\V_\theta \left[ \dfrac{\partial \ln f_\theta(X)}{\partial \theta} \right] = \V_\theta \left[ \dfrac{k_1}{\theta} - k_2 X \right] \stackrel{(*)}{=}
            k_2^2\V_\theta[X] \stackrel{(**)}{=} k_2^2 (\E_\theta[X])^2 = k_2^2 \left( \dfrac{k_1}{k_2 \theta} \right)^2 \stackrel{k_2>0}{=} \dfrac{k_1^2}{\theta^2}$$
            donde en $(*)$ se ha usado que $\V(aX + b) = a^2 \V(X)$ para todo $a,b \in \R$, y en $(**)$ que $\V_\theta(X) = {(\E_\theta[X])}^{2}$ por hipótesis del enunciado.

            Tenemos que $0 < I_X(\theta) = \nicefrac{k_1^2}{\theta^2} < + \infty$, y la mínima varianza es entonces, sabiendo que $I_{X_1, \ldots, X_n}(\theta) = n I_X(\theta)$, la que sigue:

            $$\dfrac{[g'(\theta)]^2}{I_{X_1, \ldots, X_n}(\theta)} = \dfrac{[g'(\theta)]^2}{n I_{X}(\theta)} = \dfrac{4}{\theta^2} \dfrac{\theta^2}{n k_1^2} \stackrel{\theta>0}{=} \dfrac{4}{n k_1^2}$$

            Imponemos que esta cota inferior sea mayor o igual que $0{.}2$, y despejamos $n$:

            $$\dfrac{4}{n k_1^2} \geq 0{.}2 \iff  n \leq \dfrac{4}{0{.}2 k_1^2} = \dfrac{20}{k_1^2}$$

            Así, para todo $n \in \N$ con $n \leq \nicefrac{20}{k_1^2}$, podemos asegurar que la varianza de cualquier estimador regular insesgado en $g(\theta) = \ln\theta^2$ será mayor o igual que $0.2$ para cualquier valor del parámetro $\theta$.

            \item Para $n=1$, si $U(X)$ es un estimador insesgado de $g(\theta)=\ln\theta^2$ regular, se pide calcular la covarianza de $U(X)$ y de $X$. \\
            
            Por ser $U(X)$ insesgado en $g(\theta)$, se tiene que 
            $$\E_\theta[U(X)] = g(\theta) = \ln(\theta^2) = 2 \ln \theta$$
            Por ser $U(X)$ regular, sabemos que se verifica la siguiente condición:
            $$\dfrac{\partial}{\partial \theta} E_\theta[U(X_1, \ldots, X_n)] = E_\theta \left[U(X_1, \ldots, X_n) \dfrac{\partial \ln f_\theta^n (X_1, \ldots, X_n)}
            {\partial \theta} \right]$$
            en particular para $n=1$
            $$\dfrac{\partial}{\partial \theta} E_\theta[U(X)] = E_\theta \left[U(X) \dfrac{\partial \ln f_\theta (X)}
            {\partial \theta} \right]$$
            Podemos obtener el miembro izquierdo de la igualdad
            $$\dfrac{\partial}{\partial \theta} E_\theta[U(X)] = \dfrac{\partial}{\partial \theta} (2 \ln \theta) = \dfrac{2}{\theta}$$

            y el miembro derecho, usando lo ya calculado en el apartado anterior, es
            $$E_\theta \left[U(X) \dfrac{\partial \ln f_\theta (X)}
            {\partial \theta} \right] = E_\theta \left[U(X) \left( \dfrac{k_1}{\theta} - k_2 X \right) \right] = \dfrac{k_1}{\theta} \E_\theta [U(X)] - k_2 \E_\theta[U(X)X]$$
            Juntando ambas cosas, podemos despejar el momento cruzado $\E_\theta[U(X)X]$:
            $$\dfrac{2}{\theta} = \dfrac{k_1}{\theta} \E_\theta [U(X)] - k_2 \E_\theta[U(X)X] \iff \E_\theta[U(X)X] = \dfrac{\dfrac{2}{\theta} - \dfrac{k_1}{\theta} \E_\theta [U(X)]}{-k_2} = $$ 
            $$ - \dfrac{2}{k_2\theta} + \dfrac{k_1}{k_2 \theta} \E_\theta [U(X)]$$
            Ahora, por definición de covarianza
            $$\text{Cov}_\theta(U(X),X) = \E_\theta[U(X) X] - \E_\theta[U(X)] \E_\theta[X]$$

            y sustituyendo y usando $\E_\theta[X]$ obtenida en el apartado anterior:

            $$\text{Cov}_\theta(U(X),X) = - \dfrac{2}{k_2\theta} + \dfrac{k_1}{k_2 \theta} \E_\theta [U(X)] - \E_\theta[U(X)] \dfrac{k_1}{k_2 \theta} = - \dfrac{2}{k_2\theta} $$

            \item ¿Para qué valores de $k_1$ y $k_2$ existen funciones paramétricas con estimadores eficientes? \\
            
            Buscamos aplicar el Teorema de Caracterización de Estimadores Eficientes. Para ello, obtenemos la función conjunta de la m.a.s. de $X$

            $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep}{=} \prod_{i=1}^n f_\theta(x_i)$$

            Se supondrá a partir de ahora que $x_i \in \Rmas \quad \forall i=1,\ldots,n$ y $\theta \in \Rmas$. De lo contrario, $f_\theta(x_i) = 0 \quad \forall i = 1, \ldots, n$.
            $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep}{=} \prod_{i=1}^n f_\theta(x_i) = \prod_{i=1}^n 
            e^{\left(k_1 \ln\theta - k_2 x_i \theta + S(x_i)\right)} = e^{\sum_{i=1}^{n} \left(k_1 \ln\theta - k_2 x_i \theta + S(x_i)\right)} = $$ 
            $$e^{\left(n k_1 \ln\theta- k_2 \theta \sum_{i=1}^{n} x_i + \sum_{i=1}^{n}S(x_i)\right)} = \exp \left\{n k_1 \ln\theta- k_2 \theta \sum_{i=1}^{n} x_i + \sum_{i=1}^{n}S(x_i)\right\}$$
            $$\ln f_\theta^n(x_1, \ldots, x_n) = n k_1 \ln\theta- k_2 \theta \sum_{i=1}^{n} x_i + \sum_{i=1}^{n}S(x_i)$$
            $$\dfrac{\partial \ln f_\theta^n(x_1, \ldots, x_n)}{\partial \theta} = \dfrac{n k_1}{\theta} - k_2 \left( \sum_{i=1}^{n} x_i \right) = - k_2 \left(\sum_{i=1}^{n} x_i - \dfrac{n k_1}{k_2\theta} \right) $$

            Ahora, supongamos que $T(X_1, \ldots, X_n)$ es un estimador de $g(\theta)$ función paramétrica derivable y estrictamente monótona
            ($g'(\theta) \neq 0 \quad \forall \theta \in \Theta = \Rmas$). Como el enunciado nos dice que la familia es regular, y 
            $0 < I_X(\theta) = \nicefrac{k_1^2}{\theta^2} < +\infty \quad \forall \theta \in \Theta$, $T$ es eficiente si y solo si $\forall \theta \in \Theta
            \quad \exists a(\theta) \neq 0$ tal que 
            $$P_\theta \left[ \dfrac{\partial \ln f_\theta^n(X_1, \ldots, X_n)}{\partial \theta} = a(\theta)[T(X_1, \ldots, X_n) - g(\theta)]\right] = 1$$
            y
            $$I_{(X_1, \ldots, X_n)}(\theta) = a(\theta) g'(\theta)$$
            Como 
            $$\dfrac{\partial \ln f_\theta^n(X_1, \ldots, X_n)}{\partial \theta} = a(\theta)[T(X_1, \ldots, X_n) - g(\theta)] \iff$$
            $$- k_2 \left(\sum_{i=1}^{n} X_i - \dfrac{n k_1}{k_2\theta} \right) = a(\theta) [T(X_1, \ldots, X_n) - g(\theta)]$$
            claramente por comparación se obtiene que 
            $$T(X_1, \ldots, X_n) = \sum_{i=1}^{n} X_i, \quad g(\theta) = \dfrac{n k_1}{k_2\theta}, \quad a(\theta) = -k_2$$
            Es claro que $T(X_1, \ldots, X_n) = \sum_{i=1}^n X_i$ es un estimador, pues $\Theta = \Rmas$, $g(\Theta) = \Rmas$, y $T(x_1, \ldots, x_n) > 0 \quad \forall (x_1, \ldots, x_n) \in \X^n = (\Rmas)^n$. Además, tanto 
            $a(\theta)$ como $g(\theta)$ verifican todas las condiciones del teorema, pues $$g'(\theta) = \dfrac{n k_1}{k_2} \cdot (\theta^{-1})' = \dfrac{n k_1}{k_2} \cdot (- \theta^{-2}) = -\dfrac{nk_1}{k_2 \theta^2} < 0$$ porque $n, k_1, k_2, \theta > 0$
            y $a(\theta) = -k_2 < 0$, en particular, $a(\theta) \neq 0 \quad \forall \theta \in \Theta$, y usando la aditividad de la función de información de Fisher, obtenemos
            $$n \dfrac{k_1^2}{\theta^2} = n I_X(\theta) = I_{(X_1, \ldots, X_n)}(\theta) = a(\theta) g'(\theta) = -k_2 \left( - \dfrac{nk_1}{k_2 \theta^2} \right) \stackrel{k_2 > 0}{=} \dfrac{nk_1}{\theta^2}$$
            Para que coincidan, necesariamente $$n \dfrac{k_1^2}{\theta^2} = \dfrac{nk_1}{\theta^2} \stackrel{n, \theta > 0}{\iff} k_1^2 = k_1 \iff k_1 \in \{0,1\}$$

            Sin embargo como el enunciado del problema nos dice que $k_1 \in \Rmas$ (además de que para que $T$ sea eficiente, debe ser $0 < I_X(\theta) = \nicefrac{k_1^2}{\theta^2} < + \infty$ por definición, 
            por lo que también se descarta por esta vía que $k_1 = 0$), debe ser $k_1 = 1$. Por un corolario visto en teoría, como $T(X_1, \ldots, X_n)$ es un estimador eficiente para $g(\theta)$, con $g'(\theta) \neq 0$,
            sabemos que las únicas funciones paramétricas que admiten estimadores eficientes son las de la forma $a g(\theta) + b$ y los correspondientes
            estimadores eficientes son $aT + b$, con probabilidad $1$, bajo todas las distribuciones de la familia. Además, estos estimadores eficientes existen únicamente en el caso en que $(k_1, k_2) \in \{1\} \times \Rmas$.
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[1.85 puntos]
        Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable $X\rightsquigarrow \{P_\theta:\theta \in \mathbb{R}\}$ y $S\equiv S(X_1,\ldots,X_n)$ un estimador de $\theta$:
        \begin{enumerate}[label=\alph*)]
            \item Si $S-\theta\rightsquigarrow \cc{N}(0,\sigma_0^2)\quad \forall \theta\in \mathbb{R}$. Partiendo de la función de verosimilitud de $\theta$ asociada a una realización de $S$, 
            calcular la función de verosimilitud asociada a la función $\lm = \theta^2-1$ y deducir a partir de ella el estimador máximo verosímil de $\lm$. \\

            Para obtener la función de verosimilitud de $\theta$ asociada a una realización de $S$, pongamos $s$, necesitamos la función de densidad de la normal, expresada en términos de $s$:
            $$f_\theta(s) = \dfrac{1}{\sigma_0 \sqrt{2 \pi}} \exp \left\{ - \dfrac{(s-\theta)^2}{2 \sigma_0^2}\right\}$$
            Ahora, por definición, la función de verosimilitud es
            $$L_s(\theta) = f_\theta(s) \quad \forall \theta \in \Theta = \R$$
            La función paramétrica que nos dan es $\lm = \theta^2 - 1 = g(\theta)$, y despejando $\theta$ en función de $\lm$ $$\lm = \theta^2 - 1 \iff \theta^2 = \lm + 1 \iff \theta = \pm \sqrt{\lm + 1}$$ 
            es claro que el espacio paramétrico es $\Lambda = \left[-1, +\infty \right[$. Nuevamente por definición, la función de verosimilitud asociada a la función paramétrica $\lm = g(\theta) = \theta^2 - 1$ es
            $$M_s(\lm) = \sup_{\theta \in g^{-1}(\lm)} L_s(\theta) = \max \{L_s(\sqrt{\lm+1}), L_s(-\sqrt{\lm+1})\}$$

            Queremos ver si $L_s(\sqrt{\lm+1}) \geq L_s(-\sqrt{\lm+1})$ (podríamos haber tomado el otro sentido
            de la desigualdad no estricta también), es decir, si $L_s(\sqrt{\lm+1})$ nos da la función de verosimilitud asociada a $\lm = g(\theta)$. Para ello, tratamos de llegar a una condición
            equivalente que sea sencilla de comprobar:
            $$L_s(\sqrt{\lm+1}) \geq L_s(-\sqrt{\lm+1}) \iff $$
            $$
            \dfrac{1}{\sigma_0 \sqrt{2 \pi}} \exp \left\{ - \dfrac{(s-\sqrt{\lm+1})^2}{2 \sigma_0^2}\right\} \geq
            \dfrac{1}{\sigma_0 \sqrt{2 \pi}} \exp \left\{ - \dfrac{(s-(-\sqrt{\lm+1}))^2}{2 \sigma_0^2}\right\} \iff$$
            $$\exp \left\{ - \dfrac{(s-\sqrt{\lm+1})^2}{2 \sigma_0^2}\right\} \geq
            \exp \left\{ - \dfrac{(s+\sqrt{\lm+1})^2}{2 \sigma_0^2}\right\} \stackrel{(*)}{\iff}$$
            $$- \dfrac{(s-\sqrt{\lm+1})^2}{2 \sigma_0^2} \geq - \dfrac{(s+\sqrt{\lm+1})^2}{2 \sigma_0^2} 
            \stackrel{\sigma_0^2 > 0}{\iff}$$
            $$(s-\sqrt{\lm+1})^2 \leq (s+\sqrt{\lm+1})^2 \iff$$
            $$s^2 - 2 s \sqrt{\lm + 1} + \lm + 1 \leq s^2 + 2 s \sqrt{\lm + 1} + \lm + 1 \iff$$
            $$- 2 s \sqrt{\lm + 1} \leq 2 s \sqrt{\lm + 1} \iff 
            0 \leq 4 s \sqrt{\lm + 1} \stackrel{\sqrt{\lm+1} \geq 0}{\iff} s \geq 0$$
            donde en $(*)$ se ha usado que la exponencial es estrictamente creciente. Deducimos entonces que 
            $$M_s(\lm) = \begin{cases}
                L_s(\sqrt{\lm+1}) &\text{si } s \geq 0 \\
                L_s(-\sqrt{\lm+1}) &\text{si } s < 0
            \end{cases}$$
            Ahora, hallamos el EMV por medio de las ecuaciones de verosimilitud, que dependerán del caso en que nos encontremos.
            \begin{enumerate}
                \item Si $s \geqslant 0$, entonces $M_s(\lm) = L_s(\sqrt{\lm+1})$ y maximizamos
                $$\ln M_s(\lm) = \ln \left( \dfrac{1}{\sigma_0 \sqrt{2 \pi}} \right)
                - \dfrac{(s-\sqrt{\lm+1})^2}{2 \sigma_0^2} \Longrightarrow$$ 
                $$ \dfrac{\partial \ln M_s(\lm)}{\partial \lm} = - \dfrac{1}{2 \sigma_0^2}
                2 (s - \sqrt{\lm+1}) \left( - \dfrac{1}{2 \sqrt{\lm+1}} \right) = \dfrac{s-\sqrt{\lm+1}}{2 \sigma_0^2 \sqrt{\lm+1}}$$
                y 
                $$\dfrac{\partial \ln M_s(\lm)}{\partial \lm} = 0 \iff 
                \dfrac{s-\sqrt{\lm+1}}{2 \sigma_0^2 \sqrt{\lm+1}} = 0 
                \iff s - \sqrt{\lm + 1} = 0 \iff s = \sqrt{\lm + 1} \iff $$
                $$\hat{\lm} = s^2 - 1$$
                \item Si $s < 0$, entonces $M_s(\lm) = L_s(-\sqrt{\lm+1})$ y maximizamos
                $$\ln M_s(\lm) = \ln \left( \dfrac{1}{\sigma_0 \sqrt{2 \pi}} \right)
                - \dfrac{(s+\sqrt{\lm+1})^2}{2 \sigma_0^2} \Longrightarrow$$ 
                $$ \dfrac{\partial \ln M_s(\lm)}{\partial \lm} = - \dfrac{1}{2 \sigma_0^2}
                2 (s + \sqrt{\lm+1}) \left( \dfrac{1}{2 \sqrt{\lm+1}} \right) = 
                \dfrac{s+\sqrt{\lm+1}}{2 \sigma_0^2 \sqrt{\lm+1}}$$
                y 
                $$\dfrac{\partial \ln M_s(\lm)}{\partial \lm} = 0 \iff 
                \dfrac{s+\sqrt{\lm+1}}{2 \sigma_0^2 \sqrt{\lm+1}} = 0 
                \iff s + \sqrt{\lm + 1} = 0 \iff s = - \sqrt{\lm + 1} \iff $$
                $$|s| \stackrel{s<0}{=} -s = \sqrt{\lm + 1} \iff$$
                $$\hat{\lm} = s^2 - 1$$
            \end{enumerate}

            En resumen, el estimador máximo verosímil de $\lm$ es $\hat{\lm} = s^2-1$, para todo $\lm \in \Lambda = \left[-1, +\infty \right[$.
            
            \newpage

            \item Si $S-\theta \rightsquigarrow t(n)\quad \forall \theta \in \mathbb{R}$. Encontrar el intervalo de confianza para $\theta$ de mínima longitud esperada a nivel de confianza $1-\alpha$ basado en $S$. \\
            
            Como $S-\theta\rightsquigarrow t(n)\quad \forall \theta \in \mathbb{R}$, podemos considerar como función pivote, 
            $T : \X^n \times \Theta \to \R$, dado por 
            $$T \equiv T(X_1, \ldots, X_n; \theta) = S(X_1, \ldots, X_n) - \theta \equiv S - \theta$$ que es
            una variable aleatoria cuya distribución, $t(n)$, es independiente del parámetro $\theta$ (solo depende de $n \in \N$).
            Además:
            \begin{enumerate}
                \item $T$ es estrictamente monótona en $\theta$, pues
                $$\dfrac{\partial T}{\partial \theta} = \dfrac{\partial}{\partial \theta} (S - \theta) = - 1 < 0, \quad \forall \theta \in \Theta = \R$$
                \item $T = \lm$ tiene solución en $\theta$, para todo $\lm \in \Lambda = \R$, con $\Lambda$ la imagen de $T$, pues
                $$T = \lm \iff S - \theta = \lm \iff \theta = S - \lm \in \R$$ 
            \end{enumerate}
            Por un teorema visto en teoría, sabemos que se puede construir un intervalo de confianza para $\theta$ a cualquier nivel
            de confianza $1-\alpha$, con $0 < \alpha < 1$. Por el método de la cantidad pivotal, buscamos $\lm_1 < \lm_2$ 
            verificando $$P_\theta(\lm_1 < T < \lm_2) = 1 - \alpha$$
            Es decir,
            $$\lm_1 < T < \lm_2 \iff \lm_1 < S - \theta < \lm_2 \iff \lm_1 - S < - \theta < \lm_2 - S \iff
            S - \lm_2 < \theta < S - \lm_1$$
            y el intervalo tendrá longitud $L = (S-\lm_1) - (S - \lm_2) = \lm_2 - \lm_1$, que es constante, luego coincidirá 
            con la longitud esperada $\E_\theta[L] = L \quad \forall \theta \in \Theta$. \\
            
            Sea ahora $F$ la función de distribución de la distribución $t$ de Student $t(n)$, 
            y sea $f$ la correspondiente función de densidad. La restricción es 
            $$1 - \alpha = P_\theta(\lm_1 < T < \lm_2) = F(\lm_2) - F(\lm_1)$$
            Minimizamos $\lm_2 - \lm_1$ con el método de los multiplicadores de Lagrange visto en teoría
            $$H(\lm_1, \lm_2) = \lm_2 - \lm_1 - \lm[F(\lm_2) - F(\lm_1) - (1-\alpha)]$$
            y buscamos aquellos $\lm_1, \lm_2$ que minimicen $H(\lm_1, \lm_2)$, luego obtenemos sus derivadas parciales
            $$\dfrac{\partial H}{\partial \lm_1} = - 1 + \lm f(\lm_1) \quad 
            \dfrac{\partial H}{\partial \lm_2} = 1 - \lm f(\lm_2)$$
            Ahora, igualamos ambas parciales a $0$ y despejamos $\lambda$:
            \begin{equation*}
                \left.\begin{array}{l}
                    0 = \dfrac{\partial H}{\partial \lm_1} = -1+\lm f(\lm_1) \\\\
                    0 = \dfrac{\partial H}{\partial \lm_2} = 1-\lm f(\lm_2)
                \end{array}\right\} \Longrightarrow \left.\begin{array}{l}
                    \lm = \dfrac{1}{f(\lm_1)} \\\\
                    \lm = \dfrac{1}{f(\lm_2)}
                \end{array}\right\} \Longrightarrow f(\lm_1) = f(\lm_2)
            \end{equation*}
            Como la distribución $t$ de Student tiene las mismas propiedades de simetría 
            con respecto al origen que la $\cc{N}(0, 1)$, las únicas posibilidades son o bien 
            $\lm_1 = \lm_2$ o bien $\lm_1 = -\lm_2$. La primera de las dos opciones se descarta ya que no verifica la restricción ($F(\lm_2) - F(\lm_1) = 0 \neq 1-\alpha$), 
            por lo que tiene que ser $\lm_1 = -\lm_2$. Teniendo nuevamente en cuenta la restricción $F(\lm_2) - F(\lm_1) = 1-\alpha$, estos quedan determinados por:
            \begin{equation*}
                \lm_2 = t_{n; \nicefrac{\alpha}{2}}, \qquad \lm_1 = -\lm_2 = -t_{n; \nicefrac{\alpha}{2}}
            \end{equation*}
            siendo $P[T > t_{n; \nicefrac{\alpha}{2}}] = \alpha/2$ y $T \rightsquigarrow t(n)$.
            En conclusión, el intervalo de confianza para $\theta$ de mínima longitud esperada a nivel de confianza $1-\alpha$ basado en $S$ es:
            \begin{equation*}
                \left] S - \lm_2, S - \lm_1 \right[ = \left] S - t_{n; \nicefrac{\alpha}{2}}, S + t_{n; \nicefrac{\alpha}{2}} \right[
            \end{equation*}

        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[2 puntos]
        Sea $X$ una variable aleatoria con función de densidad
        \begin{equation*}
            f_\theta(x) = \frac{3{(x-1)}^{2}}{\theta^3}, \qquad 1<x<\theta +1
        \end{equation*}
        se pide obtener el test más potente de tamaño $\alpha$ que permita resolver el contraste $H_0:\theta = \theta_0$, $H_1:\theta=\theta_1$ donde $\theta_1<\theta_0$.\\

        \noindent
        Calcule la potencia de cada test. Para $n=2$ y $\theta_0=9$, obtener el mayor valor de $\theta_1$ para que la potencia del test de Neyman-Pearson de tamaño $0.01$ sea mayor o igual que $0.64$. \\

        Tenemos un contraste de hipótesis simple frente a hipótesis simple, por lo que sabemos por el Lema de Neyman-Pearson que el 
        Test de Neyman-Pearson será el más potente de tamaño $\alpha$, de la forma
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \lambda(X_1, \ldots, X_n) > k \\
            \gamma &\text{si } \lambda(X_1, \ldots, X_n) = k \\
            0 &\text{si } \lambda(X_1, \ldots, X_n) < k
        \end{cases}$$
        para ciertos $\gamma \in [0,1]$, $k \in \R$ y 
        $$\lambda(X_1, \ldots, X_n) = \dfrac{f_1^n(X_1, \ldots, X_n)}{f_0^n(X_1, \ldots, X_n)}$$

        Definimos el espacio muestral y el espacio paramétrico, en ambos casos dependiente del parámetro
        $\theta$. Si estamos en $H_0$, entonces $\X_0 = \left]1, \theta_0 + 1 \right[$, y $\Theta_0 = 
        \{\theta_0\}$. Análogamente, si estamos en $H_1$, entonces $\X_1 = \left]1, \theta_1 + 1 \right[$, y $\Theta_1 = 
        \{\theta_1\}$. Tenemos entonces que $$\Theta = \Theta_0 \cup \Theta_1 = \{\theta_0, \theta_1\}$$ y como
        $\theta_1 < \theta_0 \iff \theta_1 + 1 < \theta_0 + 1$, entonces $\left]1, \theta_1 + 1 \right[ \subset \left]1, \theta_0 + 1 \right[$ y
        $$\X = \X_0 \cup \X_1 = \left]1, \theta_0 + 1 \right[ \cup \left]1, \theta_1 + 1 \right[ = \left]1, \theta_0 + 1 \right[$$
        Consecuentemente $$\X^n = \{(x_1, \ldots, x_n) \in \R^n : 1 < x_i < \theta_0 + 1 \quad \forall i = 1, \ldots, n\} = $$
        $$\{(x_1, \ldots, x_n) \in \R^n : 1 < x_{(1)} \quad \land \quad x_{(n)} < \theta_0 + 1\}$$

        Podemos considerar entonces $(x_1, \ldots, x_n) \in \X^n$, y obtener la función conjunta $f_\theta^n(x_1, \ldots, x_n)$, que es 
        $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^n \frac{3{(x_i-1)}^{2}}{\theta^3}$$ 
        Y vemos que $$x<\theta+1 \iff I_{\Rmenos}(x - (\theta + 1)) = 1$$
        luego $$x_i < \theta + 1 \quad \forall i = 1, \ldots, n \iff x_{(n)} < \theta + 1 \iff I_{\Rmenos}(x_{(n)} - (\theta + 1)) = 1$$
        Por tanto $$f_\theta^n(x_1, \ldots, x_n) = \prod_{i=1}^n \frac{3{(x_i-1)}^{2}}{\theta^3} I_{\Rmenos}(x_{(n)} - (\theta + 1)) = $$
        $$\left(\dfrac{3}{\theta^3} \right)^n \prod_{i=1}^n (x_i-1)^2 I_{\Rmenos}(x_{(n)} - (\theta + 1)) =
        \dfrac{3^n}{\theta^{3n}} \prod_{i=1}^n (x_i-1)^2 I_{\Rmenos}(x_{(n)} - (\theta + 1))$$
        Se tiene entonces que
        $$f_0^n(x_1, \ldots, x_n) = \dfrac{3^n}{\theta_0^{3n}} \prod_{i=1}^n (x_i-1)^2 I_{\Rmenos}(x_{(n)} - (\theta_0 + 1)) = \dfrac{3^n}{\theta_0^{3n}} \prod_{i=1}^n (x_i-1)^2 \quad \forall (x_1, \ldots, x_n) \in \X^n$$
        $$f_1^n(x_1, \ldots, x_n) = \dfrac{3^n}{\theta_1^{3n}} \prod_{i=1}^n (x_i-1)^2 I_{\Rmenos}(x_{(n)} - (\theta_1 + 1)) \quad \forall (x_1, \ldots, x_n) \in \X^n$$
        y podemos obtener $\lambda(x_1, \ldots, x_n)$, con $(x_1, \ldots, x_n) \in \X^n$: 
        $$\lambda(x_1, \ldots, x_n) = \dfrac{f_1^n(x_1, \ldots, x_n)}{f_0^n(x_1, \ldots, x_n)} = $$$$
        \dfrac{\dfrac{3^n}{\theta_1^{3n}}}{\dfrac{3^n}{\theta_0^{3n}}} \dfrac{\displaystyle\prod_{i=1}^n (x_i-1)^2 I_{\Rmenos}(x_{(n)} - (\theta_1 + 1))}
        {\displaystyle\prod_{i=1}^n (x_i-1)^2} = 
        \dfrac{\begin{cases} \dfrac{3^n}{\theta_1^{3n}} \displaystyle\prod_{i=1}^n (x_i-1)^2 &\text{si } x_{(n)} < \theta_1 + 1 \\ 0 &\text{si } x_{(n)} \geq \theta_1 + 1 \end{cases}}
        {\dfrac{3^n}{\theta_0^{3n}} \displaystyle\prod_{i=1}^n (x_i-1)^2}$$

        Ahora, si $x_{(n)} < \theta_1 + 1$, simplificamos

        $$\dfrac{\dfrac{3^n}{\theta_1^{3n}}}{\dfrac{3^n}{\theta_0^{3n}}} \dfrac{\cancel{\prod_{i=1}^n (X_i-1)^2}}
        {\cancel{\prod_{i=1}^n (X_i-1)^2}} = \dfrac{\cancel{3^n} \theta_0^{3n}}{\cancel{3^n} \theta_1^{3n}} = \left( \dfrac{\theta_0}{\theta_1} \right)^{3n}$$

        de donde
        $$\lambda(x_1, \ldots, x_n) = \dfrac{f_1^n(x_1, \ldots, x_n)}{f_0^n(x_1, \ldots, x_n)} = 
        \begin{cases} 
            \left( \dfrac{\theta_0}{\theta_1} \right)^{3n} &\text{si } x_{(n)} < \theta_1 + 1 \\ 
            0 &\text{si } x_{(n)} \geq \theta_1 + 1 
        \end{cases} \quad \forall (x_1, \ldots, x_n) \in \X^n$$

        Gráficamente, como $\theta_1 < \theta_0$:

        \begin{center}
            \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

            \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
                %uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300

                %Straight Lines [id:da9701795441758359] 
                \draw    (70,101) -- (301,102) ;
                %Straight Lines [id:da9876084884828463] 
                \draw    (141,89) -- (141,112) ;
                %Straight Lines [id:da963921157698696] 
                \draw    (210,89) -- (210,112) ;
                %Straight Lines [id:da07701744813997313] 
                \draw    (140.5,41) -- (297.67,41.33) ;
                \draw [shift={(299.67,41.33)}, rotate = 180.12] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da3774624119169432] 
                \draw    (140.5,41) -- (140.5,21.67) ;
                %Straight Lines [id:da9681405401847125] 
                \draw    (140.5,71.5) -- (140.5,51.17) ;
                %Straight Lines [id:da1310029333570385] 
                \draw    (140.5,70.5) -- (72.5,71.23) ;
                \draw [shift={(70.5,71.25)}, rotate = 359.39] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da899862814242277] 
                \draw    (210.5,171.5) -- (210.5,151.17) ;
                %Straight Lines [id:da009971342095128954] 
                \draw    (210.5,170.5) -- (73.5,170.5) ;
                \draw [shift={(71.5,170.5)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da4678439533610015] 
                \draw    (210.5,211.5) -- (210.5,191.17) ;
                %Straight Lines [id:da8507207636884839] 
                \draw    (210.5,210.5) -- (297.5,211.48) ;
                \draw [shift={(299.5,211.5)}, rotate = 180.64] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da9954362463616986] 
                \draw  [dash pattern={on 0.84pt off 2.51pt}]  (140.5,8.5) -- (140.5,211.5) ;
                %Straight Lines [id:da20565414403393933] 
                \draw  [dash pattern={on 0.84pt off 2.51pt}]  (210,8.5) -- (210,211.5) ;

                % Text Node
                \draw (333,59) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \theta _{1} \ < \ \theta _{0}$};
                % Text Node
                \draw (92,103.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \theta _{1} +1$};
                % Text Node
                \draw (215,104) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \theta _{0} +1$};
                % Text Node
                \draw (307,92) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle X_{( n)}$};
                % Text Node
                \draw (225,16) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{1} =0$};
                % Text Node
                \draw (83.33,47) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{1}  >0$};
                % Text Node
                \draw (153.33,147) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{0}  >0$};
                % Text Node
                \draw (223.33,188) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{0} =0$};
            \end{tikzpicture}
        \end{center}

        La semirrecta en que $f_0 = 0$, que se corresponde con la condición $x_{(n)} \geq \theta_0 + 1$, no nos interesa 
        ya que $(x_1, \ldots, x_n) \notin \X^n$. Gráficamente, la situación en la que estamos es la siguiente:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}
                % Ejes
                \draw[-Stealth] (-0.2,0) -- (6,0) node[right] {$X_{(n)}$};
                \draw[-Stealth] (0,-0.2) -- (0,3.5) node[left] {$\lm$};

                % Recta con altura 1
                \draw[thick] (0,2.65) -- (6,2.65);

                % Proyección sobre la recta
                \coordinate (P) at (1.5,1.65); % Ajustado para caer sobre la recta dibujada
                \draw[thick, blue] (P) -- (0,1.65);

                \draw[dashed] (1.5,0) -- (1.5,1.65);
                \coordinate (Q) at (1.5,0); % Ajustado para caer sobre la recta dibujada
                \draw (1.5,0) node[below] {$\theta_1 + 1$};
                \draw (3.5,0) node[below] {$\theta_0 + 1$};
                \coordinate (R) at (3.5,0); % Ajustado para caer sobre la recta dibujada

                \draw[thick, blue] (Q) -- (3.5,0);

                % Etiqueta k en el eje y
                \draw (0,1.65) node[left] {$\left( \dfrac{\theta_0}{\theta_1} \right)^{3n}$};
                \draw (0,2.65) node[left] {$1$};

                % Punto en la recta
                \draw (P) circle (2pt);
                % Punto en la recta
                \fill (Q) circle (2pt);

                \draw (R) circle (2pt);
            \end{tikzpicture}
        \end{figure}

        Tenemos que $k \in \left\{0, \left( \dfrac{\theta_0}{\theta_1} \right)^{3n} \right\}$, luego distinguimos entre estos dos casos. \\

        Si \boxed{k=0}, entonces el test será:
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \lambda(X_1, \ldots, X_n) > k \iff X_{(n)} < \theta_1 + 1 \\
            \gamma &\text{si } \lambda(X_1, \ldots, X_n) = k \iff X_{(n)} \geq \theta_1 + 1 \\
            0 &\text{si } \lambda(X_1, \ldots, X_n) < k \text{ nunca}
        \end{cases}$$

        es decir:

        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } X_{(n)} < \theta_1 + 1 \\
            \gamma &\text{si } X_{(n)} \geq \theta_1 + 1
        \end{cases}$$

        Determinamos $\gamma$ imponiendo tamaño $\alpha$:

        $$\alpha \stackrel{def} = \sup_{\theta \in \Theta_0} \beta_{\varphi}(\theta) =  \sup_{\theta \in \Theta_0} E_\theta[\varphi(X_1, \ldots, X_n)] = E_{\theta_0}[\varphi(X_1, \ldots, X_n)] = $$
        $$1 \cdot P_{\theta_0}[X_{(n)} < \theta_1 + 1] + \gamma P_{\theta_0}[X_{(n)} \geq \theta_1 + 1] $$

        Para calcular las probabilidades, obtenemos la función de distribución $F_X(t)$:
        $$F_X(t) \stackrel{def}{=} \int_{1}^{t} \frac{3{(x-1)}^{2}}{\theta^3} dx = \dfrac{3}{\theta^3} \int_{1}^{t} (x-1)^{2} dx = \dfrac{3}{\theta^3} \left[ \dfrac{(x-1)^3}{3} \right]_1^t = \dfrac{(t-1)^3}{\theta^3} = 
        \left( \dfrac{t-1}{\theta} \right)^3$$

        Sabemos por teoría que, para $T = X_{(n)}$, la distribución del máximo verifica $$P_{\theta}[T<t] = F_T(t) = (F_X(t))^n = \left( \dfrac{t-1}{\theta} \right)^{3n}$$

        Por lo tanto, $$P_{\theta_0}[T < \theta_1 + 1] = \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}$$

        y

        $$P_{\theta_0}[T \geq \theta_1 + 1] = 1 - P_{\theta_0}[T < \theta_1 + 1] = 1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}$$

        de donde
        $$\alpha = 1 \cdot P_{\theta_0}[X_{(n)} < \theta_1 + 1] + 
        \gamma P_{\theta_0}[X_{(n)} \geq \theta_1 + 1] = \left( \dfrac{\theta_1}{\theta_0} \right)^{3n} + \gamma \left( 1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{3n} \right) \iff $$
        $$0 \leq \gamma = \dfrac{\alpha - \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}}{1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}} \leq 1 \iff \alpha \geq \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}$$

        y el test resultante es:

        $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                1 &\text{si } X_{(n)} < \theta_1 + 1 \\
                \dfrac{\alpha - \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}}{1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}} &\text{si } X_{(n)} \geq \theta_1 + 1
            \end{cases}$$      
            
        con potencia (en $\Theta_1 = \{\theta_1\}$):

        $$\beta_{\varphi}(\theta_1) = E_{\theta_1}[\varphi(X_1, \ldots, X_n)] = P_{\theta_1}[T < \theta_1 + 1] + 
        \dfrac{\alpha - \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}}{1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}} P_{\theta_1}[T \geq \theta_1 + 1]$$

        Como $P_{\theta_1}[T < \theta_1 + 1] = 1$ y $P_{\theta_1}[T \geq \theta_1 + 1] = 1 - P_{\theta_1}[T < \theta_1 + 1] = 1 - 1 = 0$, entonces

        $$\beta_{\varphi}(\theta_1) = 1$$

        \newpage

        Si \boxed{k = \left(\dfrac{\theta_0}{\theta_1} \right)^{3n}}, el test será, mirando nuevamente la misma gráfica

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}
                % Ejes
                \draw[-Stealth] (-0.2,0) -- (6,0) node[right] {$X_{(n)}$};
                \draw[-Stealth] (0,-0.2) -- (0,3.5) node[left] {$\lm$};

                % Recta con altura 1
                \draw[thick] (0,2.65) -- (6,2.65);

                % Proyección sobre la recta
                \coordinate (P) at (1.5,1.65); % Ajustado para caer sobre la recta dibujada
                \draw[thick, blue] (P) -- (0,1.65);

                \draw[dashed] (1.5,0) -- (1.5,1.65);
                \coordinate (Q) at (1.5,0); % Ajustado para caer sobre la recta dibujada
                \draw (1.5,0) node[below] {$\theta_1 + 1$};
                \draw (3.5,0) node[below] {$\theta_0 + 1$};
                \coordinate (R) at (3.5,0); % Ajustado para caer sobre la recta dibujada

                \draw[thick, blue] (Q) -- (3.5,0);

                % Etiqueta k en el eje y
                \draw (0,1.65) node[left] {$\left( \dfrac{\theta_0}{\theta_1} \right)^{3n}$};
                \draw (0,2.65) node[left] {$1$};

                % Punto en la recta
                \draw (P) circle (2pt);
                % Punto en la recta
                \fill (Q) circle (2pt);

                \draw (R) circle (2pt);
            \end{tikzpicture}
        \end{figure}

        el que sigue:

        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \lambda(X_1, \ldots, X_n) > k \text{ nunca} \\
            \gamma &\text{si } \lambda(X_1, \ldots, X_n) = k \iff X_{(n)} < \theta_1 + 1 \\
            0 &\text{si } \lambda(X_1, \ldots, X_n) < k \iff X_{(n)} \geq \theta_1 + 1
        \end{cases}$$

        es decir:
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            \gamma &\text{si } X_{(n)} < \theta_1 + 1 \\
            0 &\text{si } X_{(n)} \geq \theta_1 + 1
        \end{cases}$$

        Determinamos $\gamma$ igual que antes imponiendo tamaño $\alpha$:

        $$\alpha \stackrel{def} = \sup_{\theta \in \Theta_0} \beta_{\varphi}(\theta) =  \sup_{\theta \in \Theta_0} E_\theta[\varphi(X_1, \ldots, X_n)] = E_{\theta_0}[\varphi(X_1, \ldots, X_n)] = $$
        $$\gamma \cdot P_{\theta_0}[X_{(n)} < \theta_1 + 1] + 0 \cdot P_{\theta_0}[X_{(n)} \geq \theta_1 + 1] = \gamma \cdot P_{\theta_0}[X_{(n)} < \theta_1 + 1] $$

        Ya sabemos del caso anterior que 
        $$P_{\theta_0}[T < \theta_1 + 1] = \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}$$

        luego
        $$\alpha = \gamma \cdot P_{\theta_0}[X_{(n)} < \theta_1 + 1] = \gamma \left( \dfrac{\theta_1}{\theta_0} \right)^{3n} \iff $$
        $$0 \leq \gamma = \dfrac{\alpha}{\left( \dfrac{\theta_1}{\theta_0} \right)^{3n}} = \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{3n}  \leq 1 \iff \alpha \leq \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}$$

        y el test resultante es:

        $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{3n} &\text{si } X_{(n)} < \theta_1 + 1 \\
                0 &\text{si } X_{(n)} \geq \theta_1 + 1
            \end{cases}$$      
            
        con potencia (en $\Theta_1 = \{\theta_1\}$):
        $$\beta_{\varphi}(\theta_1) = E_{\theta_1}[\varphi(X_1, \ldots, X_n)] = \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{3n} \cdot P_{\theta_1}[T < \theta_1 + 1] + 
        0 \cdot P_{\theta_1}[T \geq \theta_1 + 1] = $$
        $$\alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{3n} \cdot P_{\theta_1}[T < \theta_1 + 1]$$

        Igual que en el caso anterior $P_{\theta_1}[T < \theta_1 + 1] = 1$, luego

        $$\beta_{\varphi}(\theta_1) = \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{3n}$$

        Lo último que pide el ejercicio es: para $n=2$ y $\theta_0=9$, obtener el mayor valor de $\theta_1$ para que la potencia del test de Neyman-Pearson de tamaño $0.01$ sea mayor o igual que $0.64$. Distinguimos nuevamente
        entre los dos posibles valores de $k$: \\

        Si \boxed{k=0}, hemos visto que el test más potente para tamaños $\alpha \geq \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}$ es

        $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                1 &\text{si } X_{(n)} < \theta_1 + 1 \\
                \dfrac{\alpha - \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}}{1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}} &\text{si } X_{(n)} \geq \theta_1 + 1
            \end{cases}$$  

        y la potencia es $$\beta_{\varphi}(\theta_1) = 1$$

        En particular si $n = 2$, $\theta_0 = 9$ y $\alpha = 0.01$, imponemos
        $$0.01 = \alpha \geq \left( \dfrac{\theta_1}{\theta_0} \right)^{3n} = \left( \dfrac{\theta_1}{9} \right)^{6} = \dfrac{\theta_1^6}{9^6} \iff \theta_1^6 \leq 0.01 \cdot 9^6 = 5314,41 \iff
        \theta_1 \leq \sqrt[6]{5314,41} \approx 4.177$$

        Por tanto, como la potencia siempre es $1$ ($>0,64$), el mayor valor sería simplemente el mayor valor que verifica
        la restricción del tamaño, que es $\theta_1 \approx 4.177$. \\

        Si \boxed{k=\left(\dfrac{\theta_0}{\theta_1} \right)^{3n}}, hemos visto que el test más potente para tamaños $\alpha \leq \left( \dfrac{\theta_1}{\theta_0} \right)^{3n}$ es

        $$\varphi(X_1, \ldots, X_n) = 
        \begin{cases}
            \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{3n} &\text{si } X_{(n)} < \theta_1 + 1 \\
            0 &\text{si } X_{(n)} \geq \theta_1 + 1
        \end{cases}$$

        y la potencia es $$\beta_{\varphi}(\theta_1) = \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{3n}$$

        En particular si $n = 2$, $\theta_0 = 9$ y $\alpha = 0.01$, imponemos

        $$0.01 = \alpha \leq \left( \dfrac{\theta_1}{\theta_0} \right)^{3n} = \left( \dfrac{\theta_1}{9} \right)^{6} = \dfrac{\theta_1^6}{9^6} \iff \theta_1^6 \geq 0.01 \cdot 9^6 = 5314,41 \iff
        \theta_1 \geq \sqrt[6]{5314,41} \approx 4.177$$

        e imponemos la restricción sobre la potencia 
        $$\beta_{\varphi}(\theta_1) \geq 0,64 \iff \dfrac{5314,41}{\theta_1^6} = 0,01 \left( \dfrac{9}{\theta_1} \right)^{6}  = \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{3n} \geq 0,64 \iff $$
        $$\theta_1 \leq \sqrt[6]{\dfrac{5314,41}{0,64}} = 4,5$$

        Combinando ambas restricciones, obtenemos

        $$4.177 \leq \theta_1 \leq 4,5$$

        Y concluimos entonces que el mayor valor de $\theta_1$ será $\theta_1 = 4,5$.

    \end{ejercicio}

    \newpage

    \begin{ejercicio}[1.25 puntos]
        Si se tiene un modelo lineal de Gauss-Markov $Y=X\beta + \varepsilon$:
        \begin{enumerate}[label=\alph*)]
            \item Si el modelo es de rango máximo, dar el estimador de mínimos cuadrados de $\beta$ y calcular la media del vector de residuos del modelo estimado, solo con las condiciones iniciales del modelo. \\
            
            El estimador de mínimos cuadrados del vector de efectos $\beta$ es único (la existencia siempre está garantizada sin necesidad de suponer que el modelo sea de rango máximo, y la unicidad está asegurada por ser el modelo de rango
            máximo) y viene dado por 
            $$\hat{\beta} = (X^T X)^{-1} X^T Y$$
            El modelo estimado es
            $$\hat{Y} = X \hat{\beta}$$
            El vector de residuos del modelo estimado se define como
            $$R = Y - \hat{Y} = Y - X \hat{\beta}$$
            Usando únicamente las condiciones del modelo:
            $$\E[Y] = X \beta \quad \text{ y } \quad \E[\hat{Y}] = X \beta$$
            se obtiene que la media del vector de residuos es 
            $$\E[R] = \E[Y - \hat{Y}] = E[Y] - E[\hat{Y}] = 0$$
            
            \item Definir el concepto de función estimable y enunciar el Teorema de Gauss-Markov. \\
            
            Una función escalar de las componentes de $\beta$, $\psi(\beta)$, se dice que es estimable si admite
            un estimador insesgado función lineal de las componentes de $Y$, o equivalentemente:
            $$\psi(\beta) \text{ estimable } \iff \exists c \in \R^n : E[c^T Y] = \psi(\beta), \quad \forall \beta$$

            El Teorema de Gauss-Markov afirma lo siguiente: toda función estimable, $a^T \beta$, admite un único estimador
            insesgado uniformemente de mínima varianza en la clase de estimadores lineales insesgados. Dicho estimador es
            $$a^T \hat{\beta}$$
            donde $\hat{\beta}$ es el estimador de mínimos cuadrados, y se denomina estimador de mínimos cuadrados de $a^T \beta$.

            \newpage
            
            \item Describir la hipótesis lineal general y bajo hipótesis de normalidad, dar el test de razón de verosimilitudes de tamaño $\alpha$ que permite resolver el contraste, especificando detalladamente el estadístico de contraste. \\
            
            La hipótesis de normalidad es $Y \rightsquigarrow \cc{N}_n(X \beta, \sigma^2 I_n)$. La hipótesis lineal general
            se expresa como $H_0 : C \beta = 0$, con $C_{q \times k}$ una matriz conocida de rango $q \leq k$. Si todas
            las componentes del vector $C \beta$ son estimables, entonces el test de razón de verosimilitud, de tamaño $\alpha$, que
            resuelve el contraste $$\begin{cases}
                H_0 : C \beta = 0 \\
                H_1 : C \beta \neq 0
            \end{cases}$$
            viene dado por 
            $$\varphi(Y) = \begin{cases}
                1 &\text{si } F(Y) > F_{q, n-r; \alpha} \\
                0 &\text{si } F(Y) \leq F_{q, n-r; \alpha}
            \end{cases}$$
            donde 
            $$F(Y) = \dfrac{n-r}{q} \left( \dfrac{\norma{Y - X \hat{\beta}^0}^2 - \norma{Y - X \hat{\beta}}^2}{\norma{Y - X \hat{\beta}}^2} \right)$$
            siendo $\hat{\beta}^0$ el estimador máximo verosímil de $\beta$ bajo $H_0$.
            Si se desea resolver el contraste sin establecer previamente el nivel de significación con el que se desea
            trabajar, se debe calcular:
            $$p-valor = P[F_{q,n-r} > F_{exp}] \text{ siendo } F_{exp} = \dfrac{n-r}{q} \left( \dfrac{\norma{y - X \hat{\beta}^0}^2 - \norma{y - X \hat{\beta}}^2}{\norma{y - X \hat{\beta}}^2} \right)$$ 
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[1 punto]
        Se ha medido el número de particulas de $100$ muestras radioactivas en un intervalo de tiempo prefijado e igual a todas las muestras, obteniendo los siguientes datos:
        \begin{table}[H]
        \centering
        \begin{tabular}{c|ccccccc}
            Número de partículas & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
            \hline
            Número de muestras & 29 & 25 & 20 & 14 & 8 & 3 & 1
        \end{tabular}
        \end{table}
        \noindent
        Se pretende contrastar a nivel de significación $0.05$ si la distribución de los datos se corresponde con la de una Poisson. \\
    
        En este caso, como la variable aleatoria es discreta, y tenemos frecuencias, es más apropiado usar el test $\chi^2$ de Pearson.
        Sea $$X \equiv \text{``Número de partículas de una muestra''}$$ El contraste a resolver es
        $$\begin{cases}
            H_0 : X \rightsquigarrow \mathcal{P}(\lambda) \\
            H_1 : X \not\rightsquigarrow \mathcal{P}(\lambda)
        \end{cases} \quad \lambda > 0$$
        Vemos que la hipótesis nula es compuesta, luego primeramente debemos estimar el valor del parámetro $\lambda$. Sabemos que el EMV de $\lm$ es la media muestral. Así
        $$\hat{\lambda} = \overline{X} = \dfrac{0 \cdot 29 + 1 \cdot 25 + 2 \cdot 20 + 3 \cdot 14 + 4 \cdot 8 + 5 \cdot 3 + 6 \cdot 1}{100} = \dfrac{160}{100} = 1.6$$
        El constraste adaptado sería 
        $$\begin{cases}
            H_0 : X \rightsquigarrow \mathcal{P}(1.6) \\
            H_1 : X \not\rightsquigarrow \mathcal{P}(1.6)
        \end{cases}$$
        Denotemos por $N_1, \ldots, N_k$ las frecuencias observadas en las $k$ clases consideradas, y por
        $$\hat{p}_i = P_{\hat{\lambda}}(X \in A_i), \quad i = 1, \ldots, k$$
        las probabilidades teóricas bajo $H_0$ con el parámetro $\lambda$ estimado por $\hat{\lambda}$. \\

        El estadístico de contraste viene dado por

        $$\hat{\chi}(N_1, \ldots, N_k) = \sum_{i=1}^k \dfrac{(N_i - n \hat{p}_i)^2}{n \hat{p}_i}$$

        Como el parámetro se ha estimado a partir de los mismos datos, la distribución asintótica bajo $H_0$ es 
        $$\hat{\chi}(N_1, \ldots, N_k) \rightsquigarrow_{H_0} \chi^2(k - q - 1)$$
        con $q=1$ el número de parámetros estimados.

        Por teoría, para poder aplicar el test hay que verificar que $$E^*_{i} = n\hat{p}_i \geqslant 5 \quad \forall i = 1, \ldots, k, \quad n = 100, \quad \hat{p}_i = P[X=i] = e^{-1.6} \dfrac{{1.6}^i}{i!}$$
        Primero vamos tanteando.
        $$E^*_0 = 100 \cdot \hat{p}_0 = 100 \cdot e^{-{1.6}} \dfrac{{1.6}^0}{0!} \approx 20.19 \geq 5$$
        $$E^*_1 = 100 \cdot e^{-{1.6}} \dfrac{{1.6}^1}{1!} \approx 32.30 \geq 5$$
        $$E^*_2 = 100 \cdot e^{-{1.6}} \dfrac{{1.6}^2}{2!} \approx 25.84 \geq 5$$
        $$E^*_3 = 100 \cdot e^{-{1.6}} \dfrac{{1.6}^3}{3!} \approx 13.78 \geq 5$$
        $$E^*_4 = 100 \cdot e^{-{1.6}} \dfrac{{1.6}^4}{4!} \approx 5.51 \geq 5$$
        
        En este punto\footnote{hay que particionar el espacio muestral de la Poisson, que
        es $\N \cup \{0\}$, en grupos que verifiquen las condiciones del test $\chi^2$ de Pearson, por eso las consideraciones siguientes.}, como $$E^*_5 = 100 \cdot e^{-{1.6}} \dfrac{{1.6}^5}{5!} \approx 1.76 < 5$$ debemos aquí agrupar los mayores o iguales que $5$, es decir, no considerar $E^*_5$, y en su lugar, considerar
        $$E^*_{\geq 5} = n P_{\hat{\lambda}}(X \geq 5) = n (1 - P_{\hat{\lambda}}(X < 5)) = n - \sum_{i=0}^{4} n P_{\hat{\lambda}}(X = i) =  n - \sum_{i=0}^{4} E_i^* \approx $$
        $$100 - (20.19 + 32.30 + 25.84 + 13.78 + 5.51) = 2.38 < 5$$
        que sigue siendo menor estricto que $5$, por lo que agrupamos la cola derecha a partir del 4, y ahora sí
        $$E^*_{\geq 4} = n P_{\hat{\lambda}}(X \geq 4) = n - \sum_{i=0}^{3} E_i^* = 100 - \left(\sum_{i=0}^{4} E_i^* - E_4^* \right) = 100 - (97.62 - 5.51) = 100 - 92.11 = $$
        $$7.89 \geq 5$$
        Una partición sería la siguiente
        $$A_i = \{i\}, \quad A_4 = \{\geqslant 4\}, \quad i = 0,1,2,3$$
        En este caso, $k=5$ (número de clases tras agrupar), luego
        $\hat{\chi}^2(N_1, \ldots, N_k) \rightsquigarrow \chi^2(3)$. Las frecuencias observadas, denotadas por $O_i, i= 1,\ldots,k$, son 
        $$O_0 = N_0 = 29, \quad O_1 = N_1 = 25, \quad O_2 = N_2 = 20, \quad O_3 = N_3 = 14, \quad O_4 = \sum_{i=4}^{6} N_i = 8+3+1 = 12$$
        Las frecuencias esperadas son 
        $$E_0 = E^*_0 \approx 20.19 \quad E_1 = E^*_1 \approx 32.30, \quad E_2 = E^*_2 \approx 25.84 
        \quad E_3 = E^*_3 \approx 13.78 \quad E_4 = E^*_{\geq 4} \approx 7.89$$

        Obtenemos $$\chi^2_{exp} = \sum_{i=0}^{4} \dfrac{(O_i - E_i)^2}{E_i} = 
        \dfrac{(29 - 20.19)^2}{20.19} + \dfrac{(25 - 32.30)^2}{32.30} +
        \dfrac{(20 - 25.84)^2}{25.84} + \dfrac{(14 - 13.78)^2}{13.78} + $$
        $$\dfrac{(12 - 7.89)^2}{7.89} \approx 8.958$$

        \newpage

        El test asintótico de tamaño $\alpha$ es

        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \hat{\chi}^2(N_1, \ldots, N_k) \geqslant \chi^2_{k-1;\alpha} \\
            0 &\text{si } \hat{\chi}^2(N_1, \ldots, N_k) < \chi^2_{k-1;\alpha}
        \end{cases}$$

        con $$p-valor = P_{H_0}[\hat{\chi}^2(N_1, \ldots, N_k) \geqslant \chi^2_{exp}] \approx_{n \to +\infty} P[\chi^2(k-q-1) \geqslant \chi^2_{exp}]$$

        y $\chi^2_{exp}$ el valor del estadístico obtenido con la muestra observada. Usando que $k-q-1=3$, obtenemos
        $$p-valor \approx P[\chi^2(3) \geqslant 8.958] \approx 0,025$$

        Como $p-valor \leq \alpha = 0.05$, rechazamos $H_0$ a nivel de significación $0.05$, por lo que puede suponerse que la distribución de los datos 
        \textbf{no} se corresponde con la de una Poisson.
    \end{ejercicio}
\end{document}
