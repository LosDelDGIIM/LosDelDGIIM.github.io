\documentclass[12pt]{article}

\input{../../_assets/preambulo.tex}

\newcommand{\E}{\mathrm{E}}
\newcommand{\V}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rmas}{\R^{+}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\I}{\mathrm{Im}}
\newcommand{\Rmenos}{\R^{-}}
\newcommand{\rg}{\text{rg}}

\begin{document}

    % 1. Foto de fondo
    % 2. Título
    % 3. Encabezado Izquierdo
    % 4. Color de fondo
    % 5. Coord x del titulo
    % 6. Coord y del titulo
    % 7. Fecha

    
    \input{../../_assets/portada}
    \portadaExamen{ffccA4.jpg}{Inferencia \\ Estadística\\Examen II}{Inferencia Estadística. Examen II}{MidnightBlue}{-8}{28}{2026}{José Manuel Sánchez Varbas}

    \begin{description}
        \item[Asignatura] Inferencia Estadística.
        \item[Curso Académico] 2024-25.
        \item[Grado] Grado en Matemáticas y Grado en Ingeniería Informática y Matemáticas.
        \item[Descripción] Examen Ordinario.
        \item[Fecha] 23 de Enero de 2025.  
    \end{description}
    \newpage


    % ------------------------------------

    \begin{ejercicio}
        \ \\
        \begin{enumerate}[label=\alph*)]
            \item Sea $(X_1,\ldots,X_{n_1})$ una m.a.s. de una variable $X\rightsquigarrow \cc{N}(\mu_1^0,\sigma^2_1)$ e $(Y_1,\ldots,Y_{n_2})$ una m.a.s. de $Y\rightsquigarrow \cc{N}(\mu_2,\sigma_2^2)$ siendo $\mu_2,\sigma_1^2,\sigma_2^2$ desconocidos y ambas muestras independientes. Determinar un pivote para $\frac{\sigma_1^2}{\sigma_2^2}$ basado en
                \begin{equation*}
                    U = \sum_{i=1}^{n_1}{(X_i - \mu_1^0)}^{2}, \qquad V = \sum_{j=1}^{n_2}{(Y_j-\overline{Y})}^{2}
                \end{equation*}
                donde $\overline{Y}$ es la media muestral. Dar la demostración detallada para obtener la distribución del pivote, justificando todos los pasos necesarios (basándose únicamente en las propiedades de la distribución $\chi^2(n)$ y el Lema de Fisher).
            \item Sea $X_1,\ldots,X_n$ muestra aleatoria simple de $X$ con $F_\theta(x) = 1-e^{\theta-x},\quad x>\theta$. Encontrar el intervalo de confianza para $\theta$ de menor longitud media uniformemente a nivel de confianza $1-\alpha$, basado en el estadístico $T = \min X_i$.
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}\label{ej:2}
        Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con función de densidad
        \begin{equation*}
            f_\theta(x) = \frac{1}{2\theta\sqrt{x-1}}; \qquad 1<x\leq \theta^2+1
        \end{equation*}
        \begin{enumerate}[label=\alph*)]
            \item Encontrar, si existe, el UMVUE para la función paramétrica $g(\theta) = \nicefrac{1}{\theta}$, justificando detalladamente el por qué de la no existencia si corresponde.
            \item Expresar la función de verosimilitud de $\lm = \theta^2$ asociada a una realización muestral cuyo máximo valor es 5.
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}
        \ \\
        \begin{enumerate}[label=\alph*)]
            \item Sea $X$ una variable aleatoria continua con distribución en una familia regular en el sentido de Fréchet-Crámer-Rao con función de densidad
                \begin{equation*}
                    f_\theta(x) = \exp[Q(\theta) T(x) + D(\theta) + S(x)], \qquad x\in \cc{X}, \quad \theta\in \mathbb{R}^+
                \end{equation*}
                donde $T(X)$ es un estimador insesgado de $\theta^4$, regular, tal que $\V_\theta[T(X)] = \theta^2$. Calcular las funciones $Q(\theta)$ y $D(\theta)$ sabiendo que $Q(1) = 0$ y $D(0) = 1$.
            \item Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con función de densidad:
                \begin{equation*}
                    f_\theta(x) = \frac{\theta}{x^{1+\theta}}, \qquad x>1
                \end{equation*}
                \begin{enumerate}[label=\alph*)]
                    \item Sabiendo que la familia de distribuciones es regular y $\E_\theta[\ln(X)] = \frac{1}{\theta}$ y $\V_\theta[\ln(X)] = \frac{1}{\theta^2}$, calcular la función de información asociada a la muestra. Encontrar la clase de funciones paramétricas que admiten estimador eficiente y los estimadores correspondientes.
                    \item Calcular la cota para la varianza de estimadores insesgados en $\theta^2$, regulares, y justificar si se alcanza o no dicha cota.
                \end{enumerate}
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}
        Encontrar el test más potente de tamaño arbitrario para contrastar
        \begin{equation*}
            \left\{\begin{array}{l}
                H_0: \theta = 3 \\
                H_1 : \theta = \theta_1
            \end{array}\right.
        \end{equation*}
        con $\theta_1<3$, a partir de muestra muestra aleatoria simple $(X_1,\ldots,X_n)$ de una variable aleatoria $X$ con función de densidad dada en el Ejercicio~\ref{ej:2}. Calcular la potencia del test obtenido.\newline
        \textit{Aplicaciones:} Si se considera una muestra de tamaño 4:
        \begin{enumerate}[label=\alph*)]
            \item Determinar para qué valores de $\theta_1$ la potencia del correspondiente test de Neyman-Pearson de tamaño $0.05$ es como mínimo $0.8$.
            \item Usando el test óptimo a nivel de significación $0.1$ para contrastar
            \begin{equation*}
                \left\{\begin{array}{l}
                    H_0: \theta = 3 \\
                    H_1 : \theta = 1.8
                \end{array}\right.
            \end{equation*}
            ¿qué decisión debe tomarse si el mayor de los valores observados es $4.2$?
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}
        \ \\
        \begin{enumerate}[label=\alph*)]
            \item Sea $(X,Y)$ un vector aleatorio bidimensional.
                \begin{enumerate}[label=\alph*)]
                    \item ¿En qué consiste el problema de regresión lineal de $Y$ sobre $X$? Especificar las hipótesis iniciales para su resolución.
                    \item Detallar el modelo que debe usarse para resolver el problema anterior, especificando qué es cada componente y qué describe cada una de las variables aleatorias que aparecen. Demostrar que es un modelo de Gauss-Markov, imponiendo, si es necesario, hipótesis adicionales sobre las variables observadas.
                \end{enumerate}
            \item Se desea comparar el volumen de ventas de cierto artículo en tres ciudades diferentes. Para ello se toma una muestra de establecimientos en cada ciudad y se mide el volumen de ventas, en miles de artículos durante los últimos diez meses, obteniéndose:
                \begin{table}[H]
                \centering
                \begin{tabular}{c|cccccc}
                    Ciudad 1 & 30 & 20 & 27.5 & 32.5 & &\\
                    \hline
                    Ciudad 2 & 20 & 32.5 & 37.5 & 30 & 35 & 25 \\
                    \hline
                    Ciudad 3 & 15 & 15 & 25 & 20 & 25 & 
                \end{tabular}
                \end{table}
                ¿Proporcionan estos datos evidencia para rechazar que las ventas se distribuyen de igual forma en las tres ciudades, al nivel de significación $0.025$? Especificar las hipótesis sobre la distribución de las variables aleatorias involucradas en este problema, así como las hipótesis concretas que se están contrastando.
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \newpage
    \setcounter{ejercicio}{0} % Reiniciar contador de ejercicios
    \noindent
    \textbf{Solución.}


    \begin{ejercicio}
        \ \\
        \begin{enumerate}[label=\alph*)]
            \item Sea $(X_1,\ldots,X_{n_1})$ una m.a.s. de una variable $X\rightsquigarrow \cc{N}(\mu_1^0,\sigma^2_1)$ e $(Y_1,\ldots,Y_{n_2})$ una m.a.s. de $Y\rightsquigarrow \cc{N}(\mu_2,\sigma_2^2)$ siendo $\mu_2,\sigma_1^2,\sigma_2^2$ desconocidos y ambas muestras independientes. Determinar un pivote para $\frac{\sigma_1^2}{\sigma_2^2}$ basado en
                \begin{equation*}
                    U = \sum_{i=1}^{n_1}{(X_i - \mu_1^0)}^{2}, \qquad V = \sum_{j=1}^{n_2}{(Y_j-\overline{Y})}^{2}
                \end{equation*}
                donde $\overline{Y}$ es la media muestral. Dar la demostración detallada para obtener la distribución del pivote, justificando todos los pasos necesarios (basándose únicamente en las propiedades de la distribución $\chi^2(n)$ y el Lema de Fisher). \\

                El esquema de resolución se compondrá de tres pasos:
                \begin{enumerate}[label=\arabic*)]
                    \item Se obtendrá un pivote para $\sigma_1^2$ siendo $\mu_1^0$ conocida.
                    \item Se obtendrá otro para $\sigma_2^2$ siendo $\mu_2$ desconocida.
                    \item Como ambos pivotes son independientes por ser cada uno función de una de las muestras, independientes, 
                    se aplicará la construcción de la $F$ de Snedecor, para obtener el pivote para $\sigma_1^2 / \sigma_2^2$.
                \end{enumerate}
                
                Aplicamos el primer paso. El enunciado nos dice que $X \rightsquigarrow \cc{N}(\mu_1^0, \sigma_1^2)$, luego, 
                como $(X_1, \ldots, X_{n_1})$ es una m.a.s. de $X$, se tiene que $$X_i \rightsquigarrow \cc{N}(\mu_1^0, \sigma_1^2) \quad
                \forall i = 1, \ldots, n_1$$ Tipificando para cada $i = 1, \ldots, n$:
                $$\dfrac{X_i - \mu_1^0}{\sigma_1} \rightsquigarrow \cc{N}(0,1) \stackrel{(*)}{\Longrightarrow} 
                \dfrac{(X_i - \mu_1^0)^2}{\sigma_1^2} \rightsquigarrow \chi^2(1)$$

                donde en $(*)$ se ha aplicado una propiedad de la $\chi^2(n)$. Nuevamente, como $(X_1, \ldots, X_{n_1})$ es una m.a.s. de $X$,
                las variables de la m.a.s. son independientes, y se puede usar la reproductividad de la $\chi^2(n)$:
                $$\dfrac{\displaystyle\sum_{i=1}^{n_1}(X_i - \mu_1^0)^2}{\sigma_1^2} \rightsquigarrow \chi^2(n_1)$$
                Aplicamos ahora el segundo paso. Veamos que $$\dfrac{(n_2-1)S_2^2}{\sigma_2^2} \rightsquigarrow \chi^2(n_2-1)$$
                es un pivote para $\sigma_2^2$ siendo $\mu_2$ desconocida.
                \begin{proof}
                    $$\sum_{j=1}^{n_2} (Y_j - \mu_2)^2 = \sum_{j=1}^{n_2} (\overbrace{Y_j - \overline{Y}}^{a} + 
                    \overbrace{\overline{Y} - \mu_2}^{b})^2$$
                    y aplicamos el cuadrado de un binomio $(a+b)^2 = a^2 + b^2 + 2ab$:
                    $$\sum_{j=1}^{n_2} (Y_j - \mu_2)^2 = \sum_{j=1}^{n_2} ((Y_j - \overline{Y})^2 + (\overline{Y} - \mu_2)^2
                    + 2 (Y_j - \overline{Y})(\overline{Y} - \mu_2)) = $$
                    $$\sum_{j=1}^{n_2} (Y_j - \overline{Y})^2 + \sum_{j=1}^{n_2} (\overline{Y} - \mu_2)^2 + 
                    \cancelto{0}{\sum_{j=1}^{n_2} 2 (Y_j - \overline{Y})(\overline{Y} - \mu_2)} = $$
                    $$(n_2-1)S_2^2 + n_2 (\overline{Y} - \mu_2)^2$$
                    Dividimos a cada lado de los extremos de la cadena de igualdades por $\sigma_2^2 > 0$:
                    $$\underbrace{\dfrac{\sum_{j=1}^{n_2} (Y_j - \mu_2)^2}{\sigma_2^2}}_{A} = 
                    \underbrace{\dfrac{(n_2-1)S_2^2}{\sigma_2^2}}_B + 
                    \underbrace{\dfrac{n_2 (\overline{Y} - \mu_2)^2}{\sigma_2^2}}_C$$
                    Y vemos que 
                    \begin{itemize}
                        \item $A \rightsquigarrow \chi^2(n_2)$, por un argumento completamente análogo al que se ha hecho en el primer paso.
                        \item $C \rightsquigarrow \chi^2(1)$, porque sabemos que $\overline{Y} \rightsquigarrow \cc{N}(\mu_2, \sigma_2^2/n_2)$,
                        y tipificando:
                        $$\dfrac{\overline{Y} - \mu_2}{\sigma_2 / \sqrt{n}} \rightsquigarrow \cc{N}(0,1)$$
                        y aplicando la propiedad $(*)$ del primer paso
                        $$\dfrac{\overline{Y} - \mu_2}{\sigma_2 / \sqrt{n}} \rightsquigarrow \cc{N}(0,1) \stackrel{(*)}{\Longrightarrow}
                        \dfrac{(\overline{Y} - \mu_2)^2}{\sigma_2^2 / n} = \dfrac{n(\overline{Y} - \mu_2)^2}{\sigma_2^2} \rightsquigarrow \chi^2(1)$$
                    \end{itemize}
                    Ahora, como $A = B + C$, y $B$ y $C$ son independientes porque son funciones medibles de variables independientes
                    ($B$ es función medible de $S_2^2$ y $C$ función medible de $\overline{Y}$, independientes por el Lema de Fisher),
                    podemos utilizar el teorema de multiplicación de esperanzas:
                    $$M_A(t) = \E[e^{tA}] = \E[e^{t(B+C)}] = \E[e^{tB} e^{tC}] \stackrel{TME}{=} \E[e^{tB}]
                    \E[e^{tC}] = M_B(t) M_C(t)$$
                    La función generatriz de momentos de la $\chi^2$ es conocida, $$M(t) = \dfrac{1}{(1-2t)^{n/2}}, 
                    \quad t < 1/2$$
                    por lo que basta despejar $M_B(t)$, con $A \rightsquigarrow \chi^2(n)$ y $C \rightsquigarrow \chi^2(1)$:
                    $$\dfrac{1}{(1-2t)^{n/2}} = M_A(t) = M_B(t) M_C(t) = M_B(t) \dfrac{1}{(1-2t)^{1/2}} \iff $$
                    $$M_B(t) = \dfrac{(1-2t)^{1/2}}{(1-2t)^{n/2}} = (1-2t)^{1/2-n/2} = (1-2t)^{(1-n)/2} = 
                    \dfrac{1}{(1-2t)^{(n-1)/2}} \quad t < 1/2$$
                    y por el Teorema de Unicidad de la Función Generatriz de Momentos, $$\dfrac{(n_2-1)S_2^2}{\sigma_2^2} = B 
                    \rightsquigarrow \chi^2(n_2-1)$$
                    lo que concluye la prueba.
                \end{proof}
                Tenemos en este punto el primer y segundo paso completados. Falta el tercero. Ambos pivotes
                $$\dfrac{(X_i - \mu_1^0)^2}{\sigma_1^2} = \dfrac{U}{\sigma_1^2} \rightsquigarrow \chi^2(n_1)$$
                $$\dfrac{(n_2-1)S_2^2}{\sigma_2^2} = \dfrac{V}{\sigma_2^2} \rightsquigarrow \chi^2(n_2-1)$$
                son independientes por serlo las muestras. Aplicamos la construcción de la $F$ de Snedecor para obtener
                el pivote para $\sigma_1^2 / \sigma_2^2$:
                $$\dfrac{\dfrac{\dfrac{U}{\sigma_1^2}}{n_1}}{\dfrac{\dfrac{V}{\sigma_2}}{{n_2-1}}} = 
                \dfrac{\dfrac{U}{n_1 \sigma_1^2}}{\dfrac{V}{(n_2-1)\sigma_2^2}} = 
                \dfrac{U}{n_1 \sigma_1^2} : \dfrac{V}{(n_2-1)\sigma_2^2} = 
                \dfrac{(n_2-1)U\sigma_2^2}{n_1 V \sigma_1^2} = \dfrac{(n_2-1) U}{n_1 V} \dfrac{\sigma_2^2}{\sigma_1^2}
                \rightsquigarrow F(n_1, n_2-1)$$
                Para que quede el pivote para $\sigma_1^2 / \sigma_2^2$, podemos usar la siguiente propiedad de la $F$ de
                Snedecor: $X \rightsquigarrow F(n,m) \iff \frac{1}{X} = X^{-1} \rightsquigarrow F(m,n)$, y finalmente
                $$\dfrac{1}{\dfrac{(n_2-1) U}{n_1 V} \dfrac{\sigma_2^2}{\sigma_1^2}} = 
                \dfrac{n_1 V}{(n_2-1) U} \dfrac{\sigma_1^2}{\sigma_2^2} \rightsquigarrow F(n_2-1,n_1)$$
            \item Sea $X_1,\ldots,X_n$ muestra aleatoria simple de $X$ con $F_\theta(x) = 1-e^{\theta-x},\quad x>\theta$. 
            Encontrar el intervalo de confianza para $\theta$ de menor longitud media uniformemente a nivel de confianza $1-\alpha$, basado en el estadístico $T = \min X_i$. \\

            Sabemos que la distribución del mínimo es
            $$F_T(t) = 1 - (1 - F_\theta(t))^n = 1 - (1 - (1-e^{\theta-t}))^n = 1 - e^{n(\theta - t)} \quad t > \theta$$
            y como la distribución es continua, por teoría podemos tomar como función pivote, 
            $S : \X^n \times \Theta \to \R$, dada por 
            $$S \equiv S(X_1, \ldots, X_n; \theta) = F_T(T) = 1 - e^{n(\theta - X_{(1)})} = 1 - e^{n(\theta - T)} \rightsquigarrow U(0,1)$$ que es
            una variable aleatoria cuya distribución, $U(0,1)$, es independiente del parámetro $\theta$.
            Además:
            \begin{enumerate}
                \item $S$ es estrictamente monótona en $\theta$, pues
                $$\dfrac{\partial S}{\partial \theta} = \dfrac{\partial}{\partial \theta} (1 - e^{n(\theta - T)}) = - ne^{n(\theta - T)} < 0, \quad \forall \theta \in \Theta = \R, \quad \forall n \in \N$$
                \item $S = \lm$ tiene solución en $\theta$, para todo $\lm \in \Lambda = \left]0, 1 \right[$, con $\Lambda$ la imagen de $S$, ya que
                $$T > \theta \Longrightarrow \theta - T < 0 \Longrightarrow n(\theta - T) < 0 \Longrightarrow e^{n(\theta - T)} = \dfrac{1}{e^{-n(\theta-T)}} \in \left]0, 1 \right[ \Longrightarrow$$
                $$1 - e^{n(\theta - T)} \in \left]0,1\right[$$
                y la solución existe, pues
                $$S = \lm \iff 1 - e^{n(\theta - T)} = \lm \iff 1 - \lm = e^{n(\theta - T)} \iff$$
                $$\ln(1-\lm) = n(\theta -T) = n\theta - nT \iff \theta = \dfrac{\ln(1-\lambda)  + nT}{n} = \dfrac{\ln(1-\lm)}{n} + T \in \R$$ 
            \end{enumerate}
            Por un teorema visto en teoría, sabemos que se puede construir un intervalo de confianza para $\theta$ a cualquier nivel
            de confianza $1-\alpha$, con $0 < \alpha < 1$. Por el método de la cantidad pivotal, buscamos $\lm_1 < \lm_2$ 
            verificando $$P_\theta(\lm_1 < S < \lm_2) = 1 - \alpha$$
            Es decir,
            $$\lm_1 < S < \lm_2 \iff \lm_1 < 1 - e^{n(\theta - T)} < \lm_2 \iff \lm_1 - 1 < - e^{n(\theta - T)} < \lm_2 - 1 \iff $$
            $$1 - \lm_2 < e^{n(\theta - T)} < 1 - \lm_1 \stackrel{(*)}{\iff} \ln(1-\lm_2) < n(\theta-T) < \ln(1-\lm_1) \iff $$
            $$\dfrac{\ln(1-\lm_2)}{n} < \theta-T < \dfrac{\ln(1-\lm_1)}{n} \iff \dfrac{\ln(1-\lm_2)}{n} + T < \theta < \dfrac{\ln(1-\lm_1)}{n} + T$$
            y el intervalo tendrá longitud $$L = \left(\dfrac{\ln(1-\lm_1)}{n} + T \right) - \left(\dfrac{\ln(1-\lm_2)}{n} + T \right) = \dfrac{\ln(1-\lm_1) - \ln(1-\lm_2)}{n} = 
            \dfrac{\ln \left( \dfrac{1-\lm_1}{1-\lm_2} \right)}{n}$$ que es constante (fijado el tamaño $n \in \N$ de la m.a.s.), luego coincidirá 
            con la longitud esperada $\E_\theta[L] = L \quad \forall \theta \in \Theta$. \\
            
            Sea ahora $F$ la función de distribución de la distribución $U(0,1)$, es decir, $F = Id$. La restricción es 
            $$1 - \alpha = P_\theta(\lm_1 < T < \lm_2) = F(\lm_2) - F(\lm_1) = \lm_2 - \lm_1$$
            Minimizamos $\ln \left( \frac{1-\lm_1}{1-\lm_2} \right)$ con el método de los multiplicadores de Lagrange visto en teoría
            $$H(\lm_1, \lm_2) = \ln \left( \dfrac{1-\lm_1}{1-\lm_2} \right) - \lm[\lm_2 - \lm_1 - (1-\alpha)] = \ln(1-\lm_1) - \ln(1-\lm_2) - \lm[\lm_2 - \lm_1 - (1-\alpha)]$$
            donde no se ha considerado la parte constante de la longitud media por no afectar al procedimiento de minimización. 
            Buscamos aquellos $\lm_1, \lm_2$ que minimicen $H(\lm_1, \lm_2)$, luego obtenemos sus derivadas parciales
            $$\dfrac{\partial H}{\partial \lm_1} = -\dfrac{1}{1-\lm_1} + \lm \quad 
            \dfrac{\partial H}{\partial \lm_2} = \dfrac{1}{1-\lm_2} - \lm$$
            Ahora, igualamos ambas parciales a $0$ y despejamos $\lambda$:
            \begin{equation*}
                \left.\begin{array}{l}
                    0 = \dfrac{\partial H}{\partial \lm_1} = -\dfrac{1}{1-\lm_1} + \lm \\\\
                    0 = \dfrac{\partial H}{\partial \lm_2} = \dfrac{1}{1-\lm_2} - \lm
                \end{array}\right\} \Longrightarrow \left.\begin{array}{l}
                    \lm = \dfrac{1}{1-\lm_1} \\\\
                    \lm = \dfrac{1}{1-\lm_2}
                \end{array}\right\} \Longrightarrow \lm_1 = \lm_2
            \end{equation*}
            Y vemos que entonces, necesariamente, debe ser $(\lm_1, \lm_2) = (0,1)$, teniendo en cuenta que $0 \leqslant \lm_1 < \lm_2 \leqslant 1$, y en otro caso, no se cumplirá la restricción, puesto que 
            $$\lm_1 = \lm_2 \Longrightarrow \lm_1 - \lm_2 = 0 \neq 1 - \alpha \quad 0 < \alpha < 1$$
            Usando esta última $\lm_2 - \lm_1 = 1 - \alpha \iff \lm_2 = 1 - \alpha + \lm_1$, podemos reducir el problema de optimización a una variable real, quedándonos con, por ejemplo, $\lm_1$, y minimizando la longitud media esperada:
            $$g(\lm_1) = \ln \left( \frac{1-\lm_1}{1-(1 - \alpha + \lm_1)} \right) = \ln \left( \frac{1-\lm_1}{\alpha - \lm_1} \right) = \ln(1-\lm_1) - \ln(\alpha-\lm_1)$$ 
            ahora derivamos e igualamos a $0$ para obtener los puntos críticos:
            $$g'(\lm_1) = - \dfrac{1}{1-\lm_1} + \dfrac{1}{\alpha - \lm_1} = \dfrac{-(\alpha - \lm_1) + (1 - \lm_1)}{(\alpha - \lm_1)(1 - \lm_1)} = \dfrac{1 - \alpha}{(\alpha - \lm_1)(1 - \lm_1)} = 0 \iff$$
            $$\iff \alpha = 1$$
            lo cual no puede suceder, porque $\alpha \in \left]0,1 \right[$. Por tanto, $$g'(\lm_1) \neq 0 \quad \forall 0 < \lm_1 < \alpha < 1$$
            De hecho, $g'(\lm_1) > 0 \quad \forall \lm_1 \in \left]0,\alpha \right[$, porque
            $$\alpha < 1 \iff 1 - \alpha > 0$$
            $$\lm_1 < \alpha \iff \alpha - \lm_1 > 0$$
            $$\lm_1 < 1 \iff 1 - \lm_1 > 0$$
            por tanto, $g$ es estrictamente creciente como función de $\lm_1$, luego alcanzará su mínimo en $\lm_1 = 0$. Obtenemos $\lm_2$ recuperando la restricción
            $$\lm_2 = 1 - \alpha + \lm_1 = 1 - \alpha$$
            y el intervalo de confianza para $\theta$ de menor longitud media uniformemente a nivel de confianza $1-\alpha$, basado en el estadístico $T = \min X_i$, es:
            \begin{equation*}
                \left] \dfrac{\ln(1-\lm_2)}{n} + T, \dfrac{\ln(1-\lm_1)}{n} + T \right[ = \left] \dfrac{\ln(\alpha)}{n} + T, T \right[
            \end{equation*}
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}\label{ej:2}
        Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con función de densidad
        \begin{equation*}
            f_\theta(x) = \frac{1}{2\theta\sqrt{x-1}}; \qquad 1<x\leq \theta^2+1
        \end{equation*}
        \begin{enumerate}[label=\alph*)]
            \item Encontrar, si existe, el UMVUE para la función paramétrica $g(\theta) = \nicefrac{1}{\theta}$, justificando detalladamente el por qué de la no existencia si corresponde. \\
            
            Buscamos obtener el UMVUE mediante el método alternativo visto en teoría. Para ello, en primer lugar hay que encontrar un estadístico suficiente y completo $T$, y luego una función del estadístico $h(T)$
            (denotaremos indistintamente $T \stackrel{not}{\equiv} T(X_1, \ldots, X_n)$, para una m.a.s. $(X_1, \ldots, X_n)$ con $n \in \N$ fijo) insesgada en $g(\theta) = \theta^{-1}$, estimadora y con momento de segundo orden finito. Entonces $h(T)$ será el UMVUE. \\

            El estadístico suficiente se calcula por medio del Teorema de Factorización de Neyman-Fisher. La función conjunta es la siguiente
            $$f^n_\theta(x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^{n} f_\theta(x_i)$$
            Suponemos en este punto que $x_{(1)} > 1$ (de lo contrario, $f_\theta(x_i) = 0 \quad \forall i = 1, \ldots, n$), y vemos que 
            $$x_i \leq \theta^2 + 1 \quad \forall i = 1, \ldots, n \iff I_{\R_0^{-}}(x_i - (\theta^2+1)) = 1 \quad \forall i = 1, \ldots, n \iff $$
            $$I_{\R_0^{-}}(x_{(n)} - (\theta^2+1)) = 1$$ de donde se deduce que 
            $$f^n_\theta(x_1, \ldots, x_n) = \prod_{i=1}^{n} \frac{1}{2\theta\sqrt{x_i-1}} I_{\R_0^{-}}(x_{(n)} - (\theta^2+1)) = 
            (2\theta)^{-n} \prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}} I_{\R_0^{-}}(x_{(n)} - (\theta^2+1))$$
            Tomando $T(X_1, \ldots, X_n) = X_{(n)}$ y 
            $$h(x_1, \ldots, x_n) = \prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}}, \quad g_\theta(t) = 
            (2\theta)^{-n} I_{\R_0^{-}}(t - (\theta^2+1))$$
            Se cumple que 
            $$f^n_\theta(x_1, \ldots, x_n) = h(x_1, \ldots, x_n) g_\theta(T(x_1, \ldots, x_n)) \quad \forall (x_1, \ldots, x_n) \in \X^n$$
            donde $h$ es independiente del parámetro $\theta$ y $g_\theta$ depende de la muestra solo a través del estadístico, luego,
            por el Teorema de Factorización de Neyman-Fisher, el estadístico $T$ es suficiente. 

            \newpage

            Ahora, hay que comprobar que este estadístico es completo, 
            lo cual se hará por definición. Sabemos por teoría que la distribución del máximo es 
            $$F_T(t) = (F_X(t))^n \Longrightarrow f_T(t) = n(F_X(t))^{n-1} f_\theta(t)$$
            Hallamos ahora la función de distribución de $X$:
            $$F_X(t) = \int_{1}^{t} f_\theta(x) dx = \int_{1}^{t} \frac{1}{2\theta\sqrt{x-1}} dx = \dfrac{1}{2 \theta} \int_{1}^{t} \frac{1}{\sqrt{x-1}} dx = $$
            $$\dfrac{1}{2 \theta} \cdot \left[2 \sqrt{x-1} \right]_{1}^{t} = \dfrac{2 \sqrt{t-1}}{2\theta} = \dfrac{\sqrt{t-1}}{\theta} \quad 1 < t \leq \theta^2+1$$

            La función de densidad del estadístico será entonces
            $$f_T(t) = n(F_X(t))^{n-1} f_\theta(t) = n \left(\dfrac{\sqrt{t-1}}{\theta}\right)^{n-1} \frac{1}{2\theta\sqrt{t-1}} = $$
            $$\dfrac{(\sqrt{t-1})^{n-1}}{\theta^{n-1}} \frac{n}{2\theta\sqrt{t-1}} = \dfrac{n(\sqrt{t-1})^{n-2}}{2\theta^{n}} \quad 1 < t \leq \theta^2+1$$

            Sea $h$ una función medible verificando $$0 = \E[h(T)] \stackrel{def}{=} \int_{1}^{\theta^2+1} h(t) f_T(t) dt = 
            \int_{1}^{\theta^2+1} h(t) \dfrac{n(\sqrt{t-1})^{n-2}}{2\theta^{n}} dt =$$
            $$\dfrac{n}{2\theta^{n}} \int_{1}^{\theta^2+1} h(t) (\sqrt{t-1})^{n-2} dt$$
            como $\dfrac{n}{2\theta^{n}} \neq 0 \quad \forall n \in \N, \quad \forall \theta \in \Theta = \Rmas$, debe ser
            $$\int_{1}^{\theta^2+1} h(t) (\sqrt{t-1})^{n-2} dt = 0$$
            Por el Teorema Fundamental del Cálculo, podemos considerar una primitiva $H(t)$ del integrando $h(t) (\sqrt{t-1})^{n-2}$, y esta cumple,
            por la Regla de Barrow, que $H(\theta^2+1) - H(1) = 0 \quad \forall \theta \in \Theta$. Derivando respecto de $\theta$, se obtiene que
            $$\dfrac{d}{d \theta}H(\theta^2+1) = 0 \iff 2 \theta^{n-1} h(\theta^2+1) = 0 \stackrel{(*)}{\iff} h(\theta^2+1) = 0$$
            donde en $(*)$ se ha usado que $2 \theta^{n-1} \neq 0$ por ser $\theta \in \Theta$ y $2 > 0$. Equivalentemente, 
            $$\forall \theta \in \Theta \quad h(\theta^2+1) = 0 \iff h(t) = 0 \quad \forall t > 1$$ (tomando $t = \theta^2+1 \in \left]1, +\infty \right[$).
            Por tanto $$\left]1, +\infty \right[ \subseteq \{t : h(t) = 0\}$$ y consecuentemente $$1 \geqslant P[h(T) = 0] \geqslant P[T > 1] = 1
            \Longrightarrow P[h(T) = 0] = 1$$

            y entonces por definición concluimos que $T$ es un estadístico completo. Tenemos entonces en este punto
            que $T$ es un estadístico suficiente y completo. \\

            Ahora hay que buscar un estimador insesgado en $g(\theta)$ y de segundo orden finito. Sea $h$ (independiente de la anterior) función medible tal que
            $$\theta^{-1} = g(\theta) = \E[h(T)] = \dfrac{n}{2\theta^{n}} \int_{1}^{\theta^2+1} h(t) (\sqrt{t-1})^{n-2} dt \iff$$
            $$\int_{1}^{\theta^2+1} h(t) (\sqrt{t-1})^{n-2} dt = \dfrac{2 \theta^{n-1}}{n} $$

            Derivamos respecto de $\theta$ a ambos lados e igualamos. El miembro izquierdo ya lo tenemos por el apartado anterior:
            $$2 \theta^{n-1} h(\theta^2+1)$$
            y el derecho es 
            $$\dfrac{2(n-1)}{n} \theta^{n-2}$$
            Despejamos $h(\theta^2+1)$:
            $$2 \theta^{n-1} h(\theta^2+1) = \dfrac{2(n-1)}{n} \theta^{n-2} \iff $$
            $$h(\theta^2+1) = \dfrac{2(n-1)}{2n} \dfrac{\theta^{n-2}}{\theta^{n-1}} = \dfrac{n-1}{n} \dfrac{1}{\theta}$$
            de donde
            $$h(t) = \dfrac{n-1}{n} \dfrac{1}{\sqrt{t-1}}$$

            Por construcción $h(T)$ es insesgada en $g(\theta)$. Vemos que $h(T)$ también es estimador de $g(\theta)$, 
            pues $\Theta = \Rmas$, y $g(\theta) = \frac{1}{\theta} \Longrightarrow g(\Theta) = \Rmas$. 
            Como $T = X_{(n)} > 1$, entonces $T - 1 > 0 \Longrightarrow \frac{1}{\sqrt{T-1}} > 0$, y $\frac{n-1}{n} > 0$ si $n \geq 2$, luego $h(T) > 0$ si $n \geq 2$. Queda comprobar que tiene momento de segundo orden finito. \\

            Ello se cumplirá en caso de que $E[h(T)^2] < + \infty$:
            $$E[h(T)^2] \stackrel{def}{=} \int_{1}^{\theta^2+1} h(t)^2 f_T(t) dt = \int_{1}^{\theta^2+1} 
            \left( \dfrac{n-1}{n} \right)^2 \dfrac{1}{t-1} \dfrac{n(\sqrt{t-1})^{n-2}}{2\theta^{n}} dt = $$
            $$\dfrac{(n-1)^2}{2n\theta^{n}} \int_{1}^{\theta^2+1} (t-1)^{(n-4)/2}dt = 
            \dfrac{(n-1)^2}{2n\theta^{n}} \left[\dfrac{2}{(n-2)} (t-1)^{(n-2)/2} \right]_1^{\theta^2+1} = $$
            $$\dfrac{(n-1)^2}{n\theta^{n}} \dfrac{(\theta^2)^{(n-2)/2}}{(n-2)} = \dfrac{(n-1)^2}{n(n-2)} \dfrac{1}{\theta^2}$$

            Y vemos que $E[h(T)^2] < + \infty \iff n > 2$, ya que si $n \leq 2$, el momento de segundo orden no es finito.
            Por tanto, por el Teorema de Lehmann-Scheffé, $E[h(T)/T] = h(T)$ es el UMVUE para $g(\theta)$, y existe siempre y
            cuando $$n>2 \iff n \geq 3$$

            \newpage

            \item Expresar la función de verosimilitud de $\lm = \theta^2$ asociada a una realización muestral cuyo máximo valor es 5. \\
            
            Vemos que $\X = \left]1, \theta^2+1 \right]$. Sea $(x_1, \ldots, x_n) \in \X^n$ tal que 
            $x_{(n)} = 5$. La función de densidad conjunta ya se ha obtenido en el apartado anterior:
            $$f^n_\theta(x_1, \ldots, x_n) = (2\theta)^{-n} \prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}} I_{\R_0^{-}}(x_{(n)} - (\theta^2+1))$$
            Por definición de función de verosimilitud:
            $$L_{x_1, \ldots, x_n}(\theta) = f_\theta^n(x_1, \ldots, x_n) = 
            (2\theta)^{-n} \prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}} I_{\R_0^{-}}(x_{(n)} - (\theta^2+1))
            \quad \forall \theta \in \Theta = \Rmas$$
            El enunciado nos dice que $\lambda = g(\theta) = \theta^2$. Por definición 
            de función de verosimilitud asociada a una función paramétrica:
            $$M_{x_1, \ldots, x_n}(\lambda) = \sup_{\theta \in g^{-1}(\lambda)} L_{x_1, \ldots, x_n}(\theta)$$ 
            Resolvemos $\theta^2 = \lambda$ para expresar $\theta$ explícitamente en función de $\lambda$. 
            Vemos que $\lambda \geqslant 0$, y 
            $$\theta^2 = \lambda \iff \theta = \pm \sqrt{\lambda}$$ 
            A priori habría dos candidatos para cada $\lambda$. Sin embargo, por restricciones del problema, 
            $5 = x_{(n)} \leqslant \theta^2 + 1 \iff 4 \leq \theta^2 \iff 2 \leq \theta$, lo que implica que
            $$\theta = \sqrt{\lambda} \geqslant 2 \iff \lambda \geqslant 4$$
            $$\theta = - \sqrt{\lambda} \geqslant 2 \iff \sqrt{\lambda} \leqslant -2$$
            La última opción no puede darse por ser $\lambda \geqslant 0$, por tanto, nos quedamos con la primera. 
            Así, si $\lambda \geqslant 4$ (en otro caso, $M_{x_1, \ldots, x_n}(\lambda) = 0$)
            $$M_{x_1, \ldots, x_n}(\lambda) = L_{x_1, \ldots, x_n} (\sqrt{\lambda}) = 
            (2 \sqrt{\lm})^{-n} \prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}}$$
            Aunque no se pide, como $\prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}}$ es fijo, y $2 \sqrt{\lambda}$ es creciente como 
            función de $\lambda$ y $(2 \sqrt{\lm})^{-n}$ es decreciente como función de $\lambda$, y 
            $$M_{x_1, \ldots, x_n}(\lambda) \neq 0 \iff \lambda \in [4, +\infty[$$ entonces
            el máximo se alcanza en el extremo inferior del intervalo, es decir 
            $\hat{\lambda}(x_1, \ldots, x_n) = 4$. Esto puede comprobarse también con el 
            Teorema de Invarianza de Zehna pues $L_{x_1, \ldots, x_n}(\theta)$ es decreciente en 
            $\left[\sqrt{x_{(n)}-1}, +\infty \right[ = \left[2, + \infty \right[ $. 
            Por el mismo razonamiento, $\hat{\theta}(x_1, \ldots, x_n) = 2$, luego 
            $\hat{\lambda} = \hat{\theta}^2 = 2^2 = 4$.

        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}
        \ \\
        \begin{enumerate}[label=\alph*)]
            \item Sea $X$ una variable aleatoria continua con distribución en una familia regular en 
                el sentido de Fréchet-Crámer-Rao con función de densidad
                \begin{equation*}
                    f_\theta(x) = \exp[Q(\theta) T(x) + D(\theta) + S(x)], \qquad x\in \cc{X}, \quad \theta\in \mathbb{R}^+
                \end{equation*}
                donde $T(X)$ es un estimador insesgado de $\theta^4$, regular, tal que 
                $\V_\theta[T(X)] = \theta^2$. Calcular las funciones $Q(\theta)$ y $D(\theta)$ sabiendo 
                que $Q(1) = 0$ y $D(0) = 1$. \\

                Primero obtenemos relaciones con las condiciones que nos da el enunciado. 
                Calculamos:
                $$\ln f_\theta(x) = Q(\theta) T(x) + D(\theta) + S(x) \Longrightarrow 
                \dfrac{\partial \ln f_\theta(X)}{\partial \theta} = T(X) Q'(\theta) + D'(\theta)$$
                y usamos que la familia es regular
                $$\E_\theta \left[ \dfrac{\partial \ln f_\theta(X)}{\partial \theta} \right] = 0 \iff 
                \E_\theta \left[ T(X) Q'(\theta) + D'(\theta) \right] = 0 \iff $$
                $$Q'(\theta) \E_\theta \left[ T(X) \right] + D'(\theta) = 0 \iff$$
                \begin{equation}\label{eq:rel1ej2a}
                    \theta^4 \stackrel{(*)}{=} \E_\theta[T(X)] = -\dfrac{D'(\theta)}{Q'(\theta)} \iff D'(\theta) = - \theta^4 Q'(\theta)
                \end{equation}
                donde en $(*)$ se ha utilizado que $T$ es insesgado en $\theta^4$. Ahora, por ser $T$ regular, se tiene que
                $$\dfrac{\partial}{\partial \theta} E_\theta[T(X_1, \ldots, X_n)] = E_\theta \left[T(X_1, \ldots, X_n) \dfrac{\partial \ln f_\theta^n (X_1, \ldots, X_n)}
                {\partial \theta} \right]$$
                en particular para $n=1$
                $$\dfrac{\partial}{\partial \theta} E_\theta[T(X)] = E_\theta \left[T(X) \dfrac{\partial \ln f_\theta (X)}
                {\partial \theta} \right]$$

                Nuevamente, como $T$ es insesgado en $\theta^4$, podemos obtener el miembro izquierdo 
                de la igualdad
                $$\dfrac{\partial}{\partial \theta} E_\theta[T(X)] = 
                \dfrac{\partial}{\partial \theta} (\theta^4) = 4 \theta^3$$

                y el miembro derecho, usando lo ya calculado al principio, es
                $$\E_\theta \left[T(X) \dfrac{\partial \ln f_\theta (X)}
                {\partial \theta} \right] = 
                \E_\theta \left[T(X) \left( T(X) Q'(\theta) + D'(\theta) \right) \right] = 
                \E_\theta \left[T(X)^2 Q'(\theta) + T(X)D'(\theta) \right] = $$
                $$Q'(\theta) \E_\theta[T(X)^2] + D'(\theta) \E_\theta[T(X)] \stackrel{(*)}{=}
                Q'(\theta) \E_\theta[T(X)^2] + \theta^4 D'(\theta) \stackrel{(**)}{=} $$
                $$Q'(\theta) (\V_\theta[T(X)] + (\E_\theta[T(X)])^2) + \theta^4 D'(\theta) = 
                (\theta^2 + \theta^8) Q'(\theta) + \theta^4 D'(\theta)$$

                donde en $(*)$ se ha tenido en cuenta la insesgadez de $T$ en $\theta^4$ y 
                en $(**)$ que $\V_\theta[X] = \E_\theta[X^2] - (\E_\theta[X])^2$. 
                
                \newpage
                
                Juntando ambos miembros
                $$\dfrac{\partial}{\partial \theta} E_\theta[T(X)] = E_\theta \left[T(X) \dfrac{\partial \ln f_\theta (X)}
                {\partial \theta} \right] \iff$$
                \begin{equation}\label{eq:rel2ej2a}
                    4 \theta^3 = (\theta^2 + \theta^8) Q'(\theta) + \theta^4 D'(\theta)
                \end{equation}

                Podemos construir un sistema de ecuaciones con $(\ref{eq:rel1ej2a})$ y $(\ref{eq:rel2ej2a}$), y obtener $Q(\theta)$ y $D(\theta)$:
                $$\begin{cases}
                    D'(\theta) = - \theta^4 Q'(\theta) \\
                    4 \theta^3 = (\theta^2 + \theta^8) Q'(\theta) + \theta^4 D'(\theta)
                \end{cases}$$

                Sustituimos la primera expresión en la segunda
                $$4 \theta^3 = (\theta^2 + \theta^8) Q'(\theta) + \theta^4 D'(\theta) = (\theta^2 + \theta^8) Q'(\theta) + \theta^4 (-\theta^4 Q'(\theta)) = $$
                $$\theta^2 Q'(\theta) + \theta^8 Q'(\theta) - \theta^8 Q'(\theta) = \theta^2 Q'(\theta) \iff$$
                $$Q'(\theta) = 4 \theta$$

                y ahora integramos respecto de $\theta$ para hallar $Q(\theta)$:
                $$Q'(\theta) = 4 \theta \Longrightarrow Q(\theta) + c_1 = \int Q'(\theta) d \theta = \int 4 \theta d \theta = 4 \dfrac{\theta^2}{2} = 2 \theta^2 + c_2 \Longrightarrow
                Q(\theta) = 2 \theta^2 + c_2 - c_1$$
                llamando $c_2 - c_1 \equiv c$, determinamos su valor con la condición inicial $Q(1) = 0$.
                $$2 + c = Q(1) = 0 \Longrightarrow c = -2 \Longrightarrow Q(\theta) = 2 \theta^2 - 2$$
                Como hemos visto que $Q'(\theta) = 4 \theta$, sustituimos esta expresión en $(\ref{eq:rel1ej2a})$ y obtenemos $D(\theta)$ igual que con $Q(\theta)$:
                $$D'(\theta) = - \theta^4 Q'(\theta) = - \theta^4 4 \theta = - 4 \theta^5 \Longrightarrow$$
                $$D(\theta) + k_1 = \int D'(\theta) d \theta = - 4 \int \theta^5 d \theta = - 4 \dfrac{\theta^6}{6} + k_2 = \dfrac{-2\theta^6}{3} + k_2 \Longrightarrow D(\theta) = \dfrac{-2\theta^6}{3} + k_2 - k_1$$
                llamando $k_2 - k_1 \equiv k$, determinamos su valor con la condición inicial $D(0) = 1$.
                $$k = D(0) = 1 \Longrightarrow D(\theta) = \dfrac{-2\theta^6}{3} + 1$$
                En resumen, las funciones $Q(\theta)$ y $D(\theta)$ son:
                $$Q(\theta) = 2 \theta^2 - 2 \quad D(\theta) = \dfrac{-2\theta^6}{3} + 1$$

                \newpage
            \item Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con función de densidad:
                \begin{equation*}
                    f_\theta(x) = \frac{\theta}{x^{1+\theta}}, \qquad x>1
                \end{equation*}
                \begin{enumerate}[label=\alph*)]
                    \item Sabiendo que la familia de distribuciones es regular y 
                    $\E_\theta[\ln(X)] = \frac{1}{\theta}$ y $\V_\theta[\ln(X)] = \frac{1}{\theta^2}$, 
                    calcular la función de información asociada a la muestra. Encontrar la clase de 
                    funciones paramétricas que admiten estimador eficiente y los estimadores 
                    correspondientes. \\

                    En primer lugar se obtiene
                    $$\ln f_\theta(x) = \ln \left( \frac{\theta}{x^{1+\theta}} \right) = 
                    \ln \theta - \ln \left(x^{1 + \theta} \right) = 
                    \ln \theta - (1+\theta) \ln x \Longrightarrow 
                    \dfrac{\partial \ln f_\theta(X)}{\partial \theta} = \dfrac{1}{\theta} - \ln X$$

                    Sabemos por teoría que 
                    $$I_X(\theta) = \V_\theta \left[ \dfrac{\partial \ln f_\theta(X)}{\partial \theta} \right]$$
                    y
                    $$\V_\theta \left[ \dfrac{\partial \ln f_\theta(X)}{\partial \theta} \right] = \V_\theta \left[ \dfrac{1}{\theta} - \ln X \right] \stackrel{(*)}{=}
                    \V_\theta[\ln X] \stackrel{(**)}{=} \dfrac{1}{\theta^2}$$
                    donde en $(*)$ se ha usado que $\V(aX + b) = a^2 \V(X)$ para todo $a,b \in \R$, y en $(**)$ que $\V_\theta[\ln X] = 1/\theta^2$ por hipótesis del enunciado.
                    Así $$I_X(\theta) = \dfrac{1}{\theta^2}$$

                    Para la segunda parte del enunciado, buscamos aplicar el Teorema de Caracterización de Estimadores Eficientes. Para ello, obtenemos la función conjunta de la m.a.s. de $X$

                    $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep}{=} \prod_{i=1}^n f_\theta(x_i)$$

                    Se supondrá a partir de ahora que $x_i > 1 \quad \forall i=1,\ldots,n$ y $\theta \in \Rmas$. De lo contrario, $f_\theta(x_i) = 0 \quad \forall i = 1, \ldots, n$.
                    $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep}{=} \prod_{i=1}^n f_\theta(x_i) = \prod_{i=1}^n 
                    \frac{\theta}{x_i^{1+\theta}} = \theta^n \prod_{i=1}^n \dfrac{1}{x_i^{1+\theta}} $$
                    $$\ln f_\theta^n(x_1, \ldots, x_n) = \ln \left( \theta^n \prod_{i=1}^n \dfrac{1}{x_i^{1+\theta}} \right) = \ln(\theta^n) + \sum_{i=1}^{n} \ln  \left(\dfrac{1}{x_i^{1+\theta}} \right) = $$
                    $$n \ln \theta - (1+\theta) \sum_{i=1}^{n} \ln(x_i) $$
                    $$\dfrac{\partial \ln f_\theta^n(x_1, \ldots, x_n)}{\partial \theta} = \dfrac{n}{\theta} - \left( \sum_{i=1}^{n} \ln x_i \right) = - \left(\sum_{i=1}^{n} \ln x_i - \dfrac{n}{\theta} \right) $$

                    Ahora, supongamos que $T(X_1, \ldots, X_n)$ es un estimador de $g(\theta)$ función paramétrica derivable y estrictamente monótona
                    ($g'(\theta) \neq 0 \quad \forall \theta \in \Theta = \Rmas$). Como el enunciado nos dice que la familia es regular, y 
                    $0 < I_X(\theta) = \nicefrac{1}{\theta^2} < +\infty \quad \forall \theta \in \Theta$, $T$ es eficiente si y solo si $\forall \theta \in \Theta
                    \quad \exists a(\theta) \neq 0$ tal que 
                    $$P_\theta \left[ \dfrac{\partial \ln f_\theta^n(X_1, \ldots, X_n)}{\partial \theta} = a(\theta)[T(X_1, \ldots, X_n) - g(\theta)]\right] = 1$$
                    y
                    $$I_{(X_1, \ldots, X_n)}(\theta) = a(\theta) g'(\theta)$$
                    Como 
                    $$\dfrac{\partial \ln f_\theta^n(X_1, \ldots, X_n)}{\partial \theta} = a(\theta)[T(X_1, \ldots, X_n) - g(\theta)] \iff$$
                    $$- \left(\sum_{i=1}^{n} \ln X_i - \dfrac{n}{\theta} \right) = a(\theta) [T(X_1, \ldots, X_n) - g(\theta)]$$
                    claramente por comparación se obtiene que 
                    $$T(X_1, \ldots, X_n) = \sum_{i=1}^{n} \ln X_i, \quad g(\theta) = \dfrac{n}{\theta}, \quad a(\theta) = -1$$
                    Es claro que $T(X_1, \ldots, X_n) = \sum_{i=1}^n \ln X_i$ es un estimador, pues $\Theta = \Rmas$, $g(\Theta) = \Rmas$, y 
                    $T(x_1, \ldots, x_n) > 0 \quad \forall (x_1, \ldots, x_n) \in \X^n = (]1, +\infty[)^n$. Además, tanto 
                    $a(\theta)$ como $g(\theta)$ verifican todas las condiciones del teorema, pues 
                    $$g'(\theta) = n \cdot (\theta^{-1})' = n \cdot (- \theta^{-2}) = -\dfrac{n}{\theta^2} < 0$$ porque $n, \theta > 0$
                    y $a(\theta) = -1 < 0$, en particular, $a(\theta) \neq 0 \quad \forall \theta \in \Theta$, y usando la aditividad de la función de información de Fisher, obtenemos
                    $$\dfrac{n}{\theta^2} = n I_X(\theta) = I_{(X_1, \ldots, X_n)}(\theta) = a(\theta) g'(\theta) = - \left( - \dfrac{n}{\theta^2} \right) = \dfrac{n}{\theta^2}$$
        
                    Por un corolario visto en teoría, como $T(X_1, \ldots, X_n)$ es un estimador eficiente para $g(\theta)$, con $g'(\theta) \neq 0$,
                    sabemos que las únicas funciones paramétricas que admiten estimadores eficientes son las de la forma $a g(\theta) + b$ y los correspondientes
                    estimadores eficientes son $aT + b$, con probabilidad $1$, bajo todas las distribuciones de la familia.

                    \newpage

                    \item Calcular la cota para la varianza de estimadores insesgados en $\theta^2$, regulares, y justificar si se alcanza o no dicha cota. \\
                    
                    Nos piden calcular la cota de Frechét-Crámer-Rao, definida para cualquier estimador regular, insesgado en $g(\theta) = \theta^2$ y, suponemos, de segundo orden, $T$, como sigue:
                    $$\V(T(X_1, \ldots, X_n)) \geq \dfrac{[g'(\theta)]^2}{I_{X_1, \ldots, X_n}(\theta)}$$
                    donde $0 < I_X(\theta) < + \infty$. Ya tenemos el numerador 
                    $$g(\theta) = \theta^2 \Longrightarrow g'(\theta) = 2 \theta \Longrightarrow [g'(\theta)]^2 = 4 \theta^2$$
                    y el denominador $$I_X(\theta) = \dfrac{1}{\theta^2}$$

                    Tenemos que $0 < I_X(\theta) = \nicefrac{1}{\theta^2} < + \infty$, y la mínima varianza es entonces, sabiendo que $I_{X_1, \ldots, X_n}(\theta) = n I_X(\theta)$, la que sigue:

                    $$\dfrac{[g'(\theta)]^2}{I_{X_1, \ldots, X_n}(\theta)} = \dfrac{[g'(\theta)]^2}{n I_{X}(\theta)} = 4 \theta^2 \dfrac{\theta^2}{n} = \dfrac{4 \theta^4}{n}$$

                    La cota no se alcanza por reducción al absurdo. Si lo hiciera, entonces tendríamos que 
                    $g (\theta) = \theta^2$ admitiría un estimador regular, insesgado y cuya varianza alcanza 
                    la cota de FCR para cualquier valor $\theta \in \Theta$, es decir, un estimador eficiente 
                    (dado que $\{F_\theta : \theta \in \Theta = \Rmas\}$ es una familia regular, 
                    $0 < I_X(\theta) = 1/\theta^2 < + \infty \quad \forall \theta \in \Theta$ y 
                    $g(\theta) = \theta^2$ es una función paramétrica derivable). Sin embargo, 
                    hemos visto en el apartado a) del apartado b) que las únicas funciones paramétricas que admiten estimador eficiente en esta familia son las 
                    de la forma $$a \dfrac{n}{\theta} + b \quad a, b \in \R$$
                    Como $\nexists a,b \in \R : \theta^2 = a \dfrac{n}{\theta} + b$, hemos llegado a contradicción.

                \end{enumerate}
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}
        Encontrar el test más potente de tamaño arbitrario para contrastar
        \begin{equation*}
            \left\{\begin{array}{l}
                H_0: \theta = 3 \\
                H_1 : \theta = \theta_1
            \end{array}\right.
        \end{equation*}
        con $\theta_1<3$, a partir de muestra muestra aleatoria simple $(X_1,\ldots,X_n)$ de una variable aleatoria $X$ con función de densidad dada en el Ejercicio~\ref{ej:2}. Calcular la potencia del test obtenido.\newline
        \textit{Aplicaciones:} Si se considera una muestra de tamaño 4:
        \begin{enumerate}[label=\alph*)]
            \item Determinar para qué valores de $\theta_1$ la potencia del correspondiente test de Neyman-Pearson de tamaño $0.05$ es como mínimo $0.8$.
            \item Usando el test óptimo a nivel de significación $0.1$ para contrastar
            \begin{equation*}
                \left\{\begin{array}{l}
                    H_0: \theta = 3 \\
                    H_1 : \theta = 1.8
                \end{array}\right.
            \end{equation*}
            ¿qué decisión debe tomarse si el mayor de los valores observados es $4.2$?
        \end{enumerate}

        Obtenemos primero el test más potente, y luego lo usamos para resolver los problemas de aplicaciones. \\

        Tenemos un contraste de hipótesis simple frente a hipótesis simple, por lo que sabemos por el Lema de Neyman-Pearson que el 
        Test de Neyman-Pearson será el más potente de tamaño $\alpha$, de la forma
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \lambda(X_1, \ldots, X_n) > k \\
            \gamma &\text{si } \lambda(X_1, \ldots, X_n) = k \\
            0 &\text{si } \lambda(X_1, \ldots, X_n) < k
        \end{cases}$$
        para ciertos $\gamma \in [0,1]$, $k \in \R$ y 
        $$\lambda(X_1, \ldots, X_n) = \dfrac{f_1^n(X_1, \ldots, X_n)}{f_0^n(X_1, \ldots, X_n)}$$

        Definimos el espacio muestral y el espacio paramétrico, en ambos casos dependiente del parámetro
        $\theta$. En el caso en que se haga alusión a $\theta_0$, en todo momento sabemos que $\theta_0 = 3$, pero se harán las cuentas con $\theta_0$. 
        Si estamos en $H_0$, entonces $\X_0 = \left]1, \theta_0^2 + 1 \right]$, y $\Theta_0 = 
        \{\theta_0\}$. Análogamente, si estamos en $H_1$, entonces $\X_1 = \left]1, \theta_1^2 + 1 \right]$, y $\Theta_1 = 
        \{\theta_1\}$. Tenemos entonces que $$\Theta = \Theta_0 \cup \Theta_1 = \{\theta_0, \theta_1\}$$ y como
        $\theta_1 < \theta_0 \iff \theta_1^2 + 1 < \theta_0^2 + 1$, entonces $\left]1, \theta_1^2 + 1 \right] \subset \left]1, \theta_0^2 + 1 \right]$ y
        $$\X = \X_0 \cup \X_1 = \left]1, \theta_0^2 + 1 \right] \cup \left]1, \theta_1^2 + 1 \right] = \left]1, \theta_0^2 + 1 \right]$$
        Consecuentemente $$\X^n = \{(x_1, \ldots, x_n) \in \R^n : 1 < x_i \leq \theta_0^2 + 1 \quad \forall i = 1, \ldots, n\} = $$
        $$\{(x_1, \ldots, x_n) \in \R^n : 1 < x_{(1)} \quad \land \quad x_{(n)} \leq \theta_0^2 + 1\}$$

        Sea $(x_1, \ldots, x_n) \in \X^n$, y recuperamos la función conjunta $f_\theta^n(x_1, \ldots, x_n)$ del Ejercicio \ref{ej:2}, que es 
        $$f^n_\theta(x_1, \ldots, x_n) = (2\theta)^{-n} \prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}} I_{\R_0^{-}}(x_{(n)} - (\theta^2+1))$$ 
        Se tiene que
        $$f_0^n(x_1, \ldots, x_n) = (2\theta_0)^{-n} \prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}} I_{\R_0^{-}}(x_{(n)} - (\theta_0^2+1)) = (2\theta_0)^{-n} \prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}} \quad \forall (x_1, \ldots, x_n) \in \X^n$$
        $$f_1^n(x_1, \ldots, x_n) = (2\theta_1)^{-n} \prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}} I_{\R_0^{-}}(x_{(n)} - (\theta_1^2+1)) \quad \forall (x_1, \ldots, x_n) \in \X^n$$
        y podemos hallar $\lambda(x_1, \ldots, x_n)$, con $(x_1, \ldots, x_n) \in \X^n$: 
        $$\lambda(x_1, \ldots, x_n) = \dfrac{f_1^n(x_1, \ldots, x_n)}{f_0^n(x_1, \ldots, x_n)} = $$$$
        \dfrac{(2\theta_1)^{-n}}{(2\theta_0)^{-n}} \dfrac{\displaystyle\prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}} I_{\R_0^{-}}(x_{(n)} - (\theta_1^2+1))}
        {\displaystyle \prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}}} = 
        \dfrac{\begin{cases} (2\theta_1)^{-n} \displaystyle\prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}} &\text{si } x_{(n)} \leq \theta_1^2 + 1 \\ 0 &\text{si } x_{(n)} > \theta_1^2 + 1 \end{cases}}
        {(2\theta_0)^{-n} \displaystyle\prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}}}$$

        Ahora, si $x_{(n)} \leq \theta_1^2 + 1$, simplificamos

        $$\dfrac{(2\theta_1)^{-n}}{(2\theta_0)^{-n}} \dfrac{\cancel{\displaystyle\prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}}}}
        {\cancel{\displaystyle \prod_{i=1}^{n} \frac{1}{\sqrt{x_i-1}}}} = \left(\dfrac{2 \theta_0}{2 \theta_1} \right)^{n} = \left(\dfrac{\theta_0}{\theta_1} \right)^{n}$$

        de donde
        $$\lambda(x_1, \ldots, x_n) = \dfrac{f_1^n(x_1, \ldots, x_n)}{f_0^n(x_1, \ldots, x_n)} = 
        \begin{cases} 
            \left( \dfrac{\theta_0}{\theta_1} \right)^{n} &\text{si } x_{(n)} \leq \theta_1^2 + 1 \\ 
            0 &\text{si } x_{(n)} > \theta_1^2 + 1 
        \end{cases} \quad \forall (x_1, \ldots, x_n) \in \X^n$$

        Gráficamente, como $\theta_1 < \theta_0$:

        \begin{center}
            \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

            \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
                %uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300

                %Straight Lines [id:da9701795441758359] 
                \draw    (70,101) -- (301,102) ;
                %Straight Lines [id:da9876084884828463] 
                \draw    (141,89) -- (141,112) ;
                %Straight Lines [id:da963921157698696] 
                \draw    (210,89) -- (210,112) ;
                %Straight Lines [id:da07701744813997313] 
                \draw    (140.5,41) -- (297.67,41.33) ;
                \draw [shift={(299.67,41.33)}, rotate = 180.12] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da3774624119169432] 
                \draw    (140.5,41) -- (140.5,21.67) ;
                %Straight Lines [id:da9681405401847125] 
                \draw    (140.5,71.5) -- (140.5,51.17) ;
                %Straight Lines [id:da1310029333570385] 
                \draw    (140.5,70.5) -- (72.5,71.23) ;
                \draw [shift={(70.5,71.25)}, rotate = 359.39] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da899862814242277] 
                \draw    (210.5,171.5) -- (210.5,151.17) ;
                %Straight Lines [id:da009971342095128954] 
                \draw    (210.5,170.5) -- (73.5,170.5) ;
                \draw [shift={(71.5,170.5)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da4678439533610015] 
                \draw    (210.5,211.5) -- (210.5,191.17) ;
                %Straight Lines [id:da8507207636884839] 
                \draw    (210.5,210.5) -- (297.5,211.48) ;
                \draw [shift={(299.5,211.5)}, rotate = 180.64] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da9954362463616986] 
                \draw  [dash pattern={on 0.84pt off 2.51pt}]  (140.5,8.5) -- (140.5,211.5) ;
                %Straight Lines [id:da20565414403393933] 
                \draw  [dash pattern={on 0.84pt off 2.51pt}]  (210,8.5) -- (210,211.5) ;

                % Text Node
                \draw (333,59) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \theta _{1} \ < \ \theta _{0}$};
                % Text Node
                \draw (92,103.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \theta^2_{1} +1$};
                % Text Node
                \draw (215,104) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \theta^2_{0} +1$};
                % Text Node
                \draw (307,92) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle X_{( n)}$};
                % Text Node
                \draw (225,16) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{1} =0$};
                % Text Node
                \draw (83.33,47) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{1}  >0$};
                % Text Node
                \draw (153.33,147) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{0}  >0$};
                % Text Node
                \draw (223.33,188) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{0} =0$};
            \end{tikzpicture}
        \end{center}

        La semirrecta en que $f_0 = 0$, que se corresponde con la condición $x_{(n)} > \theta_0^2 + 1$, no nos interesa 
        ya que $(x_1, \ldots, x_n) \notin \X^n$. Gráficamente, la situación en la que estamos es la siguiente:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}
                % Ejes
                \draw[-Stealth] (-0.2,0) -- (6,0) node[right] {$X_{(n)}$};
                \draw[-Stealth] (0,-0.2) -- (0,3.5) node[left] {$\lm$};

                % Recta con altura 1
                \draw[thick] (0,2.65) -- (6,2.65);

                % Proyección sobre la recta
                \coordinate (P) at (1.5,1.65); % Ajustado para caer sobre la recta dibujada
                \draw[thick, blue] (P) -- (0,1.65);

                \draw[dashed] (1.5,0) -- (1.5,1.65);
                \coordinate (Q) at (1.5,0); % Ajustado para caer sobre la recta dibujada
                \draw (1.5,0) node[below] {$\theta_1^2 + 1$};
                \draw (3.5,0) node[below] {$\theta_0^2 + 1$};
                \coordinate (R) at (3.5,0); % Ajustado para caer sobre la recta dibujada

                \draw[thick, blue] (Q) -- (3.5,0);

                % Etiqueta k en el eje y
                \draw (0,1.65) node[left] {$\left( \dfrac{\theta_0}{\theta_1} \right)^{n}$};
                \draw (0,2.65) node[left] {$1$};

                % Punto en la recta
                \fill (P) circle (2pt);
                % Punto en la recta
                \draw (Q) circle (2pt);

                \fill (R) circle (2pt);
            \end{tikzpicture}
        \end{figure}

        Tenemos que $k \in \left\{0, \left( \dfrac{\theta_0}{\theta_1} \right)^{n} \right\}$, luego distinguimos entre estos dos casos. \\

        Si \boxed{k=0}, el test será:
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \lambda(X_1, \ldots, X_n) > k \iff X_{(n)} \leq \theta_1^2 + 1 \\
            \gamma &\text{si } \lambda(X_1, \ldots, X_n) = k \iff X_{(n)} > \theta_1^2 + 1 \\
            0 &\text{si } \lambda(X_1, \ldots, X_n) < k \text{ nunca}
        \end{cases}$$

        es decir:

        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } X_{(n)} \leq \theta_1^2 + 1 \\
            \gamma &\text{si } X_{(n)} > \theta_1^2 + 1
        \end{cases}$$

        Determinamos $\gamma$ imponiendo tamaño $\alpha$:

        $$\alpha \stackrel{def} = \sup_{\theta \in \Theta_0} \beta_{\varphi}(\theta) =  \sup_{\theta \in \Theta_0} E_\theta[\varphi(X_1, \ldots, X_n)] = E_{\theta_0}[\varphi(X_1, \ldots, X_n)] = $$
        $$1 \cdot P_{\theta_0}[X_{(n)} \leq \theta_1^2 + 1] + \gamma P_{\theta_0}[X_{(n)} > \theta_1^2 + 1] $$

        Para calcular las probabilidades, recuperamos también la función de distribución $F_X(t)$, obtenida previamente en el Ejercicio \ref{ej:2}:
        $$F_X(t) = \dfrac{\sqrt{t-1}}{\theta} \quad 1 < t \leq \theta^2+1$$

        Sabemos por teoría que, para $T = X_{(n)}$, la distribución del máximo verifica (visto también en el Ejercicio \ref{ej:2}): 
        $$P_{\theta}[T \leq t] = F_T(t) = (F_X(t))^n = \left( \dfrac{\sqrt{t-1}}{\theta} \right)^{n}$$

        Por lo tanto, $$P_{\theta_0}[T \leq \theta_1^2 + 1] = \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$$

        y

        $$P_{\theta_0}[T > \theta_1^2 + 1] = 1 - P_{\theta_0}[T \leq \theta_1^2 + 1] = 1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$$

        de donde
        $$\alpha = 1 \cdot P_{\theta_0}[X_{(n)} \leq \theta_1^2 + 1] + 
        \gamma P_{\theta_0}[X_{(n)} > \theta_1^2 + 1] = \left( \dfrac{\theta_1}{\theta_0} \right)^{n} + \gamma \left( 1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n} \right) \iff $$
        $$0 \leq \gamma = \dfrac{\alpha - \left( \dfrac{\theta_1}{\theta_0} \right)^{n}}{1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n}} \leq 1 \iff \alpha \geq \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$$

        el test resultante es:

        $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                1 &\text{si } X_{(n)} \leq \theta_1^2 + 1 \\
                \dfrac{\alpha - \left( \dfrac{\theta_1}{\theta_0} \right)^{n}}{1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n}} &\text{si } X_{(n)} > \theta_1^2 + 1
            \end{cases}$$   
            
        y sustituyendo $\theta_0 = 3$:

        $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                1 &\text{si } X_{(n)} \leq \theta_1^2 + 1 \\
                \dfrac{\alpha - \left( \dfrac{\theta_1}{3} \right)^{n}}{1 - \left( \dfrac{\theta_1}{3} \right)^{n}} &\text{si } X_{(n)} > \theta_1^2 + 1
            \end{cases}$$
            
        con potencia (en $\Theta_1 = \{\theta_1\}$):
        $$\beta_{\varphi}(\theta_1) = E_{\theta_1}[\varphi(X_1, \ldots, X_n)] = P_{\theta_1}[T \leq \theta_1^2 + 1] + 
        \dfrac{\alpha - \left( \dfrac{\theta_1}{3} \right)^{n}}{1 - \left( \dfrac{\theta_1}{3} \right)^{n}} P_{\theta_1}[T > \theta_1^2 + 1]$$

        Como $P_{\theta_1}[T \leq \theta_1^2 + 1] = 1$ y $P_{\theta_1}[T > \theta_1^2 + 1] = 1 - P_{\theta_1}[T \leq \theta_1 + 1] = 1 - 1 = 0$, se sigue que

        $$\beta_{\varphi}(\theta_1) = 1$$

        \newpage

        Si \boxed{k = \left(\dfrac{\theta_0}{\theta_1} \right)^{n}}, el test será, mirando nuevamente la misma gráfica

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}
                % Ejes
                \draw[-Stealth] (-0.2,0) -- (6,0) node[right] {$X_{(n)}$};
                \draw[-Stealth] (0,-0.2) -- (0,3.5) node[left] {$\lm$};

                % Recta con altura 1
                \draw[thick] (0,2.65) -- (6,2.65);

                % Proyección sobre la recta
                \coordinate (P) at (1.5,1.65); % Ajustado para caer sobre la recta dibujada
                \draw[thick, blue] (P) -- (0,1.65);

                \draw[dashed] (1.5,0) -- (1.5,1.65);
                \coordinate (Q) at (1.5,0); % Ajustado para caer sobre la recta dibujada
                \draw (1.5,0) node[below] {$\theta_1^2 + 1$};
                \draw (3.5,0) node[below] {$\theta_0^2 + 1$};
                \coordinate (R) at (3.5,0); % Ajustado para caer sobre la recta dibujada

                \draw[thick, blue] (Q) -- (3.5,0);

                % Etiqueta k en el eje y
                \draw (0,1.65) node[left] {$\left( \dfrac{\theta_0}{\theta_1} \right)^{n}$};
                \draw (0,2.65) node[left] {$1$};

                % Punto en la recta
                \fill (P) circle (2pt);
                % Punto en la recta
                \draw (Q) circle (2pt);

                \fill (R) circle (2pt);
            \end{tikzpicture}
        \end{figure}

        el que sigue:

        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \lambda(X_1, \ldots, X_n) > k \text{ nunca} \\
            \gamma &\text{si } \lambda(X_1, \ldots, X_n) = k \iff X_{(n)} \leq \theta_1^2 + 1 \\
            0 &\text{si } \lambda(X_1, \ldots, X_n) < k \iff X_{(n)} > \theta_1^2 + 1
        \end{cases}$$

        es decir:
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            \gamma &\text{si } X_{(n)} \leq \theta_1^2 + 1 \\
            0 &\text{si } X_{(n)} > \theta_1^2 + 1
        \end{cases}$$

        Determinamos $\gamma$ igual que antes imponiendo tamaño $\alpha$:

        $$\alpha \stackrel{def} = \sup_{\theta \in \Theta_0} \beta_{\varphi}(\theta) =  \sup_{\theta \in \Theta_0} E_\theta[\varphi(X_1, \ldots, X_n)] = E_{\theta_0}[\varphi(X_1, \ldots, X_n)] = $$
        $$\gamma \cdot P_{\theta_0}[X_{(n)} \leq \theta_1^2 + 1] + 0 \cdot P_{\theta_0}[X_{(n)} > \theta_1^2 + 1] = \gamma \cdot P_{\theta_0}[X_{(n)} \leq \theta_1^2 + 1] $$

        Ya sabemos del caso anterior que 
        $$P_{\theta_0}[T \leq \theta_1^2 + 1] = \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$$

        luego
        $$\alpha = \gamma \cdot P_{\theta_0}[X_{(n)} \leq \theta_1^2 + 1] = \gamma \left( \dfrac{\theta_1}{\theta_0} \right)^{n} \iff $$
        $$0 \leq \gamma = \dfrac{\alpha}{\left( \dfrac{\theta_1}{\theta_0} \right)^{n}} = \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{n}  \leq 1 \iff \alpha \leq \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$$

        por lo que el test resultante es:

        $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{n} &\text{si } X_{(n)} \leq \theta_1^2 + 1 \\
                0 &\text{si } X_{(n)} > \theta_1^2 + 1
            \end{cases}$$      

        y para $\theta_0 = 3$:

        $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                \alpha \left( \dfrac{3}{\theta_1} \right)^{n} &\text{si } X_{(n)} \leq \theta_1^2 + 1 \\
                0 &\text{si } X_{(n)} > \theta_1^2 + 1
            \end{cases}$$ 
            
        con potencia (en $\Theta_1 = \{\theta_1\}$):
        $$\beta_{\varphi}(\theta_1) = E_{\theta_1}[\varphi(X_1, \ldots, X_n)] = \alpha \left( \dfrac{3}{\theta_1} \right)^{n} \cdot P_{\theta_1}[T \leq \theta_1^2 + 1] + 
        0 \cdot P_{\theta_1}[T > \theta_1^2 + 1] = $$
        $$\alpha \left( \dfrac{3}{\theta_1} \right)^{n} \cdot P_{\theta_1}[T \leq \theta_1^2 + 1]$$

        Igual que en el caso anterior $P_{\theta_1}[T \leq \theta_1^2 + 1] = 1$, luego

        $$\beta_{\varphi}(\theta_1) = \alpha \left( \dfrac{3}{\theta_1} \right)^{n}$$

        \textit{Aplicaciones:} Si se considera una muestra de tamaño 4:
        \begin{enumerate}[label=\alph*)]
            \item Determinar para qué valores de $\theta_1$ la potencia del correspondiente test de Neyman-Pearson de tamaño $0.05$ es como mínimo $0.8$. \\
            
            Nos dicen que $n=4$, $\alpha = 0.05$ y que hay que imponer que $\beta_{\varphi}(\theta_1) \geq 0.8$. Distinguimos nuevamente
            entre los dos posibles valores de $k$: \\

            Si \boxed{k=0}, hemos visto que el test más potente para tamaños $\alpha \geq \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$ es

            $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                1 &\text{si } X_{(n)} \leq \theta_1^2 + 1 \\
                \dfrac{\alpha - \left( \dfrac{\theta_1}{3} \right)^{n}}{1 - \left( \dfrac{\theta_1}{3} \right)^{n}} &\text{si } X_{(n)} > \theta_1^2 + 1
            \end{cases}$$  

            y la potencia es $$\beta_{\varphi}(\theta_1) = 1$$

            En particular si $n=4$ y $\alpha = 0.05$, imponemos
            $$0.05 = \alpha \geq \left( \dfrac{\theta_1}{3} \right)^{4} = \dfrac{\theta_1^4}{81} \iff \theta_1^4 \leq 0.05 \cdot 81 = 4,05 \iff
            \theta_1 \leq \sqrt[4]{4,05} \approx 1,4186$$

            Por tanto, como la potencia siempre es $1$ ($\geq 0,8$), el rango de valores sería simplemente aquel intervalo que verifica
            la restricción del tamaño, que es $$\left]-\infty, \sqrt[4]{4,05} \right]$$ 

            \newpage

            Si \boxed{k=\left(\dfrac{\theta_0}{\theta_1} \right)^{n}}, hemos visto que el test más potente para tamaños $\alpha \leq \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$ es

            $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                \alpha \left( \dfrac{3}{\theta_1} \right)^{n} &\text{si } X_{(n)} \leq \theta_1^2 + 1 \\
                0 &\text{si } X_{(n)} > \theta_1^2 + 1
            \end{cases}$$

            y la potencia es $$\beta_{\varphi}(\theta_1) = \alpha \left( \dfrac{3}{\theta_1} \right)^{n}$$

            En particular si $n=4$ y $\alpha = 0.05$, imponemos
            $$0.05 = \alpha \leq \left( \dfrac{\theta_1}{3} \right)^{4} = \dfrac{\theta_1^4}{81} \iff \theta_1^4 \geq 0.05 \cdot 81 = 4,05 \iff
            \theta_1 \geq \sqrt[4]{4,05} \approx 1,4186$$

            e imponemos la restricción sobre la potencia 
            $$\beta_{\varphi}(\theta_1) \geq 0,8 \iff \dfrac{4,05}{\theta_1^4} = 0,05 \left( \dfrac{3}{\theta_1} \right)^{4}  = \alpha \left( \dfrac{3}{\theta_1} \right)^{n} \geq 0,8 \iff $$
            $$\theta_1 \leq \sqrt[4]{\dfrac{4,05}{0,8}} = 1,5$$

            Combinando ambas condiciones, obtenemos
            $$1,4186 \leq \theta_1 \leq 1,5$$
            Y concluimos entonces que el rango de valores será $\left[\sqrt[4]{4,05}, 1{.}5\right]$.

            \item Usando el test óptimo a nivel de significación $0.1$ para contrastar
            \begin{equation*}
                \left\{\begin{array}{l}
                    H_0: \theta = 3 \\
                    H_1 : \theta = 1.8
                \end{array}\right.
            \end{equation*}
            ¿qué decisión debe tomarse si el mayor de los valores observados es $4.2$? \\

            Como $\alpha = 0.1$, $\theta_0 = 3$ y $\theta_1 = 1.8$, vemos que 
            $$\dfrac{\theta_1}{\theta_0} = \dfrac{1.8}{3} = 0.6$$
            y ahora determinamos $n$ para ver cuál de los dos posibles tests es el óptimo:
            $$\alpha \geq \left( \dfrac{\theta_1}{\theta_0} \right)^{n} \iff 
            0.1 \geq \left( 0.6 \right)^{n} \iff -2,3026 \approx \ln(0,1) \geq n \ln(0.6) \approx -0,5108 n \iff$$
            $$0,5108 n \geq 2,3026 \iff n \geq \dfrac{2,3026}{0,5108} \approx 4,5078 \iff$$
            $$n \in \N \setminus \{1,2,3,4\}$$
            Si estamos en el otro caso:
            $$\alpha \leq \left( \dfrac{\theta_1}{\theta_0} \right)^{n} \iff 
            0.1 \leq \left( 0.6 \right)^{n} \iff -2,3026 \approx \ln(0,1) \leq n \ln(0.6) \approx -0,5108 n \iff$$
            $$0,5108 n \leq 2,3026 \iff n \leq \dfrac{2,3026}{0,5108} \approx 4,5078 \iff$$
            $$n \in \{1,2,3,4\}$$ 
            Distinguimos en función de estos dos casos:
            \begin{enumerate}
                \item Si $n \in \{1,2,3,4\}$, entonces $\alpha \leq \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$, y el
                test óptimo es:
                $$\varphi(X_1, \ldots, X_n) = 
                \begin{cases}
                    \alpha \left( \dfrac{3}{\theta_1} \right)^{n} &\text{si } X_{(n)} \leq \theta_1^2 + 1 \\
                    0 &\text{si } X_{(n)} > \theta_1^2 + 1
                \end{cases}$$
                En particular, para $\theta_1 = 1.8$ y $X_{(n)} = 4,2$, tenemos que 
                $$4,2 = X_{(n)} \leq \theta_1^2 + 1 = 1.8^2 + 1 = 4,24$$
                por lo tanto, la decisión tomada es rechazar la hipótesis nula $H_0$ con probabilidad 
                $$\alpha \left( \dfrac{3}{\theta_1} \right)^{n} = 
                0.1 \left( \dfrac{3}{\theta_1} \right)^{n} = 0.1 \left( \dfrac{5}{3} \right)^n = \begin{cases}
                    0.1 \left( \dfrac{5}{3} \right)^1 \approx 0,1667 &\quad n=1 \\\\
                    0.1 \left( \dfrac{5}{3} \right)^2 \approx 0,2778 &\quad n=2 \\\\
                    0.1 \left( \dfrac{5}{3} \right)^3 \approx 0,463  &\quad n=3 \\\\
                    0.1 \left( \dfrac{5}{3} \right)^4 \approx 0,7716 &\quad n=4
                \end{cases}$$
                o sea, asumir que $\theta = \theta_1 = 1.8$ con la respectiva probabilidad, según $n \in \{1,2,3,4\}$.
                \item Si $n \in \N \setminus \{1,2,3,4\}$, entonces $\alpha \geq \left( \dfrac{\theta_1}{\theta_0} \right)^{n}$, y el
                test óptimo es:
                $$\varphi(X_1, \ldots, X_n) = 
                \begin{cases}
                    1 &\text{si } X_{(n)} \leq \theta_1^2 + 1 \\
                    \dfrac{\alpha - \left( \dfrac{\theta_1}{3} \right)^{n}}{1 - \left( \dfrac{\theta_1}{3} \right)^{n}} &\text{si } X_{(n)} > \theta_1^2 + 1
                \end{cases}$$
                Nuevamente, para $\theta_1 = 1.8$ y $X_{(n)} = 4,2$, tenemos que 
                $$4,2 = X_{(n)} \leq \theta_1^2 + 1 = 1.8^2 + 1 = 4,24$$
                por lo tanto, la decisión tomada es rechazar la hipótesis nula $H_0$ con probabilidad $1$,
                o sea, asumir que $\theta = \theta_1 = 1.8$ con probabilidad $1$.
            \end{enumerate}
        \end{enumerate}
    \end{ejercicio}
    
    \newpage

    \begin{ejercicio}
        \ \\
        \begin{enumerate}[label=\alph*)]
            \item Sea $(X,Y)$ un vector aleatorio bidimensional.
                \begin{enumerate}[label=\alph*)]
                    \item ¿En qué consiste el problema de regresión lineal de $Y$ sobre $X$? 
                    Especificar las hipótesis iniciales para su resolución. \\

                    El problema de regresión lineal de $Y$ sobre $X$ consiste en aproximar
                    la variable $Y$ en términos de $X$ con la mayor precisión posible
                    mediante una función lineal, minimizando el error en el sentido
                    de mínimos cuadrados. En el caso simple (una sola variable explicativa),
                    se asume el siguiente modelo lineal:
                    $$Y = \beta_0 + \beta_1 X + E$$
                    donde $E$ es una variable aleatoria que expresa el error cometido
                    al predecir $Y$ por $\beta_0 + \beta_1 X$. El objetivo es 
                    estimar el modelo a partir de observaciones $(x_1, y_1), \ldots,
                    (x_n, y_n)$, es decir, estimar $\beta_0, \beta_1$ y $\sigma^2$. \\

                    Las hipótesis iniciales para su resolución son las siguientes:
                    \begin{enumerate}
                        \item Se supone que $\E[Y^2] < +\infty$.
                        \item Para $X = x$:
                        \begin{itemize}
                            \item $\E[Y / X = x] = \beta_0 + \beta_1 x$
                            \item $\V[Y / X = x] = \sigma^2$
                        \end{itemize}
                    \end{enumerate}
                    y se fijan $x_1, \ldots, x_n$, imponiendo que $\exists 1 \leq i\neq j \leq n$
                    tal que $x_i \neq x_j$. \\

                    \item Detallar el modelo que debe usarse para resolver el problema anterior, 
                    especificando qué es cada componente y qué describe cada una de las variables 
                    aleatorias que aparecen. Demostrar que es un modelo de Gauss-Markov, imponiendo, 
                    si es necesario, hipótesis adicionales sobre las variables observadas. \\

                    Con $x_1, \ldots, x_n$ fijados e $Y_i$ la observación aleatoria de $Y$ cuando
                    $X = x_i$, se tiene que:
                    $$\E[Y_i] = \beta_0 + \beta_1 x_i, \quad \V[Y_i] = \sigma^2$$
                    y por tanto
                    $$Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \quad E[\varepsilon_i] = 0,
                    \quad \V[\varepsilon_i] = \sigma^2, \quad i = 1, \ldots, n$$
                    Matricialmente:
                    $$\begin{pmatrix}
                        Y_1 \\
                        \vdots \\
                        Y_n
                    \end{pmatrix} = 
                    \begin{pmatrix}
                        1 & x_1 \\
                        \vdots & \vdots \\
                        1 & x_n
                    \end{pmatrix} 
                    \begin{pmatrix}
                        \beta_0 \\
                        \beta_1
                    \end{pmatrix}
                    +
                    \begin{pmatrix}
                        \varepsilon_1 \\
                        \vdots \\
                        \varepsilon_n
                    \end{pmatrix}$$
                    donde
                    \begin{itemize}
                        \item $Y = (Y_1, \ldots, Y_n)^T$ es el vector observable.
                        \item $X$ es la matriz de diseño, con $\rg(X) = 2$ por la imposición
                        sobre la existencia de, al menos, dos valores distintos $x_i \neq x_j$.
                        \item $\beta = (\beta_0, \beta_1)^T$ es el vector de efectos (fijos).
                        \item $\varepsilon = (\varepsilon_1, \ldots, \varepsilon_n)^T$ es el vector
                        de errores (no observable).
                    \end{itemize} 
                    Si suponemos también que las observaciones son independientes, entonces
                    $\varepsilon_1, \ldots, \varepsilon_n$ son independientes, centradas
                    y homocedásticas, luego el modelo cumple las hipótesis de Gauss-Markov 
                    (modelo lineal de efectos fijos y con las componentes de $\varepsilon$,
                    $\varepsilon_1, \ldots, \varepsilon_n$, variables aleatorias de segundo orden 
                    centradas, homocedásticas e incorreladas), y además es de rango máximo.
                \end{enumerate}
            \item Se desea comparar el volumen de ventas de cierto artículo en tres ciudades diferentes. 
            Para ello se toma una muestra de establecimientos en cada ciudad y se mide el volumen de ventas, 
            en miles de artículos durante los últimos diez meses, obteniéndose:
                \begin{table}[H]
                \centering
                \begin{tabular}{c|cccccc}
                    Ciudad 1 & 30 & 20 & 27.5 & 32.5 & &\\
                    \hline
                    Ciudad 2 & 20 & 32.5 & 37.5 & 30 & 35 & 25 \\
                    \hline
                    Ciudad 3 & 15 & 15 & 25 & 20 & 25 & 
                \end{tabular}
                \end{table}
                ¿Proporcionan estos datos evidencia para rechazar que las ventas se distribuyen de igual 
                forma en las tres ciudades, al nivel de significación $0.025$? 
                Especificar las hipótesis sobre la distribución de las variables aleatorias 
                involucradas en este problema, así como las hipótesis concretas que se están contrastando. \\

                Tenemos un problema de ANOVA (Análisis de la Varianza de una vía). Estamos considerando
                el modelo $Y = X \mu + \varepsilon$, que es un modelo de Gauss-Markov de rango máximo
                $k$. Definimos para $i=1,2,3$ y $j = 1, \ldots, n_i$:
                $$Y_{ij} \equiv \text{``Ventas del establecimiento $j$ en la ciudad $i$''}$$
                y suponemos que $Y_{ij} = \mu_i + \varepsilon_{ij}$, con $\varepsilon_{ij} \rightsquigarrow 
                \cc{N}(0, \sigma^2)$, o equivalentemente $Y_{ij} \rightsquigarrow \cc{N}(\mu_i, \sigma^2)$
                independientes. El contraste a resolver es
                $$\begin{cases}
                    H_0 : \mu_1 = \mu_2 = \mu_3 \\
                    H_1 : \text{alguna diferente}
                \end{cases}$$
                luego $k=3$ y el estadístico de contraste es
                $$\varphi(Y) = \begin{cases}
                    1 &\text{si } F(Y) > F_{k-1, n-k; \alpha} \\
                    0 &\text{si } F(Y) \leq F_{k-1, n-k; \alpha}
                \end{cases}$$
                y $$F(Y) = \dfrac{S_E^2}{S_R^2}$$
                con 
                $$S_E^2 = \dfrac{1}{k-1} \sum_{i=1}^{k} n_i (\overline{Y_i} - \overline{Y})^2$$
                $$S_R^2 = \dfrac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (Y_{ij} - \overline{Y_i})^2$$
                Obtenemos $n_i : i =1,2,3$:
                $$n_1 = 4, \quad n_2 = 6, \quad n_3 = 5$$
                de donde $$n = \sum_{i=1}^{k} n_i = 4+6+5 = 15$$
                Obtenemos $\overline{Y}_i : i =1,2,3$:
                $$\overline{Y}_1 = \dfrac{1}{n_1} \sum_{j=1}^{n_1} Y_{1j} = \dfrac{1}{4} (30+20+27.5+32.5) = 27.5$$
                $$\overline{Y}_2 = \dfrac{1}{n_2} \sum_{j=1}^{n_2} Y_{2j} = \dfrac{1}{6} 
                (20+32.5+37.5+30+35+25) = \dfrac{145}{6}$$
                $$\overline{Y}_3 = \dfrac{1}{n_3} \sum_{j=1}^{n_3} Y_{3j} = \dfrac{1}{5} 
                (15+15+25+20+25) = 20$$
                y calculamos $\overline{Y}$:
                $$\overline{Y} = \dfrac{1}{n} \sum_{i=1}^{k} \sum_{j=1}^{n_i} Y_{ij} = 
                \dfrac{1}{n} \sum_{i=1}^{3} n_i \overline{Y_i} = \dfrac{1}{15} (4 \cdot 27,5 + 
                6 \cdot \dfrac{145}{6} + 5 \cdot 20) = \dfrac{71}{3}$$
                Podemos determinar $S_E^2$:
                $$S_E^2 = \dfrac{1}{k-1} \sum_{i=1}^{k} n_i (\overline{Y_i} - \overline{Y})^2 = 
                \dfrac{1}{2} \left(4 \cdot \left(27,5 - \dfrac{71}{3} \right)^2 + 
                6 \cdot \left(\dfrac{145}{6} - \dfrac{71}{3} \right)^2 + 
                5 \cdot \left( 20 - \dfrac{71}{3} \right)^2 \right) = $$
                $$\dfrac{255}{4} = 63.75$$
                y ahora hallar $S_R^2$:
                $$S_R^2 = \dfrac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (Y_{ij} - \overline{Y_i})^2 = 
                \dfrac{1}{12} ((Y_{11} - \overline{Y}_1)^2 + (Y_{12} - \overline{Y}_1)^2
                + (Y_{13} - \overline{Y}_1)^2 + (Y_{14} - \overline{Y}_1)^2 + $$
                $$(Y_{21} - \overline{Y}_2)^2 + (Y_{22} - \overline{Y}_2)^2
                + (Y_{23} - \overline{Y}_2)^2 + (Y_{24} - \overline{Y}_2)^2 + 
                (Y_{25} - \overline{Y}_2)^2 + (Y_{26} - \overline{Y}_2)^2 +$$
                $$(Y_{31} - \overline{Y}_3)^2 + (Y_{32} - \overline{Y}_3)^2
                + (Y_{33} - \overline{Y}_3)^2 + (Y_{34} - \overline{Y}_3)^2 + 
                (Y_{35} - \overline{Y}_3)^2) = \dfrac{175}{24} + \dfrac{625}{18} + \dfrac{25}{3} = $$
                $$\dfrac{3625}{72} \approx 50.3472$$
                El valor de $F(Y)$ es:
                $$F(Y) = \dfrac{S_E^2}{S_R^2} = \dfrac{\dfrac{255}{4}}{\dfrac{3625}{72}} = 
                \dfrac{918}{725} \approx 1.2662$$

                Ahora, como $\alpha = 0.025$, y recordando que $F_{k-1, n-k; \alpha} = 
                F_{2, 12; \alpha}$ es aquel valor tal que $P[F_{k-1, n-k} > F_{k-1, n-k; \alpha}] = \alpha$,
                como la tabla de la $F$ de Snedecor nos da probabilidades acumuladas, obtenemos
                equivalentemente:
                $$P[F_{k-1, n-k} > F_{k-1, n-k; \alpha}] = \alpha \iff 
                1 - P[F_{k-1, n-k} \leq F_{k-1, n-k; \alpha}] = \alpha \iff$$
                $$P[F_{k-1, n-k} \leq F_{k-1, n-k; \alpha}] = 1 - \alpha \iff
                P[F_{2, 12} \leq F_{2, 12; 0.025}] = 1 - 0.025 = 0.975$$

                y mirando la tabla, vemos que $F_{2, 12; 0.025} = 5.10$, luego 
                $$1.2662 \approx F(Y) \leq F_{2, 12; 0.025} = 5.10$$
                y concluimos entonces que no se rechaza $H_0$ al nivel de significación $\alpha = 0.025$, 
                es decir, estos datos \textbf{no} proporcionan evidencia para rechazar que las ventas se 
                distribuyen de igual forma en las tres ciudades, al nivel de significación $0.025$.
        \end{enumerate}
    \end{ejercicio}


\end{document}
