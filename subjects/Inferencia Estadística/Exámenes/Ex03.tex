\documentclass[12pt]{article}

\input{../../_assets/preambulo.tex}

\newcommand{\E}{\mathrm{E}}
\newcommand{\V}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rmas}{\R^{+}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\I}{\mathrm{Im}}
\newcommand{\Rmenos}{\R^{-}}
\newcommand{\Exp}{\text{Exp}}
\newcommand{\Er}{\mathcal{E}}
\newcommand{\Z}{\mathbb{Z}}

\begin{document}

    % 1. Foto de fondo
    % 2. Título
    % 3. Encabezado Izquierdo
    % 4. Color de fondo
    % 5. Coord x del titulo
    % 6. Coord y del titulo
    % 7. Fecha

    
    \input{../../_assets/portada}
    \portadaExamen{ffccA4.jpg}{Inferencia \\ Estadística\\Examen III}{Inferencia Estadística. Examen III}{MidnightBlue}{-8}{28}{2026}{José Manuel Sánchez Varbas}

    \begin{description}
        \item[Asignatura] Inferencia Estadística.
        \item[Curso Académico] 2022-23.
        \item[Grado] Grado en Matemáticas y Grado en Ingeniería Informática y Matemáticas.
        \item[Descripción] Examen Ordinario.
        \item[Fecha] 17 de Enero de 2023.  
    \end{description}
    \newpage


    % ------------------------------------

    \begin{ejercicio}[2,25 puntos]
        Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con
        distribución en una familia paramétrica.

        \begin{enumerate}
            \item[a)] Dar la definición de estadístico suficiente. Enunciar el Teorema de
            Factorización de Neyman--Fisher. Demostrar dicho teorema para variables discretas.

            \item[b)] Si la función de distribución de $X$ es $$F_\theta(x)=1-e^{-\theta x}, \quad x>0$$
            encontrar el intervalo de confianza para $\theta$ de menor longitud media
            uniformemente a nivel de confianza $1-\alpha$, basado en un estadístico suficiente.
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}[2,25 puntos]
        Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable aleatoria con función
        de densidad $$f_\theta(x)=\dfrac{1}{2\sqrt{2\theta-1}\sqrt{x-1}}, \quad 1<x<2\theta$$

        \begin{enumerate}
            \item[a)] Sabiendo que $T=\max X_i$ es suficiente, encontrar, si existe, el UMVUE para
            $(2\theta-1)^{-1}$, especificando previamente el espacio paramétrico y el espacio
            muestral. Justificar la no existencia del UMVUE en los casos que corresponda.

            \item[b)] Calcular la función de verosimilitud y encontrar un estimador máximo
            verosímil de $2\theta-1$.
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}[2 puntos]
        Sea $X$ una variable aleatoria con distribución en una familia regular en el sentido
        de Fréchet--Cramér--Rao, cuyas funciones de densidad son de la forma:
        $$f_\theta(x)=\exp \left\{T(x)\ln\theta-\dfrac{\theta^2}{2}+S(x)\right\} \quad x\in\R, \quad \theta\in\Rmas$$
        siendo $T(X)$ un estadístico regular.

        \begin{enumerate}
        \item[a)] Calcular la función de información asociada a $X$.

        \item[b)] Basándose en una muestra aleatoria simple de $X$, $(X_1,\dots,X_n)$, y
        suponiendo $T(X)>0$, encontrar la clase de funciones paramétricas que admiten
        estimador eficiente y los estimadores correspondientes.

        \item[c)] Bajo los supuestos del apartado b), calcular la cota inferior para la varianza
        de estimadores insesgados de $\ln\theta$, regulares, y justificar si se alcanza o no
        dicha cota.
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[2,4 puntos]
        Contraste de hipótesis:

        \begin{enumerate}
            \item[a)] Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable aleatoria $X$
            con distribución en una familia $\{P_\theta \mid \theta \in \Theta\}$. Sea $\Theta_0$
            subconjunto arbitrario de $\Theta$ y supongamos que se pretende contrastar la hipótesis
            $$
            H_0:\ \theta \in \Theta_0.
            $$

            \begin{enumerate}
                \item[a1)] Detallar la hipótesis alternativa. Definir formalmente el concepto de test de
                hipótesis y dar la interpretación de sus valores.

                \item[a2)] Definir el tamaño y la función de potencia de un test arbitrario para resolver el
                problema anterior, explicando el significado de estos conceptos en términos del rechazo
                de $H_0$.

                \item[a3)] En términos del tamaño y de la función potencia, ¿qué significa que un test tiene
                nivel de significación $\alpha$ para el problema de contraste planteado? ¿Cuáles son las
                condiciones para que un test sea UMP a nivel de significación $\alpha$?
            \end{enumerate}

            \item[b)] Obtener un test de razón de verosimilitud de tamaño $\alpha$ para contrastar
            $$H_0: \theta \le \theta_0 \qquad \text{frente a} \qquad H_1: \theta > \theta_0,$$
            basado en una observación de una variable aleatoria con la siguiente función de densidad
            (detallar y justificar todos los pasos para la obtención, incluyendo el estudio detallado
            del estadístico de contraste y su representación gráfica):
            $$
            f_\theta(x)=\theta x^{-2} e^{-\theta/x}, \qquad x>0.
            $$

            ¿Qué tamaños se alcanzan con dicho test?
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}[1,1 puntos]
        En cierta academia se pretende hacer un estudio sobre la efectividad de un curso
        intensivo de aprendizaje. Para ello, se selecciona aleatoriamente a $10$ alumnos a los
        que se somete a una misma prueba de control antes y después del curso, obteniéndose
        las siguientes calificaciones:

        \begin{center}
            \begin{tabular}{c|cccccccccc}
                A & 7,25 & 7,5 & 7,5 & 8 & 8,5 & 8,75 & 8 & 8,25 & 7,25 & 8,5 \\
                \hline
                D & 7 & 8,25 & 8 & 8 & 8,75 & 9,5 & 8,5 & 8,75 & 7,75 & 8
            \end{tabular}
        \end{center}

        ¿Se puede concluir, a la vista de los datos, que el curso de aprendizaje no afecta a
        las calificaciones de los alumnos? Especificar bajo qué condiciones sobre las variables
        consideradas se puede resolver este problema y concretar las hipótesis nula y alternativa
        que se contrastan. Resolver este problema al nivel de significación $0{.}1$ mediante el
        uso de diferentes tests, detallando las hipótesis necesarias para la aplicación de cada
        uno de ellos.
    \end{ejercicio}

    \newpage

    \setcounter{ejercicio}{0}

    \begin{ejercicio}[2,25 puntos]
        Sea $(X_1,\ldots,X_n)$ una muestra aleatoria simple de una variable aleatoria $X$ con
        distribución en una familia paramétrica.

        \begin{enumerate}
            \item[a)] Dar la definición de estadístico suficiente. Enunciar el Teorema de
            Factorización de Neyman--Fisher. Demostrar dicho teorema para variables discretas. \\

            \begin{definicion}[Estadístico Suficiente]\label{definicion:d1}
                Sea $(X_1, \ldots, X_n)$ una m.a.s. de $X \rightsquigarrow F \in \{F_\theta, \theta \in \Theta\}$. Un estadístico
                $T(X_1, \ldots, X_n)$ es \textit{suficiente} para la familia de distribuciones considerada (o suficiente para $\theta$)
                si la distribución de la muestra condicionada a cualquier valor del estadístico, $T(X_1, \ldots, X_n) = t$, es independiente de $\theta$.
            \end{definicion}

            \begin{teo}[de Factorización de Neyman--Fisher]\label{teo:t1}
                Sea $(X_1, \ldots, X_n)$ una m.a.s de $X \rightsquigarrow F \in \{F_\theta, \theta \in \Theta\}$. Sea $f_\theta$ la función 
                masa de probabilidad o función de densidad de $X$ bajo $F_\theta$ y sea $f_\theta^n$ la f.m.p. o f.d.d. de la muestra bajo $F_\theta$.
                Un estadístico $T(X_1, \ldots, X_n)$ se dice que es suficiente si y solo si, para cualquier valor de $\theta \in \Theta$,
                $$f_\theta^n(x_1, \ldots, x_n) = h(x_1, \ldots, x_n)g_\theta(T(X_1, \ldots, X_n)), \quad \forall (x_1, \ldots, x_n) \in \X^n$$
                donde $h$ es independiente de $\theta$ y $g_\theta$ depende de $(x_1, \ldots, x_n)$ solo a través de $T(x_1, \ldots, x_n)$.
            \end{teo}

            \begin{proof}
                Se demostrará por doble implicación.
                \begin{itemize}
                    \item[$\Longrightarrow)$] Supongamos que $T \equiv T(X_1, \ldots, X_n)$ es suficiente:
                    $$P_\theta(X_1 = x_1, \ldots, X_n = x_n) = P_\theta(X_1 = x_1, \ldots, X_n = x_n, T = T(x_1, \ldots, x_n)) = $$
                    $$\underbrace{P_\theta(X_1 = x_1, \ldots, X_n = x_n / T = T(x_1, \ldots, x_n))}_{h(x_1, \ldots, x_n)} 
                    \underbrace{P_\theta(T = T(x_1, \ldots, x_n))}_{g_\theta(T(x_1, \ldots, x_n))}$$
                    \item[$(\Longleftarrow$] Denotamos por $\textbf{X} = (X_1, \ldots, X_n)$ y a las posibles realizaciones muestrales
                    $\textbf{x} = (x_1, \ldots, x_n)$. Supongamos que la función masa de probabilidad de la muestra se factoriza según se
                    indica, $$P_\theta(\textbf{X} = \textbf{x}) = h(\textbf{x}) g_\theta(T(\textbf{x})) \quad \forall \textbf{x} \in \X^n$$
                    Para probar la suficiencia de $T$ se calcula la distribución condicionada de la muestra a un valor arbitrario, $T=t$.
                    $$P_\theta (\textbf{X} = \textbf{x} / T = t) = \dfrac{P_\theta(\textbf{X} = \textbf{x}, T = t)}{P_\theta(T = t)} = 
                    \begin{cases}
                        0 \quad &\text{si } T(\textbf{x}) \neq t \\
                        \dfrac{P_\theta(\textbf{X} = \textbf{x})}{P_\theta(T = t)} \quad &\text{si } T(\textbf{x}) = t
                    \end{cases}$$
                    Si $T(x) = t$, entonces
                    $$P_\theta(\textbf{X} = \textbf{x}) = h(\textbf{x}) g_\theta(T(\textbf{x})) = h(\textbf{x}) g_\theta(t)$$
                    $$P_\theta(T = t) = \sum_{\textbf{x}' / T(\textbf{x}') = t} P_\theta(\textbf{X} = \textbf{x}') =
                    \sum_{\textbf{x}' / T(\textbf{x}') = t} h(\textbf{x}') g_\theta(T(\textbf{x}')) = 
                    g_\theta(t) \sum_{\textbf{x}' / T(\textbf{x}') = t} h(\textbf{x}')$$ 
                    Así, la distribución condicionada es independiente de $\theta$, y $T$ es suficiente por la Definición \ref{definicion:d1}:
                    $$P (\textbf{X} = \textbf{x} / T = t) = \begin{cases}
                        0 \quad &\text{si } T(\textbf{x}) \neq t \\
                        \dfrac{h(\textbf{x})}{\displaystyle\sum_{\textbf{x}' / T(\textbf{x}') = t} h(\textbf{x}')} \quad &\text{si } T(\textbf{x}) = t
                    \end{cases}$$
                    lo que concluye la demostración. \qedhere
                \end{itemize}
            \end{proof}

            \item[b)] Si la función de distribución de $X$ es $$F_\theta(x)=1-e^{-\theta x}, \quad x>0$$
            encontrar el intervalo de confianza para $\theta$ de menor longitud media
            uniformemente a nivel de confianza $1-\alpha$, basado en un estadístico suficiente. \\

            Vemos que $X \rightsquigarrow \Exp(\theta)$, dado que $F_\theta(x)$ es la función de distribución es la de una exponencial con 
            parámetro $\theta > 0$, y $\X = \Rmas$. Para hallar el estadístico suficiente,
            usaremos el Teorema \ref{teo:t1}. Primero obtenemos la función de densidad 
            $$f_\theta(x) = \dfrac{\partial F_\theta(x)}{\partial \theta} = \theta e^{-\theta x}, \quad x > 0$$
            Calculamos ahora la función de densidad conjunta de la muestra
            $$f^n_\theta(x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^{n} f_\theta(x_i)$$
            A partir de ahora se asume que $\theta > 0$ (de lo contrario $f_\theta(x) = 0$). 
            Como $x > 0 \iff I_{\Rmas}(x - 0) = I_{\Rmas}(x) = 1$ se deduce que 
            $$\prod_{i=1}^{n} f_\theta(x_i) = \prod_{i=1}^{n} \theta e^{-\theta x_i} \neq 0 \iff I_{\Rmas} (x_{(1)}) = 1$$
            luego $$f^n_\theta(X_1, \ldots, X_n) = \prod_{i=1}^{n} \theta e^{-\theta x_i} I_{\Rmas}(x_{(1)}) = 
            \theta^n \prod_{i=1}^{n} e^{-\theta x_i} I_{\Rmas}(x_{(1)}) = $$$$ 
            \theta^n e^{\left(-\theta \displaystyle \sum_{i=1}^{n} x_i \right)} I_{\Rmas}(x_{(1)})$$
            Tomando $$T(X_1, \ldots, X_n) = \sum_{i=1}^{n} X_i, \quad 
            h(x_1, \ldots, x_n) = I_{\Rmas}(x_{(1)}), \quad g_\theta(t) = 
            \theta^n e^{\left(-\theta t \right)}$$
            Se cumple que 
            $$f^n_\theta(x_1, \ldots, x_n) = h(x_1, \ldots, x_n) g_\theta(T(x_1, \ldots, x_n)) \quad \forall (x_1, \ldots, x_n) \in \X^n$$
            donde $h$ es independiente del parámetro $\theta$ y $g_\theta$ depende de la muestra solo a través del estadístico, luego,
            por el Teorema de Factorización de Neyman-Fisher, el estadístico $T$ es suficiente. \\

            Denotamos ahora a dicho estadístico $T$ por $\tilde{T} \stackrel{not.}{\equiv} T$. Ahora encontraremos el intervalo de confianza por medio del método de la cantidad pivotal. Nos basaremos
            en el siguiente pivote visto en teoría para $\theta$:
            $$T(X_1, \ldots, X_n; \theta) = - 2 \sum_{i=1}^n \ln F_\theta(X_i) \rightsquigarrow \chi^2(2n)$$
            Sin embargo, se pide expresarlo en función del estadístico $T$ suficiente. Para ello, considerando el cambio $Y = \theta X \Longrightarrow X = Y / \theta$, y entonces
            $$f_Y(y) = f_X(y/\theta) \cdot \dfrac{1}{\theta} = \theta e^{-\theta(y/\theta)} \cdot \dfrac{1}{\theta} = e^{-y}, \quad y > 0$$
            donde en la primera igualdad se ha usado la fórmula del cambio de variable $f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|$. De aquí que 
            $$F_Y(y) = \int_{0}^{y} f_y(t) dt = \int_{0}^{y} e^{-t} dt = - [-e^{-t}]_0^y = - \left( e^{-y} - 1 \right) = 1 - e^{-y}$$
            Luego $Y = \theta X \rightsquigarrow \Exp(1)$. Como $$Y_1, \ldots, Y_n \text{ son v.a.i.i.d. con } Y_i \rightsquigarrow \Exp(1), \quad 
            i=1, \ldots, n$$ usando que 
            $$i = 1, \ldots, n \hspace{0.2cm} X_i \rightsquigarrow \Exp(\lambda) \iff \sum_{i=1}^{n} X_i \rightsquigarrow \Er(n,\lambda) \iff \sum_{i=1}^{n} X_i \rightsquigarrow \Gamma(n,\lambda) \hspace{0.2cm} \lambda > 0, n \in \N$$
            y que la distribución $\chi^2$ de Pearson es un caso particular de la distribución gamma, verificándose que $X \rightsquigarrow \chi^2(n) \iff X \rightsquigarrow \Gamma(n/2, 1/2)$, se llega a que
            $$\sum_{i=1}^n Y_i \rightsquigarrow \Gamma(n,1) \iff 2 \sum_{i=1}^n Y_i \rightsquigarrow \chi^2(2n)$$
            y deshaciendo el cambio $Y_i = \theta X_i \quad \forall i = 1,\ldots,n$, obtenemos el pivote para $\theta$ en función de la m.a.s. de $X$ y del estadístico $\tilde{T}$ antes hallado.
            $$T(X_1, \ldots, X_n; \theta) = 2 \theta \sum_{i=1}^n X_i = 2 \theta \tilde{T}(X_1, \ldots, X_n) \rightsquigarrow \chi^2(2n)$$
            tenemos una función de la muestra y del parámetro $T : \X^n \times \Theta \to \R$, que es una v.a. cuya distribución $(\chi^2(2n))$ es independiente del parámetro $\theta$, luego por definición es una función pivote. \\
            
            Además se comprueba que $T$ es estrictamente monótona en $\theta$ (de hecho estrictamente creciente, dado que 
            $\sum_{i=1}^{n} X_i > 0$ y $\theta > 0$), y $\Lambda = \Rmas$, siendo $\Lambda$ la imagen de $T$. Para cualquier $\lambda \in \Lambda$,
            $$T(X_1, \ldots, X_n; \theta) = \lambda \iff 2 \theta \tilde{T}(X_1, \ldots, X_n) = \lambda \iff \theta = \dfrac{\lambda}{2 \tilde{T}(X_1, \ldots, X_n)} > 0$$
            luego, por un teorema visto en teoría, puede construirse un intervalo de confianza para $\theta$ a cualquier nivel de confianza. \\

            Por definición de IC a nivel de confianza $1-\alpha$ para $\theta$, este debe verificar
            $$P_{\theta}(\lambda_1 < 2 \theta \tilde{T} < \lambda_2) \geqslant 1 - \alpha$$
            Despejando $\theta$ en el centro de la cadena de desigualdades
            $$\lambda_1 < 2 \theta \tilde{T} < \lambda_2 \iff \dfrac{\lambda_1}{2 \tilde{T}} < \theta < \dfrac{\lambda_2}{2 \tilde{T}}$$
            Las soluciones se alcanzan minimizando la longitud media esperada uniforme, siendo la longitud 
            $$L = I_2(X_1, \ldots, X_n) - I_1(X_1, \ldots, X_n) = \dfrac{\lambda_2}{2 \tilde{T}} - \dfrac{\lambda_1}{2 \tilde{T}} = \dfrac{\lambda_2 - \lambda_1}{2 \tilde{T}}$$
            y la condición de minima longitud uniforme $$E_\theta[L] \leqslant E_\theta[L'] \quad \forall \theta \in \Theta$$
            con $L'$ otra longitud cualquiera de cualquier otro IC a nivel de confianza $1-\alpha$. \\

            En la práctica, aunque no sea el IC de mínima longitud uniformemente (por la asimetría de 
            la distribución $\chi^2$), se toma el de colas iguales\footnote{En teoría se ha hecho esto, con, por ejemplo, el IC para la varianza siendo la media conocida en una población normal}, o sea
            $$\lambda_1 = \chi^2_{2n;1-\alpha/2} \quad \lambda_2 = \chi^2_{2n;\alpha/2}$$
            con $P[\chi^2_{2n} > \chi^2_{2n;\alpha}] > \alpha$
            y entonces el IC para $\theta$ a nivel de confianza $1-\alpha$ es
            $$\left(\dfrac{\chi^2_{2n;1-\alpha/2}}{2 \tilde{T}(X_1, \ldots, X_n)}, \dfrac{\chi^2_{2n;\alpha/2}}{2 \tilde{T}(X_1, \ldots, X_n)}  \right)$$
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[2,25 puntos]
        Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable aleatoria con función
        de densidad $$f_\theta(x)=\dfrac{1}{2\sqrt{2\theta-1}\sqrt{x-1}}, \quad 1<x<2\theta$$

        \begin{enumerate}
            \item[a)] Sabiendo que $T=\max X_i$ es suficiente, encontrar, si existe, el UMVUE para
            $(2\theta-1)^{-1}$, especificando previamente el espacio paramétrico y el espacio
            muestral. Justificar la no existencia del UMVUE en los casos que corresponda. \\

            El espacio paramétrico se determina por la condición $2 \theta - 1 > 0$, para que la densidad esté definida.
            Entonces $\theta > 1/2$, luego $\Theta = \left]1/2, +\infty\right[$. \\
            
            El espacio muestral se determina con $x>1$, luego $\X = \left]1,+\infty\right[$. \\

            Consideramos ahora la transformación biunívoca $S = g(T) = \sqrt{T-1}$. Como 
            $X_i > 1 \quad \forall i=1, \ldots, n \Longrightarrow T = \max X_i > 1$, luego $g$ está bien definida,
            y sabemos que $g$ es biyectiva en $]1,+\infty[$, con inversa $T = g^{-1}(S) = S^2+1$.
            Así, como $S$ es una transformación biunívoca de $T$ estadístico suficiente, sabemos que $S$
            también será un estadístico suficiente para la familia $\{F_\theta, \theta \in \Theta\}$. Además
            $S = \max Y_i = \sqrt{\max (X_i - 1)} = \sqrt{T-1}$, usando que $x \mapsto \sqrt{x-1}$ es 
            estrictamente creciente.\\

            Ahora, si $x = y^2 + 1$, entonces $dx = 2y dy$ y la función de densidad es
            $$f_Y(y) \stackrel{(*)}{=} f_\theta(y^2+1)2y = \dfrac{1}{2 \sqrt{2 \theta - 1} \sqrt{y^2}} 2y = \dfrac{1}{\sqrt{2 \theta - 1}},
            \quad 0 < y < \sqrt{2 \theta -1}$$
            donde en $(*)$ se ha usado la fórmula del cambio de variable $f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|$.
            Se tiene que $Y_i \rightsquigarrow U(0,b), \quad \forall i = 1, \ldots, n$, con $b = \sqrt{2 \theta -1}$. Calculamos
            $F_Y(s)$:
            $$F_Y(s) = \int_{0}^{s} f_Y(y)dy = \int_{0}^{s} \dfrac{1}{b} dy = \dfrac{1}{b}
            \int_{0}^{s} dy = \dfrac{s}{b}$$ 
            La función de distribución del máximo $S = Y_{(n)}$ verifica que
            $$F_S(s) = (F_Y(s))^n = \left( \dfrac{s}{b} \right)^n, \quad 0 < s < b \Longrightarrow
            f_S(s) = n \left( \dfrac{s}{b} \right)^{n-1} f_\theta(s) = n \dfrac{s^{n-1}}{b^{n-1}} \dfrac{1}{b} = 
            n \dfrac{s^{n-1}}{b^{n}}$$

            Probamos ahora la completitud de $S$. Sea $h$ una función medible verificando
            $$0 = \E[h(S)] \stackrel{def}{=} \int_{0}^{b} h(s) f_S(s) ds = \int_{0}^{b} h(s) n \dfrac{s^{n-1}}{b^{n}} ds = 
            \dfrac{n}{b^n} \int_{0}^{b} h(s) s^{n-1} ds $$

            Como $n/b^n \neq 0 \quad \forall n \in \N, \quad \forall \theta \in \Theta$, debe ser
            $$\int_{0}^{b} h(s) s^{n-1} ds = 0$$
            Por el Teorema Fundamental del Cálculo, podemos considerar una primitiva $H(s)$ del integrando $h(s) s^{n-1}$, y esta cumple,
            por la regla de Barrow, que $H(b) - H(0) = 0 \quad \forall b > 0$. Derivando respecto de $b$ se obtiene que
            $$H(b) = 0 \iff h(b) b^{n-1} = 0$$
            Como $b = \sqrt{2 \theta - 1}, \quad \forall \theta \in \Theta \Longrightarrow b \neq 0 \Longrightarrow b^{n-1} \neq 0$,
            luego 
            $$H(b) = 0 \iff h(b) b^{n-1} = 0 \iff h(b) = 0$$
            Por tanto, $\mathcal{S} = ]0,b[ \subset \{s : h(s) = 0\}$, y consecuentemente
            $$1 \geqslant P[h(S) = 0] \geqslant P[S \in \mathcal{S}] = 1 \Longrightarrow P[h(S) = 0] = 1$$

            Concluimos entonces que $S$ es un estadístico completo, y como ya era suficiente, por ser transformación biunívoca de $T$, que era suficiente,
            entonces en este punto $S$ es un estadístico suficiente y completo. Falta encontrar una función medible $h(S)$ y comprobar que sea insesgada en $g(\theta) = (2\theta - 1)^{-1} = 
            (b^2)^{-1} = b^{-2}$, estimador, y que tenga momento de segundo orden finito $E_b[h(S)^2] < +\infty$. Viendo que, para $k \in \Z$, se tiene que
            $$E_b[S^k] = \int_{0}^{b} s^k \dfrac{n s^{n-1}}{b^n} ds = \dfrac{n}{b^n} \int_{0}^{b} s^{n+k-1} ds \stackrel{(**)}{=} \dfrac{n}{b^n}
            \left[ \dfrac{s^{n+k}}{n+k} \right]_0^b = $$
            $$\dfrac{n}{b^n} \dfrac{b^{n+k}}{n+k} = \dfrac{n}{n+k} b^k$$
            donde en $(**)$ se ha impuesto que $n+k > 0$, para que la integral converja. En particular, para $k=-2$, debe ser entonces
            $n - 2 > 0 \iff n > 2 \iff n \geqslant 3$, y obtenemos
            $$E_b[S^{-2}] = E_b \left[ \dfrac{1}{S^2} \right] = \dfrac{n}{n-2} b^{-2}$$
            y de aquí se deduce que 
            $$E_b \left[ \dfrac{n-2}{n} \dfrac{1}{S^2} \right] = b^{-2} = g(\theta)$$
            Tomando $$h(S) = \dfrac{n-2}{n} \dfrac{1}{S^2}$$ por construcción $E_b[h(S)] = g(\theta)$. Como $(n-2)/n > 0 \quad \forall n \geqslant 3$,
            así como $1/S^2 > 0$, porque $S > 0$, entonces $h(S) > 0$, luego también es estimador de $g(\theta) = b^{-2} > 0 \quad \forall b > 0$,
            y por último 
            $$E_b[h(S)^2] = \left( \dfrac{n-2}{n} \right)^2 E_b \left[S^{-4} \right] \stackrel{(***)}{=} \dfrac{(n-2)^2}{n^2} \dfrac{n}{n-4} b^{-4} = 
            \dfrac{(n-2)^2}{n(n-4)}b^{-4} < +\infty$$

            donde en $(***)$ hay que imponer que $n-4 > 0 \iff n>4 \iff n \geqslant 5$. Por el Teorema de Lehmann-Scheffé, 
            concluimos que $E[h(S) / S] = h(S)$ es el UMVUE para $g(\theta) = (2 \theta-1)^2 = b^{-2}$, y existe siempre que $n \geqslant 5$.
            
            \newpage

            \item[b)] Calcular la función de verosimilitud y encontrar un estimador máximo verosímil de $2\theta-1$. \\
            
            Como $$f_\theta(x)=\dfrac{1}{2\sqrt{2\theta-1}\sqrt{x-1}}, \quad 1<x<2\theta$$
            Entonces $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep}{=} \prod_{i=1}^{n} f_\theta(x_i)$$
            A partir de aquí asumimos que $x_i > 1 \quad \forall i = 1, \ldots, n \iff x_{(1)} > 1$. En otro caso, $f_\theta(x_i) = 0 \quad \forall i = 1, \ldots, n$. Tenemos que
            $$x_i < 2 \theta \quad \forall i = 1, \ldots, n \iff x_{(n)} < 2 \theta \iff I_{\Rmenos}(x_{(n)} - 2 \theta) = 1$$
            Entonces
            $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep}{=} \prod_{i=1}^{n} f_\theta(x_i) = \prod_{i=1}^n \dfrac{1}{2\sqrt{2\theta-1}\sqrt{x_i-1}} I_{\Rmenos}(x_{(n)} - 2 \theta) =$$
            $$\dfrac{1}{(2\sqrt{2\theta-1})^n} \prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}} I_{\Rmenos}(x_{(n)} - 2 \theta)$$

            La función de verosimilitud de $\theta$ será entonces
            $$L_{x_1, \ldots, x_n}(\theta) = f_\theta^n(x_1, \ldots, x_n) = \dfrac{1}{(2\sqrt{2\theta-1})^n} \prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}} I_{\Rmenos}(x_{(n)} - 2 \theta)$$

            Consideramos la función paramétrica $\lambda = g(\theta) = 2 \theta - 1$, con $g: \Theta \to \Lambda$. Por definición de función de verosimilitud de una función paramétrica, considerando una realización muestral
            $(x_1, \ldots, x_n) \in \X^n$, con $\X = \left]1,+\infty \right[$, obtenido del apartado anterior:
            $$M_{x_1, \ldots, x_n}(\lambda) = \sup_{\theta \in g^{-1}(\lambda)} L_{x_1, \ldots, x_n}(\theta)$$

            Ahora expresamos $\theta$ explícitamente en función de $\lambda$, usando que $$\lambda = 2 \theta - 1 \iff \lambda + 1 = 2 \theta \iff \theta = \dfrac{\lambda+1}{2}$$
            Así pues, la función de verosimilitud de la función paramétrica $g(\theta)$ es
            $$M_{x_1, \ldots, x_n}(\lambda) = L_{x_1, \ldots, x_n}\left(\dfrac{\lambda+1}{2}\right) = \dfrac{1}{(2\sqrt{\lambda})^n} \prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}} I_{\Rmenos}(x_{(n)} - (\lambda+1))$$

            Ahora, puede razonarse que, expresando la función de verosimilitud en función de lambda, debe ser $x_{(n)} < \lambda + 1 \iff \lambda > x_{(n)} - 1$
            y como $\prod_{i=1}^n \frac{1}{\sqrt{x_i-1}} > 0$ no influye en el estudio de la monotonía, la monotonía la determina $\dfrac{1}{(2\sqrt{\lambda})^n} = (2\sqrt{\lambda})^{-n}$, 
            que es estrictamente decreciente como función de $\lambda$, hecho del cual se deduce que el máximo se alcanza en el ínfimo de $]x_{(n)} - 1, +\infty[$.
            Por tanto, $\hat{\lambda}(x_1, \ldots, x_n) \stackrel{not}{\equiv} \hat{\lambda} = x_{(n)} - 1$ es el EMV de $\lambda = g(\theta) = 2 \theta - 1$.
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[2 puntos]
        Sea $X$ una variable aleatoria con distribución en una familia regular en el sentido
        de Fréchet--Cramér--Rao, cuyas funciones de densidad son de la forma:
        $$f_\theta(x)=\exp \left\{T(x)\ln\theta-\dfrac{\theta^2}{2}+S(x)\right\} \quad x\in\R, \quad \theta\in\Rmas$$
        siendo $T(X)$ un estadístico regular.

        \begin{enumerate}
            \item[a)] Calcular la función de información asociada a $X$. \\
            
            Por el enunciado se deduce que $(X_1, \ldots, X_n)$ m.a.s de $X$ es de tamaño $n=1$, y que solo hay una observación de $X$.
            Por ser $T$ regular, se cumple que
            $$\dfrac{\partial}{\partial \theta} E_\theta[T(X_1, \ldots, X_n)] = E_\theta \left[T(X_1, \ldots, X_n) \dfrac{\partial \ln f_\theta^n (X_1, \ldots, X_n)}
            {\partial \theta} \right]$$
            en particular para $n=1$
            $$\dfrac{\partial}{\partial \theta} E_\theta[T(X)] = E_\theta \left[T(X) \dfrac{\partial \ln f_\theta (X)}
            {\partial \theta} \right]$$
            Primero usamos que la familia es regular
            $$\E_\theta \left[ \dfrac{\partial \ln f_\theta(X)}{\partial \theta} \right] = 0 \iff 
            \E_\theta \left[ \dfrac{T(X)}{\theta} - \theta \right] = 0 \iff
            \dfrac{1}{\theta} E_\theta[T(X)] - \theta = 0 \iff$$
            $$E_\theta[T(X)] = \theta^2$$
            y entonces
            $$\dfrac{\partial}{\partial \theta} E_\theta[T(X)] = \dfrac{\partial}{\partial \theta} (\theta^2) = 2 \theta$$
            Calculando
            $$\ln f_\theta(X) = T(X)\ln\theta-\dfrac{\theta^2}{2}+S(X)$$
            $$\dfrac{\partial \ln f_\theta(X)}{\partial \theta} = \dfrac{T(X)}{\theta} - \theta$$
            Como $$E_\theta \left[T(X) \dfrac{\partial \ln f_\theta (X)}
            {\partial \theta} \right] = E_\theta \left[T(X) \left( \dfrac{T(X)}{\theta} - \theta \right)  \right] = 
            \dfrac{1}{\theta} E_\theta[T(X)^2] - \theta E_\theta[T(X)] \stackrel{E_\theta[T(X)] = \theta^2}{=} $$
            $$\dfrac{1}{\theta} E_\theta[T(X)^2] - \theta^3$$
            Tenemos que 
            $$2 \theta = \dfrac{\partial}{\partial \theta} E_\theta[T(X)] = E_\theta \left[T(X) \dfrac{\partial \ln f_\theta (X)}
            {\partial \theta} \right] = \dfrac{1}{\theta} E_\theta[T(X)^2] - \theta^3 \iff$$
            $$2 \theta = \dfrac{1}{\theta} E_\theta[T(X)^2] - \theta^3 \iff E_\theta[T(X)^2] = \theta(2 \theta + \theta^3) = 2 \theta^2 + \theta^4$$

            Ahora, sabemos que la función de información asociada a $X$ puede calcularse usando
            $$I_X(\theta) = E_\theta \left[ \left( \dfrac{\partial \ln f_\theta (X)}{\partial \theta} \right)^2 \right] \quad \forall \theta
            \in \Theta$$

            Desarrollando
            $$I_X(\theta) = E_\theta \left[ \left( \dfrac{\partial \ln f_\theta (X)}{\partial \theta} \right)^2 \right] = 
            E_\theta \left[ \left( \dfrac{T(X)}{\theta} - \theta \right)^2 \right] = 
            E_\theta \left[ \left(\dfrac{T(X)}{\theta} \right)^2 - 2 \cdot T(X) + \theta^2 \right] =$$
            $$\dfrac{1}{\theta^2} E_\theta [T(X)^2] - 2 E_\theta[T(X)] + E_\theta[\theta^2] \stackrel{E_\theta[T(X)] = \theta^2}{=} 
            \dfrac{1}{\theta^2} E_\theta [T(X)^2] - 2 \theta^2 + \theta^2 \stackrel{E_\theta[T(X)^2] = 2 \theta^2 + \theta^4}{=}$$
            $$\dfrac{1}{\theta^2} (2 \theta^2 + \theta^4) - \theta^2 = 2 + \theta^2 - \theta^2 = 2$$

            \item[b)] Basándose en una muestra aleatoria simple de $X$, $(X_1,\dots,X_n)$, y
            suponiendo $T(X)>0$, encontrar la clase de funciones paramétricas que admiten
            estimador eficiente y los estimadores correspondientes. \\

            Buscamos aplicar el Teorema de Caracterización de Estimadores Eficientes. Para ello, obtenemos la función conjunta de la m.a.s. de $X$

            $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep}{=} \prod_{i=1}^n f_\theta(x_i)$$

            Se supondrá a partir de ahora que $x_i \in \R \quad \forall i=1,\ldots,n$ y $\theta \in \Rmas$. De lo contrario, $f_\theta(x) = 0$.
            $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep}{=} \prod_{i=1}^n f_\theta(x_i) = \prod_{i=1}^n 
            e^{\left(T(x_i)\ln\theta-\frac{\theta^2}{2}+S(x)\right)} = e^{\left(\ln\theta(\sum_{i=1}^n T(x_i))-\frac{n\theta^2}{2}+ \sum_{i=1}^{n}S(x)\right)} =$$
            $$\exp \left\{\ln\theta \left(\sum_{i=1}^n T(x_i) \right)-\frac{n\theta^2}{2}+ \sum_{i=1}^{n}S(x)\right\}$$
            $$\ln f_\theta^n(x_1, \ldots, x_n) = \ln\theta \left(\sum_{i=1}^n T(x_i) \right)-\frac{n\theta^2}{2}+ \sum_{i=1}^{n}S(x)$$
            $$\dfrac{\partial \ln f_\theta^n(x_1, \ldots, x_n)}{\partial \theta} = \dfrac{1}{\theta} \left(\sum_{i=1}^n T(x_i) \right) - 
            n \theta = \dfrac{1}{\theta} \left( \sum_{i=1}^{n} T(X_i) - n \theta^2 \right)$$

            Ahora, supongamos que $T(X_1, \ldots, X_n)$ es un estimador de $g(\theta)$ función paramétrica derivable y estrictamente monótona
            ($g'(\theta) \neq 0 \quad \forall \theta \in \Theta = \Rmas$). Como el enunciado nos dice que la familia es regular, y 
            $0 < I_X(\theta) = 2 < +\infty \quad \forall \theta \in \Theta$, $T$ es eficiente si y solo si $\forall \theta \in \Theta
            \quad \exists a(\theta) \neq 0$ tal que 
            $$P_\theta \left[ \dfrac{\partial \ln f_\theta^n(X_1, \ldots, X_n)}{\partial \theta} = a(\theta)[T(X_1, \ldots, X_n) - g(\theta)]\right] = 1$$
            y
            $$I_{(X_1, \ldots, X_n)}(\theta) = a(\theta) g'(\theta)$$
            Como 
            $$\dfrac{\partial \ln f_\theta^n(X_1, \ldots, X_n)}{\partial \theta} = a(\theta)[T(X_1, \ldots, X_n) - g(\theta)] \iff$$
            $$\dfrac{1}{\theta} \left( \sum_{i=1}^{n} T(X_i) - n \theta^2 \right) = a(\theta) [T(X_1, \ldots, X_n) - g(\theta)]$$
            claramente por comparación se obtiene que 
            $$T(X_1, \ldots, X_n) = \sum_{i=1}^{n} T(X_i), \quad g(\theta) = n \theta^2, \quad a(\theta) = \dfrac{1}{\theta}$$
            y tanto $a(\theta)$ como $g(\theta)$ verifican todas las condiciones del teorema, pues $g'(\theta) = 2n \theta > 0$, porque $\theta > 0$
            y $2n \geqslant 2$, y usando la aditividad de la función de información de Fisher
            $$2n = n I_X(\theta) = I_{(X_1, \ldots, X_n)}(\theta) = a(\theta) g'(\theta) = \dfrac{1}{\theta} 2 n \theta = 2n$$

            Por un corolario visto en teoría, como $T(X_1, \ldots, X_n)$ es un estimador eficiente para $g(\theta)$, con $g'(\theta) \neq 0$,
            sabemos que las únicas funciones paramétricas que admiten estimadores eficientes son las de la forma $a g(\theta) + b$ y los correspondientes
            estimadores eficientes son $aT + b$, con probabilidad $1$, bajo todas las distribuciones de la familia.

            \item[c)] Bajo los supuestos del apartado b), calcular la cota inferior para la varianza
            de estimadores insesgados de $\ln\theta$, regulares, y justificar si se alcanza o no dicha cota. \\

            En este apartado $g(\theta) = \ln \theta$. Ya se han comprobado las hipótesis del Teorema de Cota de FCR en el apartado b), y 
            supuesto que los estimadores insesgados sean además de segundo orden (si no, la varianza no será finita), se tiene que
            la cota inferior de la varianza es, tomando como uno de estos estimadores $T(X_1, \ldots, X_n)$:
            $$\V_\theta[T(X_1, \ldots, X_n)] \geqslant \dfrac{(g'(\theta))^2}{I_{(X_1, \ldots, X_n)}(\theta)} \quad \forall \theta \in \Theta$$
            Ahora, $g'(\theta) = 1 / \theta$, y ya hemos visto que $I_{(X_1, \ldots, X_n)}(\theta) = n I_X(\theta) = 2n$, luego la cota inferior
            para la varianza de todo estos estimadores será
            $$\V_\theta[T(X_1, \ldots, X_n)] \geqslant \dfrac{(1/\theta)^2}{2n} = \dfrac{1}{2n \theta^2}$$ 

            La cota no se alcanza por reducción al absurdo. Si lo hiciera, entonces tendríamos que $g (\theta) = \ln \theta$ admitiría
            un estimador regular, insesgado y cuya varianza alcanza la cota de FCR para cualquier valor $\theta \in \Theta$, es decir, un estimador eficiente (dado que $\{F_\theta : \theta \in \Theta = \Rmas\}$ es una familia regular, $0 < I_X(\theta) = 2 < + \infty
            \quad \forall \theta \in \Theta$ y $g(\theta) = \ln \theta$ es una función paramétrica derivable). Sin embargo, 
            hemos visto en b) que las únicas funciones paramétricas que admiten estimador eficiente en esta familia son las 
            de la forma $a n\theta^2 + b$, $a, b \in \R$. Como $\nexists a,b \in \R : \ln \theta = a n\theta^2 + b$, hemos llegado a contradicción.   
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[2,4 puntos]
        Contraste de hipótesis:

        \begin{enumerate}
            \item[a)] Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable aleatoria $X$
            con distribución en una familia $\{P_\theta \mid \theta \in \Theta\}$. Sea $\Theta_0$
            subconjunto arbitrario de $\Theta$ y supongamos que se pretende contrastar la hipótesis
            $$
            H_0: \theta \in \Theta_0
            $$

            \begin{enumerate}
                \item[a1)] Detallar la hipótesis alternativa. Definir formalmente el concepto de test de
                hipótesis y dar la interpretación de sus valores. \\

                La hipótesis alternativa es la negación de la hipótesis nula $H_0$, y se denota por
                $$H_1: \theta \in \Theta_1 = \Theta - \Theta_0$$ El problema de contraste de hipótesis,
                en general, es el siguiente: se constrasta la hipótesis nula, $H_0: \theta \in \Theta_0$,
                frente a la hipótesis alternativa, $H_1: \theta \in \Theta_1$. La hipótesis sobre el parámetro
                establece una partición sobre el espacio paramétrico, pues $\Theta = \Theta_0 \cup \Theta_1$
                y $\Theta_0 \cap \Theta_1 = \emptyset$, y además sobre la familia de distribuciones, ya que
                $$\{F_\theta, \theta \in \Theta\} = \{F_\theta, \theta \in \Theta_0\} \cup 
                \{F_\theta, \theta \in \Theta_1\}$$

                La definición formal del concepto de test de hipótesis es la siguiente:

                \begin{definicion}[Test de Hipótesis]
                    Un \textit{test de hipótesis} es un estadístico, $\varphi(X_1, \ldots, X_n)$, con valores en 
                    $[0,1]$, que especifica la probabilidad de rechazar $H_0$ para cada realización muestral.
                \end{definicion}

                Distinguimos dos tipos de test de hipótesis, los no aleatorizados y los aleatorizados. \\

                En los no aleatorizados, se considera la región crítica o de rechazo $\mathcal{C} \subset \X^n$,
                que es aquella en la que se rechaza con probabilidad $1$ la hipótesis nula $H_0$ y
                el test se puede escribir como $\varphi : \X^n \to \{0,1\}$, dado por 
                $$\varphi(X_1, \ldots, X_n) = {\begin{cases}
                    1 \quad &\text{si } (X_1, \ldots, X_n) \in \mathcal{C} \\
                    0 \quad &\text{si } (X_1, \ldots, X_n) \notin \mathcal{C}
                \end{cases}}$$

                En los aleatorizados, el codominio del test se extiende a $[0,1]$, y el test es
                $\varphi: \X^n \to [0,1]$, donde $\varphi(X_1, \ldots, X_n)$ es la probabilidad de
                rechazar la hipótesis nula $H_0$ en cada realización muestral, al igual que en el caso
                de los tests no aleatorizados.

                \newpage

                \item[a2)] Definir el tamaño y la función de potencia de un test arbitrario para resolver el
                problema anterior, explicando el significado de estos conceptos en términos del rechazo
                de $H_0$. \\

                Se define la función de potencia de un test arbitrario como sigue:

                \begin{definicion}[Función de Potencia]
                    Para un test $\varphi$, se define la \textit{función de potencia} como la función 
                    que asocia a cada parámetro, $\theta$, la probabilidad media de rechazar $H_0$ cuando
                    el (verdadero) valor del parámetro es $\theta$
                    \Func{\beta_\varphi}{\Theta}{[0,1]}{\theta}{B_\varphi(\theta) = E_\theta[\varphi(X_1, \ldots, X_n)]}
                \end{definicion}

                En el caso de un test no aleatorizado, se puede definir como
                \Func{\beta_\varphi}{\Theta}{[0,1]}{\theta}{B_\varphi(\theta) = P_\theta[(X_1, \ldots, X_n) \in \mathcal{C}]}

                Asimismo, se define el tamaño de un test arbitrario de la siguiente forma, recordando que
                un error de tipo I consiste en rechazar la hipótesis nula $H_0$ siendo esta verdadera:
                \begin{definicion}[Tamaño de un Test]
                    Para un test $\varphi$, se define el \textit{tamaño del test} como la máxima probabilidad
                    media de cometer un error de tipo I con dicho test, es decir
                    $$\sup_{\theta \in \Theta_0} E_\theta[\varphi(X_1, \ldots, X_n)] = \sup_{\theta \in \Theta_0} B_\varphi(\theta)$$
                    En el caso de un test $\varphi$ no aleatorizado, se puede definir como 
                    $$\sup_{\theta \in \Theta_0} P_\theta[(X_1, \ldots, X_n) \in \mathcal{C}]$$
                \end{definicion} 

                \item[a3)] En términos del tamaño y de la función potencia, ¿qué significa que un test tiene
                nivel de significación $\alpha$ para el problema de contraste planteado? ¿Cuáles son las
                condiciones para que un test sea UMP a nivel de significación $\alpha$? \\

                El nivel de significación se define por:
                \begin{definicion}[Nivel de Significación]
                    Se dice que $\varphi$ es un test de hipótesis con \textit{nivel de significación} $\alpha \in [0,1]$
                    si su tamaño es menor o igual que $\alpha$ (actuando $\alpha$ como cota superior de las probabilidades
                    medias de cometer un error de tipo $I$), es decir:
                    $$\forall \theta \in \Theta_0, \quad \beta_\varphi(\theta) = E_\theta[\varphi(X_1, \ldots, X_n)]
                    \leqslant \alpha$$
                    En el caso de un test $\varphi$ no aleatorizado, se puede definir como
                    $$\forall \theta \in \Theta_0, \quad P_\theta[(X_1, \ldots, X_n) \in \mathcal{C}] \leqslant \alpha$$
                \end{definicion}

                Por lo tanto, un test tiene nivel de significación $\alpha \in [0,1]$ si su tamaño, es decir,
                el supremo de la función de potencia en el espacio paramétrico de la hipótesis nula, es menor o igual
                que $\alpha$. \\

                Se dice que un test es uniformemente más potente a nivel de significación $\alpha$ si verifica 
                la Definición \ref{definicion:d2}:
                \begin{definicion}[Test Uniformemente Más Potente (UMP)]\label{definicion:d2}
                    Dado un nivel de significación $\alpha$ fijo, se dice que un test $\varphi$ es \textit{uniformemente
                    más potente} (UMP), a ese nivel, si se cumplen las siguientes dos condiciones:
                    \begin{enumerate}
                        \item $\varphi$ tiene nivel de significación $\alpha$, es decir
                        $$E_\theta[\varphi(X_1, \ldots, X_n)] \leqslant \alpha \quad \forall \theta \in \Theta_0$$
                        \item Para cualquier otro test $\varphi^{*}$, con nivel de significación $\alpha$, se cumple
                        que la función de potencia en $\varphi$ es mayor que en $\varphi^{*}$, para cualquier $\theta \in 
                        \Theta_1$. Equivalentemente
                        $$B_\varphi(\theta) \geqslant \beta_{\varphi^{*}}(\theta) \quad \forall \theta \in \Theta_1$$
                    \end{enumerate}
                \end{definicion}
                El hecho de considerar la maximización solo en $\Theta_1$ y no en $\Theta_0$ la función de potencia
                se debe a que se busca minimizar el error de tipo $II$, que consiste en no 
                rechazar $H_0$ siendo $H_0$ falsa. \\

                En general, no tiene porqué existir un test UMP para un problema dado.
            \end{enumerate}

            \item[b)] Obtener un test de razón de verosimilitud de tamaño $\alpha$ para contrastar
            $$
            H_0:\ \theta \le \theta_0
            \qquad \text{frente a} \qquad
            H_1:\ \theta > \theta_0,
            $$
            basado en una observación de una variable aleatoria con la siguiente función de densidad
            (detallar y justificar todos los pasos para la obtención, incluyendo el estudio detallado
            del estadístico de contraste y su representación gráfica):
            $$
            f_\theta(x)=\theta x^{-2} e^{-\theta/x}, \qquad x>0.
            $$

            ¿Qué tamaños se alcanzan con dicho test? \\

            Dado que las hipótesis no son simples, no podemos aplicar el Lema de Neyman-Pearson, por lo que habrá que utilizar el Test
            de la Razón de Verosimilitudes (TRV). Se tiene que $\X = \Rmas$ y la muestra se compone de una única observación (pongamos que $x$
            es el valor obtenido en la realización muestral). Obtenemos ahora el EMV de $\theta$.
            $$L_x(\theta) = f_\theta(x) = \theta x^{-2} e^{-\theta/x} \Longrightarrow \ln f_\theta(x) = \ln \theta - 2 \ln x - \theta/x \Longrightarrow$$
            $$\dfrac{\partial \ln f_\theta(x)}{\partial \theta} = \dfrac{1}{\theta} - \dfrac{1}{x} = \dfrac{x - \theta}{x \theta}$$
            Resolviendo la ecuación de verosimilitud
            $$\dfrac{\partial \ln f_\theta(x)}{\partial \theta} = 0 \iff \dfrac{x - \theta}{x \theta} = 0 \iff \theta = x$$
            Como $$\dfrac{\partial^2 \ln f_\theta(x)}{\partial^2 \theta} = - \dfrac{1}{\theta^2} < 0$$
            se tiene que $\hat{\theta} = x$ es el EMV de $\theta$. 
            La razón de verosimilitudes es entonces
            $$\lambda(x) = \dfrac{\sup_{\theta \in \Theta_0} L_x(\theta)}{\sup_{\theta \in \Theta} L_x(\theta)}, \quad x \in \X$$
            Como $\hat{\theta}$ es el EMV de $x$, entonces $\sup_{\theta \in \Theta} L_x(\theta) = L_x(x)$.
            Ahora, estudiamos la monotonía respecto de $\theta$ de la función de verosimilitud
            $$L_x(\theta) = \theta x^{-2} e^{-\theta/x} \Longrightarrow \dfrac{\partial L_x(\theta)}{\partial \theta} = 
            x^{-2} \left(e^{-\theta/x} + \theta e^{-\theta/x} \dfrac{-1}{x} \right) = x^{-2} e^{-\theta/x} \left(1 - \dfrac{\theta}{x} \right)$$
            Como $x^{-2} e^{- \theta/x} > 0$ el signo lo determinará el factor $1 - \theta / x$. Hay tres posibilidades:
            \begin{enumerate}
                \item Si $1 - \theta / x < 0 \iff 1 < \theta/x \iff x < \theta$, entonces $L_x(\theta)$ es decreciente.
                \item Si $1 - \theta / x > 0 \iff 1 > \theta/x \iff x > \theta$, entonces $L_x(\theta)$ es creciente.
                \item Si $1 - \theta / x = 0 \iff 1 = \theta/x \iff \theta = x$, sabemos que se llega al EMV de $\theta$.
            \end{enumerate}
            Por lo tanto, $L_x(\theta)$ es creciente en $]0, x[$ y decreciente en $]x, +\infty[$. Usando esto, se deduce que
            $$\sup_{\theta \in \Theta_0} L_x(\theta) = \begin{cases} 
                L_x(x) &\text{si } x \leqslant \theta_0 \\
                L_x(\theta_0) &\text{si } x>\theta_0
            \end{cases}$$
            y como $$\dfrac{L_x(\theta_0)}{L_x(x)} = \dfrac{\theta_0 x^{-2} e^{-\theta_0/x}}{x x^{-2} e^{-x/x}} = \dfrac{\theta_0}{x} e^{1-\theta_0/x}$$ 
            la razón de verosimilitudes se escribe como
            $$\lambda(x) = \begin{cases}
                \dfrac{L_x(x)}{L_x(x)} = 1 &\text{si } x \leqslant \theta_0 \\\\
                \dfrac{L_x(\theta_0)}{L_x(x)} = \dfrac{\theta_0}{x} e^{1-\theta_0/x} &\text{si } x > \theta_0
            \end{cases}$$
            Por construcción $0 \leqslant \lambda(x) \leqslant 1$, y el test de razón de verosimilitud (TRV) es:
            $$\varphi(X) = \begin{cases}
                1 &\text{si } \lambda(X) < c \\
                0 &\text{si } \lambda(X) \geqslant c
            \end{cases}$$
            donde $c \in ]0,1]$ constante se determina imponiendo el tamaño o nivel de significación $\alpha \in [0,1]$. \\

            Estudiamos ahora la monotonía de $\lambda$. Si $x > \theta_0$ consideramos $t = \theta_0 / x \in ]0,1]$,
            de donde $\lambda(x) = t e ^{1-t} = g(t)$. Como $g'(t) = e^{1-t} + te^{1-t} \cdot(-1) = e^{1-t}(1-t) > 0 \quad \forall t \in ]0,1[$.
            Por tanto, $g$ es creciente en $t$, pero $t = \theta_0 / x$ decrece si $x$ lo hace, luego
            $\lambda(x)$ es estrictamente decreciente en $]\theta_0, +\infty[$. Además
            $\lambda(\theta_0) = 1$ y $\lim\limits_{x \to \infty} \lambda(x) = 0$. Gráficamente:

            \begin{figure}[H]
                \centering
                \begin{tikzpicture}
                    % Ejes
                    \draw[-Stealth] (-0.2,0) -- (6,0) node[right] {$x$};
                    \draw[-Stealth] (0,-0.2) -- (0,3.5) node[left] {$\lm$};

                    % Recta con pendiente negativa
                    \draw[thick] (3,2.65) -- (6,0.65);
                    \draw[thick] (0,2.65) -- (3,2.65);

                    % Punto c en el eje x
                    \coordinate (C) at (4.5,0);
                    \draw (C) node[below] {$k$};

                    % Proyección sobre la recta
                    \coordinate (K) at (4.5,1.65);
                    \draw[dashed] (C) -- (K);
                    \draw[dashed] (K) -- (0,1.65);

                    \draw[dashed] (3,0) -- (3,2.65);
                    \draw (3,0) node[below] {$\theta_0$};

                    % Etiqueta k en el eje y
                    \draw (0,1.65) node[left] {$c$};
                    \draw (0,2.65) node[left] {$1$};

                    % Punto en la recta
                    \fill (K) circle (2pt);
                \end{tikzpicture}
            \end{figure}

            Luego
            $$
                \left.\begin{array}{l}
                    \lm < c \\
                    \lm \geq c
                \end{array}\right\} \Longleftrightarrow \left\{\begin{array}{l}
                    x > k \\
                    x \leq k
                \end{array}\right. \qquad \text{con\ } k\geq \theta_0
            $$
            Por lo que el test a considerar será:
            $$
            \varphi(X) = \begin{cases}
                1 &\text{si } X > k \\
                0 &\text{si } X \leqslant k
            \end{cases} 
            $$

            Imponemos tamaño $\alpha$:
            $$\alpha = \sup_{\theta \in \Theta_0} \E_\theta[\varphi(X)] = \sup_{\theta \leqslant \theta_0} P_\theta
            [X > k]$$
            Como $$f_\theta(x) = \theta x^{-2} e^{-\theta/x}$$
            entonces considerando el cambio $u = \theta/t \Longrightarrow t = \theta/u \Longrightarrow 
            dt = - \dfrac{\theta}{u^2}$, y $t^{-2} = (u/\theta)^2$, sustituyendo $$\theta t^{-2} e^{-\theta/t} dt = \theta \left( \dfrac{u}{\theta} \right)^2 e^{-u}
            \left( - \dfrac{\theta}{u^2} du \right) = -e^{-u} du$$
            y viendo que si $t \to 0^{+} \Longrightarrow u = \theta / t \to +\infty$
            y si $t = x \Longrightarrow u = \theta / x$, se tiene que 
            $$F_\theta(x) = \int_{0}^{t} f_\theta(t) dt = \int_{0}^{t} \theta x^{-2} e^{-\theta/x} dt = $$
            $$\int_{+\infty}^{\theta/x} -e^{-u} du = \int_{\theta/x}^{+\infty} e^{-u} du = [-e^{-u}]_{\theta/x}^{+\infty}
            = e^{-\theta/x}$$
            Así pues, para $x \in \X$, $F_\theta(x) = e^{-\theta/x}$, de donde
            $$P_\theta[X > k] = 1 - F_\theta(k) = 1 - e^{-\theta/k}$$
            Entonces el tamaño queda
            $$\alpha = \sup_{\theta \leqslant \theta_0} P_\theta [X > k] = P_{\theta_0}[X>k] = 1 - e^{-\theta_0/k}$$
            y $k$ queda determinado por
            $$\alpha = 1 - e^{-\theta_0/k} \iff \alpha - 1 = - e^{-\theta_0/k} \iff
            1 - \alpha = e^{-\theta_0/k} \iff \ln(1-\alpha) = -\theta_0 / k \iff$$$$k = - \dfrac{\theta_0}{\ln(1-\alpha)}$$
            y el TRV no aleatorizado es
            $$
            \varphi(X) = \begin{cases}
                1 &\text{si } X > - \dfrac{\theta_0}{\ln(1-\alpha)} \\\\
                0 &\text{si } X \leqslant - \dfrac{\theta_0}{\ln(1-\alpha)}
            \end{cases} 
            $$

            Los tamaños que se alcanzan con este test pueden hallarse considerando el tamaño como función de $k$
            $$\alpha(k) = 1 - e^{-\theta/k}, \quad k > 0$$
            Vemos que $$\lim\limits_{k \to +\infty} \alpha(k) = \lim\limits_{k \to +\infty} (1 - e^{-\theta/k}) = $$
            $$1 - \lim\limits_{k \to +\infty} e^{-\theta/k} = 1 - e^{0} = 0$$
            y que el menor $k$ se obtiene cuando $k = \theta_0$, con
            $\alpha(\theta_0) = 1 - e^{-\theta_0/\theta_0} = 1 - e^{-1}$.
            Por lo tanto, los tamaños que pueden obtenerse con el TRV no aleatorizado son
            $$\alpha \in [0, 1 - e^{-1}]$$ 

            Para obtener cualquier tamaño $\alpha \in [0,1]$ hay que aleatorizar el TRV, es decir
            $$\varphi(X) = \begin{cases}
                1 &\text{si } X > \theta_0 \\
                \gamma &\text{si } X \leqslant \theta_0
            \end{cases} \quad \gamma \in [0,1]$$
            Si $X > \theta_0$ se rechaza siempre, y si $X \leqslant \theta_0$ se rechaza con probabilidad $\gamma$.
            Imponiendo tamaño $\alpha$:
            $$\alpha = \sup_{\theta \in \Theta_0} \E_\theta[\varphi(X)] = E_{\theta_0}[\varphi(X)]
            = P_{\theta_0}[X >\theta_0] + \gamma P_{\theta_0}[X \leqslant \theta_0]$$
            y como ya sabemos que 
            $$P_{\theta_0}[X >\theta_0] = 1 - e^{-1} \quad P_{\theta_0}[X \leqslant \theta_0] = e^{-1}$$
            sacamos
            $$\alpha = (1-e^{-1}) + \gamma e^{-1}$$
            De esta manera, dado un tamaño $\alpha \in ]1-e^{-1}, 1]$, se obtiene que 
            $$\alpha = (1-e^{-1}) + \gamma e^{-1} \iff \alpha - (1-e^{-1}) = \gamma e^{-1} \iff
            \gamma = \dfrac{\alpha - (1-e^{-1})}{e^{-1}} \in [0,1]$$
            Y como es natural, si $\gamma = 0$, volvemos al TRV no aleatorizado, y si $\gamma = 1$, 
            $\alpha = 1$ también.

        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}[1,1 puntos]
        En cierta academia se pretende hacer un estudio sobre la efectividad de un curso
        intensivo de aprendizaje. Para ello, se selecciona aleatoriamente a $10$ alumnos a los
        que se somete a una misma prueba de control antes y después del curso, obteniéndose
        las siguientes calificaciones:

        \begin{center}
            \begin{tabular}{c|cccccccccc}
                A & 7,25 & 7,5 & 7,5 & 8 & 8,5 & 8,75 & 8 & 8,25 & 7,25 & 8,5 \\
                \hline
                D & 7 & 8,25 & 8 & 8 & 8,75 & 9,5 & 8,5 & 8,75 & 7,75 & 8
            \end{tabular}
        \end{center}

        ¿Se puede concluir, a la vista de los datos, que el curso de aprendizaje no afecta a
        las calificaciones de los alumnos? Especificar bajo qué condiciones sobre las variables
        consideradas se puede resolver este problema y concretar las hipótesis nula y alternativa
        que se contrastan. Resolver este problema al nivel de significación $0{.}1$ mediante el
        uso de diferentes tests, detallando las hipótesis necesarias para la aplicación de cada
        uno de ellos. \\

        Para estudiar si el curso de aprendizaje ``no afecta'' a las calificaciones de los alumnos,
        trabajamos con la variable $X = D-A$, y con la m.a.s. asociada $X_i = D_i - A_i \quad \forall i = 1, \ldots, n$, 
        con $n=10$. De esta manera, no afectar es equivalente a que la distribución de $X$ esté centrada en $0$,
        para lo cual usaremos el constraste de la mediana $M_X$. La hipótesis bilateral será
        $$\begin{cases}
            H_0 : M_X = 0 \\
            H_1 : M_X \neq 0
        \end{cases}$$
        La m.a.s. de $X$ quedaría $$(-0{.}25, 0{.}75, 0{.}5,0,0{.}25,{0.75},0{.}5,0{.}5, 0{.}5, -0{.}5)$$ Por teoría 
        sabemos que, como hay un $0$, se elimina ese dato, y se reajusta el tamaño de la muestra a $n=9$. La muestra
        resultante es entonces $$(-0{.}25, 0{.}75, 0{.}5,0{.}25,0{.}75,0{.}5,0{.}5, 0{.}5, -0{.}5)$$
        En primer lugar aplicaremos el test de los signos. 
        \section*{Test de los Signos de Fisher}
        Se supondrá que $X$ tiene una distribución continua. Hay un total de $2$ signos negativos, y $9-2 = 7$ positivos. El estadístico es 
        $$T = \text{Número de Signos i : } X_i > 0 \rightsquigarrow_{H_0} B(n,1/2)$$ 
        Entonces $T_{exp} = 7$ y ya sabíamos que $n=9$. El p-valor sabemos que
        $$p-valor = \begin{cases}
            2 P_{H_0}[T(X_1, \ldots, X_n) \leqslant T_{exp}] &\text{si } T_{exp} \leqslant n/2 \\
            2 P_{H_0}[T(X_1, \ldots, X_n) \geqslant T_{exp}] &\text{si } T_{exp} \geqslant n/2
        \end{cases}$$
        En este caso $7 = T_{exp} \geqslant n/2 = 4.5$. Obtenemos esta probabilidad usando que $T \rightsquigarrow_{H_0}
        B(9,1/2)$, luego $$P_{H_0}[T=k] = \binom{9}{k} \left( \dfrac{1}{2} \right)^k \left( \dfrac{1}{2} \right)^{9-k} 
        = \binom{9}{k} \left( \dfrac{1}{2} \right)^9 = \dfrac{\binom{9}{k}}{2^9}$$
        y
        $$P_{H_0}[T(X_1, \ldots, X_n) \geqslant 7] = P_{H_0}[T(X_1, \ldots, X_n) = 7] + 
        P_{H_0}[T(X_1, \ldots, X_n) = 8] + $$$$P_{H_0}[T(X_1, \ldots, X_n) = 9] = 
        \dfrac{\binom{9}{7} + \binom{9}{8} + \binom{9}{9}}{2^9} = \dfrac{36 + 9 + 1}{512} = \dfrac{46}{512} \approx 0.0898$$
        Entonces $p-valor = 2 \cdot P_{H_0}[T(X_1, \ldots, X_n) \geqslant 7] \approx 2 \cdot 0.0898 = 0.1796$.
        Como el nivel de significación que nos dan es $\alpha = 0.1$, concluimos que no se rechaza $H_0$ con el test
        de los signos.
        \section*{Test de los Rangos Signados de Wilcoxon}

        Se supondrá que además de ser $X$ continua, como en el caso anterior, es además simétrica (alrededor de la mediana).
        Consideramos $D_i = X_i - 0 = X_i, \quad \forall i = 1, \ldots, n=9$. Ordenamos crecientemente los valores
        $|D_i|$, y luego asignamos el lugar que ocupan o rango $r(|D_i|)$ con $i=1, \ldots, n$. Sabemos que 
        si hay empates, entonces se le asigna el promedio de los rangos a cada uno de los $|D_i|$ que producen el empate.
        Hay en total tres $|D_i|$, que son $0{.}25$, $0{.}5$ y $0{.}75$. Además
        \begin{enumerate}
            \item $0{.}25$ aparece $2$ veces, con rangos $1$ y $2$, luego a cada uno le asignamos el rango promedio $1.5$.
            \item $0{.}5$ aparece $5$ veces, con rangos $3,4,5,6$ y $7$, luego a cada uno le asignamos el rango promedio 
            $(3+4+5+6+7)/5 = 5$.
            \item $0{.}75$ aparece $2$ veces, con rangos $8$ y $9$, luego a cada uno le asignamos el rango $8.5$
        \end{enumerate}
        El estadístico de Wilcoxon es:
        $$T^{+}(X_1, \ldots, X_n) = \text{Suma de los Rangos de los $D_i > 0$}$$
        Obtenemos su valor experimental:
        $$T_{exp}^{+} = T(x_1, \ldots, x_n) = 1.5 + 4 \cdot 5 + 2 \cdot 8.5 = 38.5$$
        La distribución de $T^{+}$ es simétrica alrededor de $n(n+1)/4$ y está tabulada para $9 = n \leqslant 15$.
        En este caso $n(n+1)/4 = 9 \cdot 10 / 4 = 22.5$. Como $H_1 : M_X \neq 0$, 
        sabemos que se rechaza $H_0$ para un nivel de significación $\alpha$ si
        $$T^{+}_{exp} \leqslant k \quad \text{ó} \quad T^{+}_{exp} \geqslant \dfrac{n(n+1)}{2} - k$$
        con $k$ aquel valor tal que $P[T^{+}(X_1, \ldots, X_n) \leqslant k] \leqslant \alpha/2$.
        La cola izquierda será para valores $t \leqslant 22$, y la cola derecha para valores $t \geqslant 23$.
        En este caso, $n(n+1)/2 = 9(10)/2 = 45$, y buscamos en la cola izquierda el mayor $k$ tal que:
        $$P[T^{+}(X_1, \ldots, X_n) \leqslant k] \leqslant \alpha/2 = 0.05$$
        Usando la tabla de Wilcoxon para $n=9$:
        $$P[T^{+}(X_1, \ldots, X_n) \leqslant 8] = 0.049 \leqslant 0.05$$
        $$P[T^{+}(X_1, \ldots, X_n) \leqslant 9] = 0.064 > 0.05$$
        Por tanto, $k=8$. Como 
        $$38.5 = T^{+}_{exp} \geqslant \dfrac{n(n+1)}{2} - k = 45 - 8 = 37$$
        se rechaza $H_0$ a nivel de significación $\alpha = 0.1$.
    \end{ejercicio}

\end{document}
