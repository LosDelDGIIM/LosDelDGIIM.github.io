\documentclass[12pt]{article}

\input{../../_assets/preambulo.tex}

\newcommand{\E}{\mathrm{E}}
\newcommand{\V}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rmas}{\R^{+}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\I}{\mathrm{Im}}
\newcommand{\Rmenos}{\R^{-}}

\begin{document}

    % 1. Foto de fondo
    % 2. Título
    % 3. Encabezado Izquierdo
    % 4. Color de fondo
    % 5. Coord x del titulo
    % 6. Coord y del titulo
    % 7. Fecha

    
    \input{../../_assets/portada}
    \portadaExamen{ffccA4.jpg}{Inferencia \\ Estadística\\Examen III}{Inferencia Estadística. Examen III}{MidnightBlue}{-8}{28}{2026}{José Manuel Sánchez Varbas}

    \begin{description}
        \item[Asignatura] Inferencia Estadística.
        \item[Curso Académico] 2023-24.
        \item[Grado] Grado en Matemáticas y Grado en Ingeniería Informática y Matemáticas.
        \item[Descripción] Examen Ordinario.
        \item[Fecha] 22 de Enero de 2024.  
    \end{description}
    \newpage


    % ------------------------------------

    \begin{ejercicio}
        Sean $(X_1,\dots,X_{n_1})$ y $(Y_1,\dots,Y_{n_2})$ m.a.s. de $X$ e $Y$, variables que siguen
        $N(\mu_1,4)$ y $N(\mu_2,5)$ respectivamente.

        \begin{enumerate}
            \item[a)] Si $\mu_1=2$, $\mu_2=3$ y sean $(X_1,\dots,X_8)$ y $(Y_1,\dots,Y_{10})$ dos muestras
            de tamaño $8$ y $10$ respectivamente con medias muestrales $\overline X$, $\overline Y$,
            calcular el percentil $99$ de
            \begin{gather}
                V=\frac{\overline X-\overline Y+1}
                {\sqrt{\dfrac{\displaystyle\sum_{i=1}^8 (X_i-\overline X)^2}{4}
                +\dfrac{\displaystyle\sum_{i=1}^{10}(Y_i-\overline Y)^2}{5}}}.
            \end{gather}

            \item[b)] Calcular el intervalo de confianza para $\mu_1-\mu_2$ de menor longitud esperada
            uniformemente a nivel de confianza $1-\alpha$. ¿Cómo sería el intervalo si las varianzas 
            fueran desconocidas pero iguales?
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}~
        \begin{enumerate}
            \item[a)] Sea $X$ una v.a. con función de densidad
            \begin{gather}
                f_\theta(x)=\frac{\theta}{2}\,x^{-3/2}, \qquad x>\theta^2.
            \end{gather}
            Calcular el UMVUE y determinar para qué valores de $n$ existe.
            ¿Es eficiente?

            \item[b)] Sea
            $$
            f_\theta(x)=\theta\,T(x)e^{-\theta x}, \qquad x>0,\ \theta>0,
            $$
            en una familia regular según Fréchet--Cramér--Rao.
            \begin{enumerate}
                \item[b1)] Sabiendo que\footnote{Originalmente era $I_{X_1,\dots,X_n}(\theta)=\dfrac{2n}{\theta^2}$, pero este dato era incompatible
                con el ejercicio. Se ha optado por eliminar el $2$ para que todo salga como se espera.} $I_{X_1,\dots,X_n}(\theta)=\dfrac{n}{\theta^2}$, calcular
                $\mathrm{E}[X]$ y $\mathrm{Var}[X]$.
                \item[b2)] Sabiendo que $$\sum_{i=1}^n \dfrac{T(X_i)}{n}$$ es un estimador eficiente de $2/\theta$,
                calcular $T(x)$.
            \end{enumerate}
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}~
        \begin{enumerate}
            \item[a)] Enunciar y demostrar el Teorema de Zehna definiendo previamente los siguientes
            conceptos: función de verosimilitud de un parámetro, función de verosimilitud de una
            función paramétrica y estimador máximo verosímil para funciones paramétricas.

            \item[b)] Calcular la función de verosimilitud de $$\lambda=(\theta-1)^2$$
            asociada a una realización muestral cuyo máximo valor es $3$ si $$f_\theta(x)=e^{x-\theta}, \qquad x \leqslant \theta, \theta>0.$$
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}
        Sea $(X_1,\dots,X_n)$ una m.a.s.\ de $X$ v.a.\ con función de densidad
        \begin{gather}
            f_\theta(x)=\frac{1}{\sqrt{8\theta}\sqrt{x-1}}, \qquad 1<x<2\theta+1.
        \end{gather}
        Deducir el test más potente de tamaño arbitrario para contrastar
        $$H_0:\theta=\theta_0 \quad \text{frente a} \quad H_1:\theta=\theta_1$$
        siendo $\theta_1<\theta_0$. Calcular la potencia.
    \end{ejercicio}

    \begin{ejercicio}~
        \begin{enumerate}
            \item[a)] Test de Kolmogorov--Smirnov.
            \begin{enumerate}
                \item[i)] Plantear el problema de contraste.
                \item[ii)] Dar el valor del estadístico.
                \item[iii)] Enunciar el teorema que justifica su uso.
                \item[iv)] Ventajas frente al test $\chi^2$.
            \end{enumerate}
            \item[b)] Se cuentan el número de tutorías a lo largo de un curso por $50$ profesores.
            Se quiere contrastar si el número de tutorías por profesor se puede describir
            mediante una distribución de Poisson.

            \begin{center}
                \begin{tabular}{c|cccccc}
                    Número de tutorías & 0 & 1 & 2 & 3 & 4 & 5 \\ \hline
                    Número de profesores & 2 & 5 & 10 & 14 & 12 & 7
                \end{tabular}
            \end{center}
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \setcounter{ejercicio}{0}

    \begin{ejercicio}
        Sean $(X_1,\dots,X_{n_1})$ y $(Y_1,\dots,Y_{n_2})$ m.a.s.\ de $X$ e $Y$, variables que siguen
        $N(\mu_1,4)$ y $N(\mu_2,5)$ respectivamente.

        \begin{enumerate}
            \item[a)] Si $\mu_1=2$, $\mu_2=3$ y sean $(X_1,\dots,X_8)$ y $(Y_1,\dots,Y_{10})$ dos muestras
            de tamaño $8$ y $10$ respectivamente con medias muestrales $\overline X$, $\overline Y$,
            calcular el percentil $99$ de
            \begin{gather}
                V=\frac{\overline X-\overline Y+1}
                {\sqrt{\dfrac{\displaystyle\sum_{i=1}^8 (X_i-\overline X)^2}{4}
                +\dfrac{\displaystyle\sum_{i=1}^{10}(Y_i-\overline Y)^2}{5}}}.
                \tag{1}
            \end{gather}

            Del enunciado del problema sacamos que 
            $$X_i \rightsquigarrow N(\mu_1, 4), \quad i=1,\ldots,n_1$$
            $$Y_j \rightsquigarrow N(\mu_2,5) \quad j=1,\ldots, n_2$$
            y en este apartado, $n_1 = 8$ y $n_2 = 10$. Buscamos obtener la 
            distribución de $V$. Para ello, primero hallamos la distribución del numerador. \\

            Como $\mu_1 = 2$ y $\mu_2 = 3$, entonces 
            $$\E[\overline{X} - \overline{Y} + 1] = \E[\overline{X}] - \E[\overline{Y}] + 1 = 
            \mu_1 - \mu_2 + 1 = 2 - 3 + 1 = 0$$

            Por teoría sabemos que
            $$X \rightsquigarrow N(\mu_1, 4) \Longrightarrow \overline{X} \rightsquigarrow N \left( \mu_1, \dfrac{4}{n_1} \right)$$
            $$Y \rightsquigarrow N(\mu_2, 5) \Longrightarrow \overline{Y} \rightsquigarrow N \left( \mu_2, \dfrac{5}{n_2} \right)$$

            Así, asumiendo que $(X_1,\dots,X_{n_1})$ y $(Y_1,\dots,Y_{n_2})$ son independientes, entonces
            
            $$\overline{X} - \overline{Y} \rightsquigarrow N \left( \mu_1 - \mu_2, \dfrac{4}{n_1} + \dfrac{5}{n_2} \right)$$

            y se obtiene que
            $$\V(\overline{X} - \overline{Y} + 1) = \V(\overline{X} - \overline{Y}) = \dfrac{4}{n_1} + \dfrac{5}{n_2} = \dfrac{4}{8} + \dfrac{5}{10} = \dfrac{1}{2} + \dfrac{1}{2} = 1$$

            Deducimos entonces que $Z = \overline{X} - \overline{Y} + 1 \rightsquigarrow N(0,1)$ \\

            Falta hallar la distribución del denominador. Usaremos que 

            $$\dfrac{(n_1 - 1) S_1^2}{\sigma_1^2} = \dfrac{\displaystyle\sum_{i=1}^{n_1} (X_i - \overline{X})^2}{\sigma_1^2} \rightsquigarrow \chi^2(n_1 -1)$$
            $$\dfrac{(n_2 - 1) S_2^2}{\sigma_2^2} = \dfrac{\displaystyle\sum_{j=1}^{n_2} (Y_j - \overline{Y})^2}{\sigma_2^2} \rightsquigarrow \chi^2(n_2 -1)$$

            y como estas últimas v.a. son independientes, por serlo las m.a.s. de $X$ e $Y$, podemos aplicar la reproductividad de la distribución $\chi^2$:
            $$W = \dfrac{\displaystyle\sum_{i=1}^{n_1} (X_i - \overline{X})^2}{\sigma_1^2} + \dfrac{\displaystyle\sum_{j=1}^{n_2} (Y_j - \overline{Y})^2}{\sigma_2^2}
            \rightsquigarrow \chi^2(n_1 - 1 + n_2 - 1) = \chi^2(n_1 + n_2 - 2) = \chi^2(16)$$

            En estas condiciones puede aplicarse la construcción de la distribución $T$ de Student
            $$T = \dfrac{Z}{\sqrt{W/16}} \rightsquigarrow t(16)$$
            y basta notar que 
            $$V = \dfrac{Z}{\sqrt{W}} = \dfrac{Z}{\sqrt{16 \cdot W/16}} = \dfrac{Z}{\sqrt{16} \sqrt{W/16}} = \dfrac{1}{4} \dfrac{Z}{\sqrt{W/16}} = 
            \dfrac{1}{4} T$$

            La tabla proporcionada para la $T$ de Student cumple que 
            $$P(T \leqslant q_p) = p \iff P(T_n > t_{n,{1-p}}) = 1-p$$
            donde $q_p$ es el percentil $p \in ]0,1[$, $t_{n,1-p}$ es el valor tabulado para la fila $n$ y la columna $1-p$,
            y $T_n \stackrel{not}{\equiv} T \rightsquigarrow t(n)$. El percentil $99$ de $V$ será consecuentemente
            $$q_{0.99}(V) = \dfrac{1}{4} t_{16; 0.01} \approx \dfrac{1}{4} \cdot 2.5835 \approx 0.645875$$

            \item[b)] Calcular el intervalo de confianza para $\mu_1-\mu_2$ de menor longitud esperada
            uniformemente a nivel de confianza $1-\alpha$. ¿Cómo sería el intervalo si las varianzas 
            fueran desconocidas pero iguales? \\

            Ya sabemos que $$\overline{X} - \overline{Y} \rightsquigarrow 
            N \left( \mu_1 - \mu_2, \dfrac{4}{n_1} + \dfrac{5}{n_2} \right)$$

            Y como nos piden el intervalo de confianza (se asume que bilateral), entonces por el método del pivote 
            visto en teoría, usando como pivote 
            $$T(X_1, \ldots, X_{n_1}, Y_1, \ldots, Y_{n_2}; \mu_1 - \mu_2) = \dfrac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}
            {\sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}}} \rightsquigarrow N(0,1)$$
            este debe verificar, por definición de intervalo de confianza, lo siguiente 
            $$P_{\mu_1, \mu_2} \left( \lambda_1 < \dfrac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}
            {\sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}}} < \lambda_2 \right) \geqslant 1 - \alpha \quad \forall (\mu_1, \mu_2) \in \R^2$$

            para ciertos $\lambda_1, \lambda_2 \in \R$. Buscamos aislar la diferencia de medias poblacionales en el centro de la 
            cadena de desigualdades, luego

            $$\lambda_1 < \dfrac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}
            {\sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}}} < \lambda_2$$ $$\iff 
            \lambda_1 \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}} < (\overline{X} - \overline{Y}) - (\mu_1 - \mu_2) < 
            \lambda_2 \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}}$$ $$\iff 
            \overline{Y} - \overline{X} + \lambda_1 \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}} < 
            -(\mu_1 - \mu_2) < \overline{Y} - \overline{X} + \lambda_2 \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}} $$ $$\iff
            \overline{X} - \overline{Y} - \lambda_2 \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}} < 
            \mu_1 - \mu_2 < \overline{X} - \overline{Y} - \lambda_1 \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}}$$

            La solución para obtener el intervalo de confianza de menor longitud esperada uniformemente a nivel de confianza $1-\alpha$
            se alcanza con $\lambda_1 = - z_{\alpha/2}$, $\lambda_2 = z_{\alpha/2}$, donde $P[Z > z_{\alpha}] > \alpha$, y el intervalo
            de confianza bilateral es 
            $$\left(\overline{X} - \overline{Y} - z_{\alpha/2} \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}},
            \overline{X} - \overline{Y} + z_{\alpha/2} \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}} \right)$$
            Si se pidiera cualquiera de los dos unilaterales, entonces
            \begin{enumerate}
                \item Si $\lambda_1 = -\infty$, se sustituye el extremo superior del intervalo bilateral anterior por $+\infty$,
                y $z_{\alpha/2}$ por $z_{\alpha}$.
                \item Si $\lambda_2 = +\infty$, se sustituye el extremo inferior del intervalo bilateral anterior por $-\infty$,
                y $z_{\alpha/2}$ por $z_{\alpha}$.
            \end{enumerate}

            Si las varianzas fueran desconocidas pero iguales, entonces el pivote sería
            $$T(X_1, \ldots, X_{n_1}, Y_1, \ldots, Y_{n_2}; \mu_1 - \mu_2) = \dfrac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}
            {S_p \sqrt{\dfrac{1}{n_1} + \dfrac{1}{n_2}}} \rightsquigarrow t(n_1 + n_2 - 2)$$
            donde $$S_p^2 = \dfrac{(n_1 - 1) S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}$$
            y como la $T$ de Student tiene las mismas propiedades de simetría respecto al origen que $N(0,1)$, entonces
            el intervalo de confianza bilateral a nivel de confianza $1 - \alpha$ sería
            $$\left(\overline{X} - \overline{Y} - t_{n_1+n_2-2;\alpha/2} S_p \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}},
            \overline{X} - \overline{Y} + t_{n_1+n_2-2;\alpha/2} S_p \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}} \right)$$
            donde $P[T_{n_1+n_2-2} > t_{n_1+n_2-2;\alpha}] = \alpha$ y nuevamente si se pidiera cualquiera de los dos unilaterales, entonces
            \begin{enumerate}
                \item Si $\lambda_1 = -\infty$, se sustituye el extremo superior del intervalo bilateral anterior por $+\infty$,
                y $t_{n_1+n_2-2;\alpha/2}$ por $t_{n_1+n_2-2;\alpha}$.
                \item Si $\lambda_2 = +\infty$, se sustituye el extremo inferior del intervalo bilateral anterior por $-\infty$,
                y $t_{n_1+n_2-2;\alpha/2}$ por $t_{n_1+n_2-2;\alpha}$.
            \end{enumerate}
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}~
        \begin{enumerate}
            \item[a)] Sea $X$ una v.a. con función de densidad
            \begin{gather}
                f_\theta(x)=\dfrac{\theta}{2} x^{-3/2}, \qquad x>\theta^2.
                \tag{2}
            \end{gather}
            Calcular el UMVUE y determinar para qué valores de $n$ existe.
            ¿Es eficiente? \\

            Buscamos obtener el UMVUE mediante el método alternativo visto en teoría. Para ello, en primer lugar hay que encontrar un estadístico suficiente y completo $T$, y luego una función del estadístico $h(T)$
            (denotaremos indistintamente $T \stackrel{not}{\equiv} T(X_1, \ldots, X_n)$, para una m.a.s. $(X_1, \ldots, X_n)$ con $n \in \N$ fijo) insesgada en 
            $g(\theta) = \theta$ (como no se especifica de quién es el UMVUE, se asume que del parámetro), estimadora y con momento de segundo orden finito. Entonces $h(T)$ será el UMVUE. \\

            El estadístico suficiente se calcula por medio del Teorema de Factorización de Neyman-Fisher. La función conjunta es la siguiente
            $$f^n_\theta(x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^{n} f_\theta(x_i)$$
            Suponemos en este punto que $\theta > 0$ (de lo contrario, $f_\theta(x_i) = 0 \quad \forall i = 1, \ldots, n$), y 
            vemos que $$x_i > \theta^2 \quad \forall i = 1, \ldots, n \iff I_{\Rmas}(x_i - \theta^2) = 1 \quad \forall 
            i = 1, \ldots, n \iff I_{\Rmas}(x_{(1)} - \theta^2) = 1$$ de donde se deduce que 
            $$f^n_\theta(x_1, \ldots, x_n) = \prod_{i=1}^{n} \dfrac{\theta}{2} x_i^{-3/2} I_{\Rmas}(x_{(1)} - \theta^2) = 
            \left(\dfrac{\theta}{2} \right)^{n} \prod_{i=1}^{n} x_i^{-3/2} I_{\Rmas}(x_{(1)} - \theta^2)$$
            Tomando $T(X_1, \ldots, X_n) = X_{(1)}$ y 
            $$h(x_1, \ldots, x_n) = \prod_{i=1}^{n} x_i^{-3/2}, \quad g_\theta(t) = 
            \left(\dfrac{\theta}{2} \right)^{n} I_{\Rmas}(t - \theta^2)$$
            Se cumple que 
            $$f^n_\theta(x_1, \ldots, x_n) = h(x_1, \ldots, x_n) g_\theta(T(x_1, \ldots, x_n)) \quad \forall (x_1, \ldots, x_n) \in \X^n$$
            donde $h$ es independiente del parámetro $\theta$ y $g_\theta$ depende de la muestra solo a través del estadístico, luego,
            por el Teorema de Factorización de Neyman-Fisher, el estadístico $T$ es suficiente. 

            \newpage

            Ahora, hay que comprobar que este estadístico es completo, lo cual se hará por definición. 
            Sabemos por teoría que la distribución del mínimo es 
            $$F_T(t) = 1 - (1 -F_X(t))^n \Longrightarrow f_T(t) = n(1-F_X(t))^{n-1} f_\theta(t)$$
            Hallamos ahora la función de distribución de $X$:
            $$F_X(t) = \int_{\theta^2}^{t} f_\theta(x) dx = \int_{\theta^2}^{t} \dfrac{\theta}{2} x^{-3/2} dx = 
            \dfrac{\theta}{2} \int_{\theta^2}^{t} x^{-3/2} dx = \dfrac{\theta}{2} \cdot 
            \left[ \dfrac{x^{-1/2}}{-1/2} \right]_{\theta^2}^{t} = $$
            $$\dfrac{\theta}{2} \cdot (-2\left(t^{-1/2} - (\theta^2)^{-1/2} \right)) = 
            - \theta \left( \dfrac{1}{\sqrt{t}} - \dfrac{1}{\theta} \right) = -\theta \dfrac{\theta - \sqrt{t}}{\theta \sqrt{t}} = $$
            $$\dfrac{\sqrt{t} - \theta}{\sqrt{t}} = 1 - \dfrac{\theta}{\sqrt{t}} \quad t > \theta^2$$

            La función de densidad del estadístico será entonces
            $$f_T(t) = n(1-F_X(t))^{n-1} f_\theta(t) = 
            n \left(\dfrac{\theta}{\sqrt{t}}\right)^{n-1} \dfrac{\theta}{2} t^{-3/2} = $$
            $$\dfrac{n \theta^n}{2} \dfrac{t^{-3/2}}{t^{(n-1)/2}} = \dfrac{n \theta^n}{2} t^{-(n/2+1)} \quad t > \theta^2$$

            Sea $h$ una función medible verificando $$0 = \E[h(T)] \stackrel{def}{=} \int_{\theta^2}^{+\infty} h(t) f_T(t) dt = 
            \int_{\theta^2}^{+\infty} h(t) \dfrac{n \theta^n}{2} t^{-(n/2+1)} dt =$$
            $$\dfrac{n \theta^n}{2} \int_{\theta^2}^{+\infty} h(t) t^{-(n/2+1)} dt$$
            como $\dfrac{n \theta^n}{2} \neq 0 \quad \forall n \in \N, \quad \forall \theta>0$, debe ser
            $$\int_{\theta^2}^{+\infty} h(t) t^{-(n/2+1)} dt = 0$$
            Por el Teorema Fundamental del Cálculo, podemos considerar una primitiva $H(t)$ del 
            integrando $h(t) t^{-(n/2+1)}$, y esta cumple,
            por la Regla de Barrow, que $\lim\limits_{m \to +\infty} H(m) - H(\theta^2) = 0 \quad \forall \theta > 0$. 
            Derivando respecto de $\theta$, se obtiene que
            $$-\dfrac{d}{d \theta} H(\theta^2) = 0 \iff -h(\theta^2) (\theta^2)^{-(n/2+1)} (2 \theta) = 0 
            \iff -2\theta^{-(n+1)}h(\theta^2) = 0 \stackrel{(*)}{\iff} h(\theta^2) = 0$$
            donde en $(*)$ se ha usado que $-2\theta^{-(n+1)} \neq 0$ por ser $\theta > 0$, $n \in \N$ y $-2 \neq 0$. 
            Equivalentemente, 
            $$\forall \theta \in \Theta = \left]0, + \infty \right[ \quad h(\theta^2) = 0 \iff h(t) = 0 \quad \forall t > 0$$ 
            (tomando $t = \theta^2 \in \left]0, +\infty \right[$).
            Por tanto $$\left]0, +\infty \right[ \subseteq \{t : h(t) = 0\}$$ y consecuentemente 
            $$1 \geqslant P[h(T) = 0] \geqslant P[T > 0] = 1 \Longrightarrow P[h(T) = 0] = 1$$

            y entonces por definición concluimos que $T$ es un estadístico completo. Tenemos entonces en este punto
            que $T$ es un estadístico suficiente y completo. 

            \newpage

            Ahora hay que buscar un estimador insesgado en $g(\theta)$ y de segundo orden finito. 
            Sea $h$ (independiente de la anterior) función medible tal que
            $$\theta = g(\theta) = \E[h(T)] = \dfrac{n \theta^n}{2} \int_{\theta^2}^{+\infty} h(t) t^{-(n/2+1)} dt \iff$$
            $$\int_{\theta^2}^{+\infty} h(t) t^{-(n/2+1)} dt = \dfrac{2 \theta}{n \theta^n} = \dfrac{2}{n \theta^{n-1}} = 
            \dfrac{2}{n} \theta^{1-n} $$

            Derivamos respecto de $\theta$ a ambos lados e igualamos. El miembro izquierdo ya lo tenemos por el apartado anterior:
            $$-2\theta^{-(n+1)}h(\theta^2)$$
            y el derecho es 
            $$\dfrac{2}{n} (1-n) \theta^{1 - n - 1} = \dfrac{2(1-n)}{n} \theta^{-n}$$
            Despejamos $h(\theta^2)$:
            $$-2\theta^{-(n+1)}h(\theta^2) = \dfrac{2(1-n)}{n} \theta^{-n} \iff $$
            $$h(\theta^2) = \dfrac{2(1-n)}{-2\theta^{-(n+1)}} \theta^{-n} = \dfrac{n-1}{n} 
            \dfrac{\theta^{-n}}{\theta^{-(n+1)}} = \dfrac{n-1}{n} \theta$$
            de donde
            $$h(t) = \dfrac{n-1}{n} \sqrt{t}$$
            Por construcción $h(T)$ es insesgada en $g(\theta)$. Vemos que $h(T)$ también es estimador de $g(\theta)$, 
            pues $\Theta = \left] 0, +\infty \right[$, y $g(\theta) = \theta \Longrightarrow g(\Theta) = \left]0,+\infty \right[$. 
            Como $T = X_{(1)} > \theta^2 > 0$ y $\frac{n-1}{n} > 0$ si $n \geq 2$, luego $h(T) > 0$ si $n \geq 2$. Queda comprobar que tiene momento de segundo orden finito. \\

            Ello se cumplirá en caso de que $E[h(T)^2] < + \infty$:
            $$E[h(T)^2] \stackrel{def}{=} \int_{\theta^2}^{+\infty} h(t)^2 f_T(t) dt = \int_{\theta^2}^{+\infty} 
            \left( \dfrac{n-1}{n} \right)^2 t \dfrac{n \theta^n}{2} t^{-(n/2+1)} dt = $$
            $$\dfrac{\theta^n(n-1)^2}{2n} \int_{\theta^2}^{+\infty} t^{-n/2} dt = 
            \dfrac{\theta^n(n-1)^2}{2n} \left[t^{-n/2+1} \right]_{\theta^2}^{+\infty} = $$
            $$\dfrac{\theta^n(n-1)^2}{2n} \left( \lim\limits_{m \to +\infty} m^{-n/2+1} - \theta^{-n/2+1} \right)$$

            Y vemos que $$E[h(T)^2] < + \infty \iff \lim\limits_{m \to +\infty} m^{-n/2+1} < +\infty \iff
            -\dfrac{n}{2} + 1 < 0 \iff$$$$ -\dfrac{n}{2} < -1 \iff
            -n < -2 \iff n > 2 \iff n \geq 3$$ ya que si $n < 3$, es decir $n \in \{1,2\}$:
            \begin{enumerate}
                \item Si $n=1$
                $$\lim\limits_{m \to +\infty} m^{-n/2+1} = \lim\limits_{m \to +\infty} m^{-1/2+1} = 
                \lim\limits_{m \to +\infty} \sqrt{m} = + \infty$$
                \item Si $n=2$
                $$\int_{\theta^2}^{+\infty} t^{-n/2} dt = \int_{\theta^2}^{+\infty} t^{-1} dt = 
            \int_{\theta^2}^{+\infty} \dfrac{1}{t} dt = \ln[t]_{\theta^2}^{+\infty}$$
            \end{enumerate} 
            y en ambos casos el momento de segundo orden no sería finito (aparte de no ser $h(T)$ estimador si $n=1$ por lo estudiado
            antes). Por tanto, por el Teorema de Lehmann-Scheffé, $E[h(T)/T] = h(T)$ es el UMVUE para $g(\theta)$, y existe siempre y
            cuando $n>2 \iff n \geq 3$. \\

            Respecto a la eficiencia, sabemos por un corolario visto en teoría que solo existen estimadores eficientes para familias
            de tipo exponencial (además de regulares en el sentido de Fréchet--Cramér--Rao), pero esta familia no puede serlo, 
            ya que $\X = \left]-\theta^2, +\infty \right[$, es decir, el conjunto de valores de la variable depende de $\theta$,
            luego ya no tendríamos que la familia es de tipo exponencial. Consecuentemente, el UMVUE obtenido 
            no verifica la definición de estimador eficiente, definido a su vez sobre familias regulares, por lo que 
            no es eficiente.


            \item[b)] Sea $$ f_\theta(x)=\theta T(x)e^{-\theta x}, \qquad x>0,\ \theta>0,$$
            en una familia regular según Fréchet--Cramér--Rao.
            \begin{enumerate}
                \item[b1)] Sabiendo que\footnote{Originalmente era $I_{X_1,\dots,X_n}(\theta)=\dfrac{2n}{\theta^2}$, pero este dato era incompatible
                con el ejercicio. Se ha optado por eliminar el $2$ para que todo salga como se espera.} $I_{X_1,\dots,X_n}(\theta)=\dfrac{n}{\theta^2}$, calcular
                $\mathrm{E}[X]$ y $\mathrm{Var}[X]$. \\

                Por ser regular según FCR, sabemos que se verifica que $$\E_\theta \left[\dfrac{\partial \ln f_\theta(x)}{\partial \theta}\right] = 0$$
                Así
                $$\ln f_\theta(x) = \ln \theta + \ln (T(x)) - \theta x \Longrightarrow \dfrac{\partial \ln f_\theta(x)}{\partial \theta} = \dfrac{1}{\theta} - x$$
                La esperanza de $X$ puede obtenerse ya
                $$\E_\theta \left[\dfrac{1}{\theta} - X \right] = \E_\theta \left[\dfrac{\partial \ln f_\theta(x)}{\partial \theta}\right] = 0 
                \iff \E_\theta \left[\dfrac{1}{\theta} \right] - E[X] = 0 \iff E[X] = \dfrac{1}{\theta}$$
                
                Como $$\V_\theta \left[ \dfrac{\partial \ln f_\theta(x)}{\partial \theta} \right] = I_X(\theta)$$ usando la aditividad de la función de información de Fisher respecto a la v.a. y respecto a la m.a.s. de la v.a., 
                deducimos que $$I_{X_1, \ldots, X_n}(\theta) = n I_X(\theta) \iff \dfrac{n}{\theta^2} = n I_X(\theta) \iff I_X(\theta) = \dfrac{1}{\theta^2}$$
                de donde 
                $$\V_\theta \left[ \dfrac{1}{\theta} - X \right] = \V_\theta(X) = I_X(\theta) = \dfrac{1}{\theta^2}$$

                \item[b2)] Sabiendo que $$\sum_{i=1}^n \dfrac{T(X_i)}{n}$$ es un estimador eficiente de $2/\theta$, calcular $T(x)$. \\
                
                Buscamos aplicar el Teorema de Caracterización de Estimadores Eficientes. Para ello, obtenemos la función conjunta de la m.a.s. de $X$

                $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep}{=} \prod_{i=1}^n f_\theta(x_i)$$

                Se supondrá a partir de ahora que $x_i \in \Rmas \quad \forall i=1,\ldots,n$ y $\theta \in \Rmas$. De lo contrario, $f_\theta(x_i) = 0 \quad \forall i = 1, \ldots, n$.
                $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep}{=} \prod_{i=1}^n f_\theta(x_i) = \prod_{i=1}^n \theta T(x_i)e^{-\theta x_i} = \theta^n \prod_{i=1}^n T(x_i)e^{-\theta x_i}$$ 

                $$\ln f_{\theta}^n (x_1, \ldots, x_n) = n \ln \theta + \sum_{i=1}^n (\ln T(x_i) - \theta x_i)$$
                $$\dfrac{\partial \ln f_{\theta}^n (X_1, \ldots, X_n)}{\partial \theta} = \dfrac{n}{\theta} - \sum_{i=1}^n X_i = -\dfrac{n}{2} \cdot \left( \sum_{i=1}^n \dfrac{2X_i}{n} - \dfrac{2}{\theta} \right)$$

                Sea ahora $g(\theta) = 2 / \theta$. Denotamos $\overline{T} \stackrel{not}{\equiv} \sum_{i=1}^n T(X_i)/n$. Como $g(\theta)$ es una función paramétrica derivable y estrictamente monótona
                ($g'(\theta) \neq 0 \quad \forall \theta \in \Theta = \Rmas$), el enunciado nos dice que la familia es regular, y 
                $0 < I_X(\theta) = 2 / \theta^2 < +\infty \quad \forall \theta \in \Theta$, por el Teorema de Caracterización de Estimadores Eficientes, sabemos que $\overline{T}$ es 
                eficiente si y solo si $\forall \theta \in \Theta
                \quad \exists a(\theta) \neq 0$ tal que 
                $$P_\theta \left[ \dfrac{\partial \ln f_\theta^n(X_1, \ldots, X_n)}{\partial \theta} = a(\theta)[\overline{T}(X_1, \ldots, X_n) - g(\theta)]\right] = 1$$
                y
                $$I_{(X_1, \ldots, X_n)}(\theta) = a(\theta) g'(\theta)$$
                Como 
                $$\dfrac{\partial \ln f_\theta^n(X_1, \ldots, X_n)}{\partial \theta} = a(\theta)[\overline{T}(X_1, \ldots, X_n) - g(\theta)] \iff$$
                $$-\dfrac{n}{2} \cdot \left( \sum_{i=1}^n \dfrac{2X_i}{n} - \dfrac{2}{\theta} \right) = a(\theta) [\overline{T}(X_1, \ldots, X_n) - g(\theta)]$$
                claramente por comparación se obtiene que 
                $$\sum_{i=1}^n \dfrac{T(X_i)}{n} = \overline{T}(X_1, \ldots, X_n) = \sum_{i=1}^{n} \dfrac{2X_i}{n}, \quad g(\theta) = \dfrac{2}{\theta}, \quad a(\theta) = -\dfrac{n}{2}$$
                Es claro que $\overline{T}(X_1, \ldots, X_n) = \sum_{i=1}^n 2X_i/n$ es un estimador, pues $\Theta = \Rmas$, $g(\Theta) = \Rmas$, y $T(x_1, \ldots, x_n) > 0 \quad \forall (x_1, \ldots, x_n) \in \X^n = (\Rmas)^n$
                y $n > 0 \quad \forall n \in \N$. Además, tanto 
                $a(\theta)$ como $g(\theta)$ verifican todas las condiciones del teorema, pues $$g'(\theta) = 2 \cdot (\theta^{-1})' = 2 \cdot (- \theta^{-2}) = -\dfrac{2}{\theta^2} < 0$$ porque $2, \theta > 0$
                y $a(\theta) = -n/2 < 0 \quad \forall n \in \N$, en particular, $a(\theta) \neq 0 \quad \forall \theta \in \Theta \quad \forall n \in \N$, y usando la aditividad de la función de información de Fisher, obtenemos
                $$\dfrac{n}{\theta^2} = I_{(X_1, \ldots, X_n)}(\theta) = a(\theta) g'(\theta) = -\dfrac{n}{2} \left( -\dfrac{2}{\theta^2} \right) = \dfrac{n}{\theta^2}$$

                Por comparación directa $$\sum_{i=1}^n \dfrac{T(X_i)}{n} = \overline{T}(X_1, \ldots, X_n) = \sum_{i=1}^{n} \dfrac{2X_i}{n} \iff T(X_i) = 2 X_i$$
                de donde $T(x) = 2x$.
            \end{enumerate}
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}~
        \begin{enumerate}
            \item[a)] Enunciar y demostrar el Teorema de Zehna definiendo previamente los siguientes
            conceptos: función de verosimilitud de un parámetro, función de verosimilitud de una
            función paramétrica y estimador máximo verosímil para funciones paramétricas. \\
            \begin{definicion}[Función de Verosimilitud de un Parámetro]\label{definicion:d1}
                Sea $X$ una v.a. con distribución en una familia paramétrica de distribuciones, $\{F_\theta, \theta \in \Theta\}$. Sea
                $f_\theta(x)$ la f.m.p. (caso discreto) ó la f.d.d. (caso continuo) de $X$. Se considera $X_1, \ldots, X_n$ una m.a.s. de $X$ y 
                sea $f_\theta^n(x_1, \ldots, x_n)$ su f.m.p. ó f.d.d. (respectivamente) conjunta con $\theta \in \Theta$. Para cada $x_1, \ldots, x_n$
                realización muestral, se define la \textit{función de verosimilitud} asociada a dichos valores de la muestra como una función de $\theta$
                de la siguiente forma: 
                \Func{L_{x_1, \ldots, x_n}}{\Theta}{\Rmas \cup \{0\}}{\theta}{L_{x_1, \ldots, x_n}(\theta) = f_\theta^n(x_1, \ldots, x_n)}
            \end{definicion}

            \begin{definicion}[Función de Verosimilitud de una Función Paramétrica]\label{definicion:d2}
                Sea $g: \Theta \to \Lambda$ una función paramétrica. En el contexto de la Definición \ref{definicion:d1}, para cada $x_1, \ldots, x_n$
                realización muestral, se define la \textit{función de verosimilitud} de $\lambda = g(\theta)$ asociada a dicha realización como: 
                \Func{M_{x_1, \ldots, x_n}}{\Lambda}{\Rmas \cup \{0\}}{\lambda}{M_{x_1, \ldots, x_n}(\lambda) = \sup_{\theta \in g^{-1}(\lambda)} L_{x_1, \ldots, x_n}(\theta)}
            \end{definicion}

            \begin{definicion}[Estimador Máximo Verosímil para Funciones Paramétricas]\label{definicion:d3}
                Un estimador $\hat{\lambda}(X_1, \ldots, X_n)$ de $\lambda$ es \textit{estimador de máxima verosimilitud} (EMV) de $\lambda$ si:
                $$\forall (x_1, \ldots, x_n) \in \X^n, \quad M_{x_1, \ldots, x_n}(\hat{\lambda}(x_1, \ldots, x_n)) = \max_{\lambda \in \Lambda} M_{x_1, \ldots,
                x_n} (\lambda)$$
            \end{definicion}

            \begin{teo}[de Invarianza de Zehna]\label{teo:t1}
                Sea $X$ una v.a. con distribución en una familia paramétrica de distribuciones, $\{F_\theta, \theta \in \Theta\}$. Sea $X_1, \ldots, X_n$
                una m.a.s. de $X$. Sea $g$ una función medible. Si $\hat{\theta}(X_1, \ldots, X_n)$ es EMV de $\theta$, entonces 
                $g(\hat{\theta}(X_1, \ldots, X_n))$ es EMV de $g(\theta)$.
            \end{teo}

            \begin{proof}
                Sea $\lambda = g(\theta)$ y, fijada una realización muestral, $(x_1, \ldots, x_n) \in \X^n$, notemos 
                $\hat{\lambda} \equiv g(\hat{\theta}(x_1, \ldots, x_n))$ (de esta manera, $\hat{\theta}(x_1, \ldots, x_n) \in g^{-1}(\hat{\lambda})$). \\

                Obtenemos la función de verosimilitud de la función paramétrica usando la Definición \ref{definicion:d2}:
                $$M_{x_1, \ldots, x_n} (\hat{\lambda}) \stackrel{def}{=} \sup_{\theta \in g^{-1}(\hat{\lambda})} L_{x_1, \ldots, x_n}(\theta) 
                \stackrel{\hat{\theta} \in g^{-1}(\hat{\lambda})}{=} L_{x_1, \ldots, x_n} (\hat{\theta}(x_1, \ldots, x_n)) \quad (*)$$
                donde en la última igualdad se ha usado que $\hat{\theta}(X_1, \ldots, X_n)$ es EMV de $\theta$, luego maximiza la función de 
                verosimilitud de la Definición \ref{definicion:d1}. Ahora, vemos que 
                $$\forall \lambda \in \Lambda, \quad M_{x_1, \ldots, x_n} (\lambda) = \sup_{\theta \in g^{-1}(\lambda)} L_{x_1, \ldots, x_n}(\theta) \leqslant
                \sup_{\theta \in \Theta} L_{x_1, \ldots, x_n}(\theta) = L_{x_1, \ldots, x_n} (\hat{\theta}(x_1, \ldots, x_n)) \quad (**)$$
                la última igualdad nuevamente por ser $\hat{\theta}(X_1, \ldots, X_n)$ es EMV de $\theta$. Deducimos entonces que 
                $$\forall \lambda \in \Lambda, \quad M_{x_1, \ldots, x_n} (\lambda) \stackrel{(**)}{\leqslant} L_{x_1, \ldots, x_n} (\hat{\theta}(x_1, \ldots, x_n))
                \stackrel{(*)}{=} M_{x_1, \ldots, x_n} (\hat{\lambda}) $$
                Es decir, $\hat{\lambda} = g(\hat{\theta}(x_1, \ldots, x_n))$ maximiza $M_{x_1, \ldots, x_n}$, para cada $(x_1, \ldots, x_n) \in \X^n$. 
                Por la Definición \ref{definicion:d3}, $g(\hat{\theta}(X_1, \ldots, X_n))$ es el EMV de $\lambda = g(\theta)$.
            \end{proof}

            \item[b)] Calcular la función de verosimilitud de $$\lambda=(\theta-1)^2$$
            asociada a una realización muestral cuyo máximo valor es $3$ si $$ f_\theta(x)=e^{x-\theta}, \qquad x \leqslant \theta, \theta>0.$$
            Vemos que $\X = \left]-\infty, \theta \right]$. Sea $(x_1, \ldots, x_n) \in \X^n$ tal que $x_{(n)} = 3$. Calculamos la función de densidad 
            conjunta, asumiendo a partir de ahora que $\theta > 0$ (en otro caso, $f_\theta(x) = 0$).
            $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^n f_\theta(x_i)$$
            Ahora, vemos que $x \leqslant \theta \iff I_{\R_0^{-}}(x - \theta) = 1$ de donde se deduce que 
            $$\prod_{i=1}^n f_\theta(x_i) = \prod_{i=1}^n e^{x_i - \theta} \neq 0 \iff I_{\R_0^{-}}(x_i - \theta) = 1 \quad \forall i = 1, \ldots, n$$
            y a su vez
            $$x_i \leqslant \theta \quad \forall i = 1, \ldots, n \iff x_{(n)} \leqslant \theta \iff I_{\R_0^{-}}(x_{(n)} - \theta) = 1$$
            luego
            $$f_\theta^n(x_1, \ldots, x_n) = \prod_{i=1}^n e^{x_i - \theta} I_{\R_0^{-}}(x_{(n)} - \theta) = 
            e^{\left(\displaystyle\sum_{i=1}^n x_i - n \theta \right)} I_{\R_0^{-}}(x_{(n)} - \theta)$$
            Por la Definición \ref{definicion:d1}
            $$L_{x_1, \ldots, x_n}(\theta) = f_\theta^n(x_1, \ldots, x_n) = e^{\left(\displaystyle\sum_{i=1}^n x_i - n \theta \right)} I_{\R_0^{-}}(x_{(n)} - \theta)
            \quad \forall \theta \in \Theta = \Rmas$$
            El enunciado nos dice que $\lambda = g(\theta) = (\theta-1)^2$. Por la Definición \ref{definicion:d2}
            $$M_{x_1, \ldots, x_n}(\lambda) = \sup_{\theta \in g^{-1}(\lambda)} L_{x_1, \ldots, x_n}(\theta)$$ 
            Resolvemos $(\theta-1)^2 = \lambda$ para expresar $\theta$ explícitamente en función de $\lambda$. Vemos que $\lambda \geqslant 0$, y 
            $(\theta - 1)^2 = \lambda \iff \theta - 1 = \pm \sqrt{\lambda} \iff \theta = 1 \pm \sqrt{\lambda}$. \\
            A priori habría dos candidatos para cada $\lambda$. Sin embargo, por restricciones del problema, $3 = x_{(n)} \leqslant \theta$, lo que implica que
            $$\theta = 1 + \sqrt{\lambda} \geqslant 3 \iff \sqrt{\lambda} \geqslant 2 \iff \lambda \geqslant 4$$
            $$\theta = 1 - \sqrt{\lambda} \geqslant 3 \iff - \sqrt{\lambda} \geqslant 2 \iff \sqrt{\lambda} \leqslant -2$$
            La última opción no puede darse por ser $\lambda \geqslant 0$, por tanto, nos quedamos con la primera. Así, si $\lambda \geqslant 4$ (en otro caso,
            $M_{x_1, \ldots, x_n}(\lambda) = 0$)
            $$M_{x_1, \ldots, x_n}(\lambda) = L_{x_1, \ldots, x_n} (1 + \sqrt{\lambda}) = e^{\left(\displaystyle\sum_{i=1}^n x_i - n (1 + \sqrt{\lambda}) \right)}$$
            Aunque no se pide, como $e^{\sum_{i=1}^n x_i}$ es fijo, y $1 + \sqrt{\lambda}$ es creciente como función de $\lambda$ y 
            $e^{-n(1+\sqrt{\lambda})}$ es decreciente como función de $\lambda$, y $M_{x_1, \ldots, x_n}(\lambda) \neq 0 \iff \lambda \in [4, +\infty[$, entonces
            el máximo se alcanza en el extremo inferior del intervalo, es decir $\hat{\lambda}(x_1, \ldots, x_n) = 4$. Esto puede comprobarse también con el 
            Teorema \ref{teo:t1} pues $L_{x_1, \ldots, x_n}(\theta)$ es decreciente en $\left[x_{(n)}, +\infty \right[ = 
            \left[3, + \infty \right[$. Por el mismo razonamiento, $\hat{\theta}(x_1, \ldots, x_n) = 3$, luego $\hat{\lambda} = (\hat{\theta} - 1)^2 = (3-1)^2 = 4$.
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}
        Sea $(X_1,\dots,X_n)$ una m.a.s.\ de $X$ v.a.\ con función de densidad
        \begin{gather}
            f_\theta(x)=\dfrac{1}{\sqrt{8\theta}\sqrt{x-1}}, \qquad 1<x<2\theta+1.
            \tag{3}
        \end{gather}
        Deducir el test más potente de tamaño arbitrario para contrastar
        $$H_0:\theta=\theta_0 \quad \text{frente a} \quad H_1:\theta=\theta_1$$
        siendo $\theta_1<\theta_0$. Calcular la potencia. \\

        Tenemos un contraste de hipótesis simple frente a hipótesis simple, por lo que sabemos por el Lema de Neyman-Pearson que el 
        Test de Neyman-Pearson será el más potente de tamaño $\alpha$, de la forma
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \lambda(X_1, \ldots, X_n) > k \\
            \gamma &\text{si } \lambda(X_1, \ldots, X_n) = k \\
            0 &\text{si } \lambda(X_1, \ldots, X_n) < k
        \end{cases}$$
        para ciertos $\gamma \in [0,1]$, $k \in \R$ y 
        $$\lambda(X_1, \ldots, X_n) = \dfrac{f_1^n(X_1, \ldots, X_n)}{f_0^n(X_1, \ldots, X_n)}$$

        Definimos el espacio muestral y el espacio paramétrico, en ambos casos dependiente del parámetro
        $\theta$. Si estamos en $H_0$, entonces $\X_0 = \left]1, 2\theta_0 + 1 \right[$, y $\Theta_0 = 
        \{\theta_0\}$. Análogamente, si estamos en $H_1$, entonces $\X_1 = \left]1, 2\theta_1 + 1 \right[$, y $\Theta_1 = 
        \{\theta_1\}$. Tenemos entonces que $$\Theta = \Theta_0 \cup \Theta_1 = \{\theta_0, \theta_1\}$$ y como
        $\theta_1 < \theta_0 \iff 2\theta_1 + 1 < 2\theta_0 + 1$, entonces $\left]1, 2\theta_1 + 1 \right[ \subset \left]1, 2\theta_0 + 1 \right[$ y
        $$\X = \X_0 \cup \X_1 = \left]1, 2\theta_0 + 1 \right[ \cup \left]1, 2\theta_1 + 1 \right[ = \left]1, 2\theta_0 + 1 \right[$$
        Consecuentemente $$\X^n = \{(x_1, \ldots, x_n) \in \R^n : 1 < x_i < 2\theta_0 + 1 \quad \forall i = 1, \ldots, n\} = $$
        $$\{(x_1, \ldots, x_n) \in \R^n : 1 < x_{(1)} \quad \land \quad x_{(n)} < 2\theta_0 + 1\}$$

        Podemos considerar entonces $(x_1, \ldots, x_n) \in \X^n$, y obtener la función conjunta $f_\theta^n(x_1, \ldots, x_n)$, que es 
        $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^n \dfrac{1}{\sqrt{8\theta}\sqrt{x_i-1}}$$ 
        Y vemos que $$x<2\theta+1 \iff I_{\Rmenos}(x - (2\theta + 1)) = 1$$
        luego $$x_i < 2 \theta + 1 \quad \forall i = 1, \ldots, n \iff x_{(n)} < 2 \theta + 1 \iff I_{\Rmenos}(x_{(n)} - (2 \theta + 1)) = 1$$
        Por tanto $$f_\theta^n(x_1, \ldots, x_n) = \prod_{i=1}^n \dfrac{1}{\sqrt{8\theta}\sqrt{x_i-1}} I_{\Rmenos}(x_{(n)} - (2\theta + 1)) = $$
        $$(8\theta)^{-n/2} \prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}} I_{\Rmenos}(x_{(n)} - (2\theta + 1)) =
        8^{-n/2}\theta^{-n/2} \prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}} I_{\Rmenos}(x_{(n)} - (2\theta + 1))$$
        Se tiene entonces que
        $$f_0^n(x_1, \ldots, x_n) = 8^{-n/2}\theta_0^{-n/2} \prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}} I_{\Rmenos}(x_{(n)} - (2\theta_0 + 1)) = $$
        $$8^{-n/2}\theta_0^{-n/2} \prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}} \quad \forall (x_1, \ldots, x_n) \in \X^n$$
        $$f_1^n(x_1, \ldots, x_n) = 8^{-n/2}\theta_1^{-n/2} \prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}} I_{\Rmenos}(x_{(n)} - (2\theta_1 + 1)) \quad \forall (x_1, \ldots, x_n) \in \X^n$$
        y podemos obtener $\lambda(x_1, \ldots, x_n)$, con $(x_1, \ldots, x_n) \in \X^n$: 
        $$\lambda(x_1, \ldots, x_n) = \dfrac{f_1^n(x_1, \ldots, x_n)}{f_0^n(x_1, \ldots, x_n)} = $$$$
        \dfrac{8^{-n/2}\theta_1^{-n/2}}{8^{-n/2}\theta_0^{-n/2}} \dfrac{\displaystyle\prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}} I_{\Rmenos}(x_{(n)} - (2\theta_1 + 1))}
        {\displaystyle\prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}}} = 
        \dfrac{\begin{cases} 8^{-n/2}\theta_1^{-n/2} \displaystyle\prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}} &\text{si } x_{(n)} < 2\theta_1 + 1 \\ 0 &\text{si } x_{(n)} \geq 2\theta_1 + 1 \end{cases}}
        {8^{-n/2}\theta_0^{-n/2} \displaystyle\prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}}}$$

        Ahora, si $x_{(n)} < 2\theta_1 + 1$, simplificamos

        $$\dfrac{\cancel{8^{-n/2}}\theta_1^{-n/2}}{\cancel{8^{-n/2}}\theta_0^{-n/2}} \dfrac{\cancel{\displaystyle\prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}}}}
        {\cancel{\displaystyle\prod_{i=1}^n \dfrac{1}{\sqrt{x_i-1}}}} = \left( \dfrac{\theta_1}{\theta_0} \right)^{-n/2} = \left( \dfrac{\theta_0}{\theta_1} \right)^{n/2}$$

        de donde
        $$\lambda(x_1, \ldots, x_n) = \dfrac{f_1^n(x_1, \ldots, x_n)}{f_0^n(x_1, \ldots, x_n)} = 
        \begin{cases} 
            \left( \dfrac{\theta_0}{\theta_1} \right)^{n/2} &\text{si } x_{(n)} < 2\theta_1 + 1 \\ 
            0 &\text{si } x_{(n)} \geq 2\theta_1 + 1 
        \end{cases} \quad \forall (x_1, \ldots, x_n) \in \X^n$$

        Gráficamente, como $\theta_1 < \theta_0$:

        \begin{center}
            \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

            \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
                %uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300

                %Straight Lines [id:da9701795441758359] 
                \draw    (70,101) -- (301,102) ;
                %Straight Lines [id:da9876084884828463] 
                \draw    (141,89) -- (141,112) ;
                %Straight Lines [id:da963921157698696] 
                \draw    (210,89) -- (210,112) ;
                %Straight Lines [id:da07701744813997313] 
                \draw    (140.5,41) -- (297.67,41.33) ;
                \draw [shift={(299.67,41.33)}, rotate = 180.12] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da3774624119169432] 
                \draw    (140.5,41) -- (140.5,21.67) ;
                %Straight Lines [id:da9681405401847125] 
                \draw    (140.5,71.5) -- (140.5,51.17) ;
                %Straight Lines [id:da1310029333570385] 
                \draw    (140.5,70.5) -- (72.5,71.23) ;
                \draw [shift={(70.5,71.25)}, rotate = 359.39] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da899862814242277] 
                \draw    (210.5,171.5) -- (210.5,151.17) ;
                %Straight Lines [id:da009971342095128954] 
                \draw    (210.5,170.5) -- (73.5,170.5) ;
                \draw [shift={(71.5,170.5)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da4678439533610015] 
                \draw    (210.5,211.5) -- (210.5,191.17) ;
                %Straight Lines [id:da8507207636884839] 
                \draw    (210.5,210.5) -- (297.5,211.48) ;
                \draw [shift={(299.5,211.5)}, rotate = 180.64] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
                %Straight Lines [id:da9954362463616986] 
                \draw  [dash pattern={on 0.84pt off 2.51pt}]  (140.5,8.5) -- (140.5,211.5) ;
                %Straight Lines [id:da20565414403393933] 
                \draw  [dash pattern={on 0.84pt off 2.51pt}]  (210,8.5) -- (210,211.5) ;

                % Text Node
                \draw (333,59) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \theta _{1} \ < \ \theta _{0}$};
                % Text Node
                \draw (86,103.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 2\theta _{1} +1$};
                % Text Node
                \draw (215,104) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 2\theta _{0} +1$};
                % Text Node
                \draw (307,92) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle X_{( n)}$};
                % Text Node
                \draw (225,16) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{1} =0$};
                % Text Node
                \draw (83.33,47) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{1}  >0$};
                % Text Node
                \draw (153.33,147) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{0}  >0$};
                % Text Node
                \draw (223.33,188) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f_{0} =0$};
            \end{tikzpicture}
        \end{center}

        La semirrecta en que $f_0 = 0$, que se corresponde con la condición $x_{(n)} \geq 2\theta_0 + 1$, no nos interesa 
        ya que $(x_1, \ldots, x_n) \notin \X^n$. Gráficamente, la situación en la que estamos es la siguiente:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}
                % Ejes
                \draw[-Stealth] (-0.2,0) -- (7,0) node[right] {$X_{(n)}$};
                \draw[-Stealth] (0,-0.2) -- (0,4) node[left] {$\lm$};

                % Recta con altura 1
                \draw[thick] (0,1.65) -- (6,1.65);

                % Proyección sobre la recta
                \coordinate (P) at (1.5,2.65); % Ajustado para caer sobre la recta dibujada
                \draw[thick, blue] (P) -- (0,2.65);

                \draw[dashed] (1.5,0) -- (1.5,2.65);
                \coordinate (Q) at (1.5,0); % Ajustado para caer sobre la recta dibujada
                \draw (1.5,0) node[below] {$2\theta_1 + 1$};
                \draw (3.5,0) node[below] {$2\theta_0 + 1$};
                \coordinate (R) at (3.5,0); % Ajustado para caer sobre la recta dibujada

                \draw[thick, blue] (Q) -- (3.5,0);

                % Etiqueta k en el eje y
                \draw (0,2.65) node[left] {$\left( \dfrac{\theta_0}{\theta_1} \right)^{n/2}$};
                \draw (0,1.65) node[left] {$1$};

                % Punto en la recta
                \draw (P) circle (2pt);
                % Punto en la recta
                \fill (Q) circle (2pt);

                \draw (R) circle (2pt);
            \end{tikzpicture}
        \end{figure}

        Tenemos que $k \in \left\{0, \left( \dfrac{\theta_0}{\theta_1} \right)^{n/2} \right\}$, luego distinguimos entre estos dos casos. \\

        Si \boxed{k=0}, entonces el test será:
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \lambda(X_1, \ldots, X_n) > k \iff X_{(n)} < 2\theta_1 + 1 \\
            \gamma &\text{si } \lambda(X_1, \ldots, X_n) = k \iff X_{(n)} \geq 2\theta_1 + 1 \\
            0 &\text{si } \lambda(X_1, \ldots, X_n) < k \text{ nunca}
        \end{cases}$$

        es decir:

        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } X_{(n)} < 2\theta_1 + 1 \\
            \gamma &\text{si } X_{(n)} \geq 2\theta_1 + 1
        \end{cases}$$

        Determinamos $\gamma$ imponiendo tamaño $\alpha$:

        $$\alpha \stackrel{def} = \sup_{\theta \in \Theta_0} \beta_{\varphi}(\theta) =  \sup_{\theta \in \Theta_0} E_\theta[\varphi(X_1, \ldots, X_n)] = E_{\theta_0}[\varphi(X_1, \ldots, X_n)] = $$
        $$1 \cdot P_{\theta_0}[X_{(n)} < 2\theta_1 + 1] + \gamma P_{\theta_0}[X_{(n)} \geq 2\theta_1 + 1] $$

        Para calcular las probabilidades, obtenemos la función de distribución $F_X(t)$:
        $$F_X(t) \stackrel{def}{=} \int_{1}^{t} \dfrac{1}{\sqrt{8\theta}\sqrt{x-1}} dx = \dfrac{1}{\sqrt{8\theta}} \int_{1}^{t} \dfrac{1}{\sqrt{x-1}} dx = \dfrac{1}{\sqrt{8\theta}} \left[ 2 \sqrt{x-1} \right]_1^t = 
        \dfrac{2\sqrt{t-1}}{\sqrt{8\theta}} = 
        \dfrac{\sqrt{t-1}}{\sqrt{2\theta}}$$

        Sabemos por teoría que, para $T = X_{(n)}$, la distribución del máximo verifica $$P_{\theta}[T<t] = F_T(t) = (F_X(t))^n = \left( \dfrac{\sqrt{t-1}}{\sqrt{2\theta}} \right)^{n} = \left( \sqrt{\dfrac{t-1}{2\theta}} \right)^{n} = 
        \left( \dfrac{t-1}{2\theta} \right)^{n/2}$$

        Por lo tanto, $$P_{\theta_0}[T < 2\theta_1 + 1] = \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}$$

        y

        $$P_{\theta_0}[T \geq 2\theta_1 + 1] = 1 - P_{\theta_0}[T < 2\theta_1 + 1] = 1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}$$

        de donde
        $$\alpha = 1 \cdot P_{\theta_0}[X_{(n)} < 2\theta_1 + 1] + 
        \gamma P_{\theta_0}[X_{(n)} \geq 2\theta_1 + 1] = \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2} + \gamma \left( 1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2} \right) \iff $$
        $$0 \leq \gamma = \dfrac{\alpha - \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}}{1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}} \leq 1 \iff \alpha \geq \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}$$

        y el test resultante es:

        $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                1 &\text{si } X_{(n)} < 2\theta_1 + 1 \\
                \dfrac{\alpha - \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}}{1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}} &\text{si } X_{(n)} \geq 2\theta_1 + 1
            \end{cases}$$      
            
        con potencia (en $\Theta_1 = \{\theta_1\}$):
        $$\beta_{\varphi}(\theta_1) = E_{\theta_1}[\varphi(X_1, \ldots, X_n)] = P_{\theta_1}[T < 2\theta_1 + 1] + 
        \dfrac{\alpha - \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}}{1 - \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}} P_{\theta_1}[T \geq 2\theta_1 + 1]$$

        Como $P_{\theta_1}[T < 2\theta_1 + 1] = 1$ y $P_{\theta_1}[T \geq 2\theta_1 + 1] = 1 - P_{\theta_1}[T < 2\theta_1 + 1] = 1 - 1 = 0$, entonces

        $$\beta_{\varphi}(\theta_1) = 1$$

        \newpage

        Si \boxed{k = \left(\dfrac{\theta_0}{\theta_1} \right)^{n/2}}, el test será, mirando nuevamente la misma gráfica

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}
                % Ejes
                \draw[-Stealth] (-0.2,0) -- (7,0) node[right] {$X_{(n)}$};
                \draw[-Stealth] (0,-0.2) -- (0,4) node[left] {$\lm$};

                % Recta con altura 1
                \draw[thick] (0,1.65) -- (6,1.65);

                % Proyección sobre la recta
                \coordinate (P) at (1.5,2.65); % Ajustado para caer sobre la recta dibujada
                \draw[thick, blue] (P) -- (0,2.65);

                \draw[dashed] (1.5,0) -- (1.5,2.65);
                \coordinate (Q) at (1.5,0); % Ajustado para caer sobre la recta dibujada
                \draw (1.5,0) node[below] {$2\theta_1 + 1$};
                \draw (3.5,0) node[below] {$2\theta_0 + 1$};
                \coordinate (R) at (3.5,0); % Ajustado para caer sobre la recta dibujada

                \draw[thick, blue] (Q) -- (3.5,0);

                % Etiqueta k en el eje y
                \draw (0,2.65) node[left] {$\left( \dfrac{\theta_0}{\theta_1} \right)^{n/2}$};
                \draw (0,1.65) node[left] {$1$};

                % Punto en la recta
                \draw (P) circle (2pt);
                % Punto en la recta
                \fill (Q) circle (2pt);

                \draw (R) circle (2pt);
            \end{tikzpicture}
        \end{figure}

        el que sigue:

        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } \lambda(X_1, \ldots, X_n) > k \text{ nunca} \\
            \gamma &\text{si } \lambda(X_1, \ldots, X_n) = k \iff X_{(n)} < 2\theta_1 + 1 \\
            0 &\text{si } \lambda(X_1, \ldots, X_n) < k \iff X_{(n)} \geq 2\theta_1 + 1
        \end{cases}$$

        es decir:
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            \gamma &\text{si } X_{(n)} < 2\theta_1 + 1 \\
            0 &\text{si } X_{(n)} \geq 2\theta_1 + 1
        \end{cases}$$

        Determinamos $\gamma$ igual que antes imponiendo tamaño $\alpha$:

        $$\alpha \stackrel{def} = \sup_{\theta \in \Theta_0} \beta_{\varphi}(\theta) =  \sup_{\theta \in \Theta_0} E_\theta[\varphi(X_1, \ldots, X_n)] = E_{\theta_0}[\varphi(X_1, \ldots, X_n)] = $$
        $$\gamma \cdot P_{\theta_0}[X_{(n)} < 2\theta_1 + 1] + 0 \cdot P_{\theta_0}[X_{(n)} \geq 2\theta_1 + 1] = \gamma \cdot P_{\theta_0}[X_{(n)} < 2\theta_1 + 1] $$

        Ya sabemos del caso anterior que 
        $$P_{\theta_0}[T < 2\theta_1 + 1] = \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}$$

        luego
        $$\alpha = \gamma \cdot P_{\theta_0}[X_{(n)} < 2\theta_1 + 1] = \gamma \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2} \iff $$
        $$0 \leq \gamma = \dfrac{\alpha}{\left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}} = \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{n/2}  \leq 1 \iff \alpha \leq \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}$$

        y el test resultante es:

        $$\varphi(X_1, \ldots, X_n) = 
            \begin{cases}
                \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{n/2} &\text{si } X_{(n)} < 2\theta_1 + 1 \\
                0 &\text{si } X_{(n)} \geq 2\theta_1 + 1
            \end{cases}$$      
            
        con potencia (en $\Theta_1 = \{\theta_1\}$):
        $$\beta_{\varphi}(\theta_1) = E_{\theta_1}[\varphi(X_1, \ldots, X_n)] = \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{n/2} \cdot P_{\theta_1}[T < 2\theta_1 + 1] + 
        0 \cdot P_{\theta_1}[T \geq 2\theta_1 + 1] = $$
        $$\alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{n/2} \cdot P_{\theta_1}[T < 2\theta_1 + 1]$$

        Igual que en el caso anterior $P_{\theta_1}[T < 2\theta_1 + 1] = 1$, luego

        $$\beta_{\varphi}(\theta_1) = \alpha \left( \dfrac{\theta_0}{\theta_1} \right)^{n/2}$$
    \end{ejercicio}

    \begin{ejercicio}~
        \begin{enumerate}
            \item[a)] Test de Kolmogorov--Smirnov.
            \begin{enumerate}
                \item[i)] Plantear el problema de contraste. \\
                
                Sea una función de distribución específica $F_0$, y sea $(X_1, \ldots, X_n)$ una m.a.s. de una v.a. $X$ continua que se distribuye
                según una función de distribución $F$ que es completamente desconocida. El constraste a resolver es
                $$\begin{cases}
                    H_0 : F = F_0 \\
                    H_1 : F \neq F_0
                \end{cases}$$

                \item[ii)] Dar el valor del estadístico. \\
                
                El estadístico que se usa para resolver el problema es el estadístico de Kolmogorov-Smirnov

                $$D(X_1, \ldots, X_n) = \sup_{x \in \R} |F_{X_1, \ldots, X_n}^{*}(x) - F_0(x)|$$

                \item[iii)] Enunciar el teorema que justifica su uso. \\
                
                El test se basa en el teorema de Glivenko-Cantelli: \\

                \begin{teo}[de Glivenko-Cantelli]
                    Sea $\{X_n\}_{n \in \N}$ una sucesión de v.a.i.i.d. con función de distribución común $F$. Si $F_{X_1, \ldots, X_n}^{*}$
                    es la función de distribución muestral asociada a la m.a.s. $(X_1, \ldots, X_n)$, se verifica que $F_{X_1, \ldots, X_n}^{*}$
                    converge casi seguramente y uniformemente a la función de distribución de $X$, $F$.
                    $$P \left\{ \lim\limits_{n \to +\infty} \sup_{x \in \R} |F_{X_1, \ldots, X_n}^{*}(x) - F(x)| = 0 \right\} = 1$$
                \end{teo}

                \item[iv)] Ventajas frente al test $\chi^2$. \\
                
                Se asume que nos referimos al test $\chi^2$ de Pearson. \\
                \begin{enumerate}
                    \item El test de Kolmogorov-Smirnov no necesita hacer particiones de los datos, mientras que el test $\chi^2$ de Pearson sí.
                    \item Para v.a. continuas, es más apropiado usar el test de Kolmogorov-Smirnov que el test $\chi^2$ de Pearson.
                    \item Bajo $H_0$, si $F_0$ es continua, la distribución del estadístico de Kolmogorov-Smirnov no depende de $F_0$, mientras
                    que el test $\chi^2$ de Pearson es asintótico.
                \end{enumerate}

            \end{enumerate}
            \item[b)] Se cuentan el número de tutorías a lo largo de un curso por $50$ profesores.
            Se quiere contrastar si el número de tutorías por profesor se puede describir
            mediante una distribución de Poisson.

            \begin{center}
                \begin{tabular}{c|cccccc}
                    Número de tutorías   & 0 & 1 &  2 & 3  & 4  & 5 \\ \hline
                    Número de profesores & 2 & 5 & 10 & 14 & 12 & 7
                \end{tabular}
            \end{center}

            En este caso, como la variable aleatoria es discreta, y tenemos frecuencias, es más apropiado usar el test $\chi^2$ de Pearson.
            Sea $$X \equiv \text{``Número de tutorías por un profesor''}$$ El contraste a resolver es
            $$\begin{cases}
                H_0 : X \rightsquigarrow \mathcal{P}(\lambda) \\
                H_1 : X \not\rightsquigarrow \mathcal{P}(\lambda)
            \end{cases} \quad \lambda > 0$$
            Vemos que la hipótesis nula es compuesta, luego primeramente debemos estimar el valor del parámetro $\lambda$. Sabemos que el EMV de $\lm$ es la media muestral. Así
            $$\hat{\lambda} = \overline{X} = \dfrac{0 \cdot 2 + 1 \cdot 5 + 2 \cdot 10 + 3 \cdot 14 + 4 \cdot 12 + 5 \cdot 7}{50} = \dfrac{150}{50} = 3$$
            El constraste adaptado sería 
            $$\begin{cases}
                H_0 : X \rightsquigarrow \mathcal{P}(3) \\
                H_1 : X \not\rightsquigarrow \mathcal{P}(3)
            \end{cases}$$
            Denotemos por $N_1, \ldots, N_k$ las frecuencias observadas en las $k$ clases consideradas, y por
            $$\hat{p}_i = P_{\hat{\lambda}}(X \in A_i), \quad i = 1, \ldots, k$$
            las probabilidades teóricas bajo $H_0$ con el parámetro $\lambda$ estimado por $\hat{\lambda}$. \\

            El estadístico de contraste viene dado por

            $$\hat{\chi}(N_1, \ldots, N_k) = \sum_{i=1}^k \dfrac{(N_i - n \hat{p}_i)^2}{n \hat{p}_i}$$

            Como el parámetro se ha estimado a partir de los mismos datos, la distribución asintótica bajo $H_0$ es 
            $$\hat{\chi}(N_1, \ldots, N_k) \rightsquigarrow_{H_0} \chi^2(k - q - 1)$$
            con $q=1$ el número de parámetros estimados.

            Por teoría, para poder aplicar el test hay que verificar que $$E^*_{i} = n\hat{p}_i \geqslant 5 \quad \forall i = 1, \ldots, k, \quad n = 50, \quad \hat{p}_i = P[X=i] = e^{-3} \dfrac{3^i}{i!}$$

            Primero vamos tanteando (no se ponen a propósito $E^*_0$ ni $E^*_6$):

            $$E^*_1 = 50 \cdot e^{-3} \dfrac{3^1}{1!} \approx 7.47$$
            $$E^*_2 = 50 \cdot e^{-3} \dfrac{3^2}{2!} \approx 11.2$$
            $$E^*_3 = 50 \cdot e^{-3} \dfrac{3^3}{3!} \approx 11.2$$
            $$E^*_4 = 50 \cdot e^{-3} \dfrac{3^4}{4!} \approx 8.4$$
            $$E^*_5 = 50 \cdot e^{-3} \dfrac{3^5}{5!} \approx 5.05$$

            En este punto\footnote{hay que particionar el espacio muestral de la Poisson, que
            es $\N \cup \{0\}$, en grupos que verifiquen las condiciones del test $\chi^2$ de Pearson, por eso las consideraciones siguientes.}, como 
            $$E^*_0 = 50 \cdot \hat{p}_0 = 50 \cdot e^{-3} \dfrac{3^0}{0!} \approx 2.49$$
            $$E^*_6 = 50 \cdot e^{-3} \dfrac{3^6}{6!} \approx 2.52$$
            
            debemos aquí agrupar los mayores o iguales que $5$. Por ejemplo, podemos agrupar $0$ y $1$, y no considerar $E^*_6$, y en su lugar, considerar
            $$E^*_{\geq 6} = n P_{\hat{\lambda}}(X \geq 6) = n (1 - P_{\hat{\lambda}}(X < 6)) = n - \sum_{i=0}^{5} n P_{\hat{\lambda}}(X = i) =  n - \sum_{i=0}^{5} E_i^* \approx $$
            $$50 - (2.49 + 7.47 + 11.2 + 11.2 + 8.4 + 5.05) = 4,19 < 5$$
            que sigue siendo menor estricto que $5$, por lo que agrupamos la cola derecha a partir del 5, y ahora sí
            $$E^*_{\geq 5} = n P_{\hat{\lambda}}(X \geq 5) = n - \sum_{i=0}^{4} E_i^* = 50 - \left(\sum_{i=0}^{5} E_i^* - E_5^* \right) = 50 - (45,81 - 5,05) = 50 - 40,76 = $$
            $$9,24 \geq 5$$

            Una partición sería la siguiente
            $$A_1 = \{0,1\}, \quad A_i = \{i\}, \quad A_5 = \{\geqslant 5\}, \quad i = 2,3,4$$
            En este caso, $k=5$ (número de clases tras agrupar), luego
            $\hat{\chi}^2(N_1, \ldots, N_k) \rightsquigarrow \chi^2(3)$. Las frecuencias observadas, denotadas por $O_i, i= 1,\ldots,k$, son 
            $$O_1 = N_0 + N_1 = 2 + 5 = 7, \quad O_2 = 10, \quad O_3 = 14, \quad O_4 = 12, \quad O_5 = 7$$
            Las frecuencias esperadas son 
            $$E_1 = E^*_0 + E^*_1 \approx 2.49 + 7.47 = 9.96, \quad E_2 = E^*_2 \approx 11.2 \quad E_3 = E^*_3 \approx 11.2 
            \quad E_4 = E^*_4 \approx 8.4$$$$E_5 = 50 - \sum_{i=1}^{4} E_j \approx 50 - (9.96 + 11.2 + 11.2 + 8.4) = 9.24$$

            Obtenemos $$\chi^2_{exp} = \sum_{i=1}^{5} \dfrac{(O_i - E_i)^2}{E_i} = \dfrac{(7 - 9.96)^2}{9.96} + \dfrac{(10 - 11.2)^2}{11.2} +
            \dfrac{(14 - 11.2)^2}{11.2} + \dfrac{(12 - 8.4)^2}{8.4} + $$$$\dfrac{(7 - 9.24)^2}{9.24} \approx 3.79$$

            El test asintótico de tamaño $\alpha$ es

            $$\varphi(X_1, \ldots, X_n) = \begin{cases}
                1 &\text{si } \hat{\chi}^2(N_1, \ldots, N_k) \geqslant \chi^2_{k-1;\alpha} \\
                0 &\text{si } \hat{\chi}^2(N_1, \ldots, N_k) < \chi^2_{k-1;\alpha}
            \end{cases}$$

            con $$p-valor = P_{H_0}[\hat{\chi}^2(N_1, \ldots, N_k) \geqslant \chi^2_{exp}] \approx_{n \to +\infty} P[\chi^2(k-q-1) \geqslant \chi^2_{exp}]$$

            y $\chi^2_{exp}$ el valor del estadístico obtenido con la muestra observada. Usando que $k-q-1=3$, obtenemos
            $$p-valor \approx P[\chi^2(3) \geqslant 3.79] \approx 0.3$$

            Como el $p-valor$ es grande (respecto a los niveles habituales de significación), se acepta $H_0$, por lo que 
            puede suponerse que el número de tutorías por profesor se puede describir mediante una distribución de Poisson.
        \end{enumerate}
    \end{ejercicio}


\end{document}