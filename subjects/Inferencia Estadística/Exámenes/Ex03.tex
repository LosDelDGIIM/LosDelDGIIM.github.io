\documentclass[12pt]{article}

\input{../../_assets/preambulo.tex}

\newcommand{\E}{\mathrm{E}}
\newcommand{\V}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rmas}{\R^{+}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\I}{\mathrm{Im}}
\newcommand{\Rmenos}{\R^{-}}

\begin{document}

    % 1. Foto de fondo
    % 2. Título
    % 3. Encabezado Izquierdo
    % 4. Color de fondo
    % 5. Coord x del titulo
    % 6. Coord y del titulo
    % 7. Fecha

    
    \input{../../_assets/portada}
    \portadaExamen{ffccA4.jpg}{Inferencia \\ Estadística\\Examen III}{Inferencia Estadística. Examen III}{MidnightBlue}{-8}{28}{2026}{José Manuel Sánchez Varbas}

    \begin{description}
        \item[Asignatura] Inferencia Estadística.
        \item[Curso Académico] 2023-24.
        \item[Grado] Grado en Matemáticas y Grado en Ingeniería Informática y Matemáticas.
        \item[Descripción] Examen Ordinario.
        \item[Fecha] 23 de Enero de 2024.  
    \end{description}
    \newpage


    % ------------------------------------

    \begin{ejercicio}
        Sean $(X_1,\dots,X_{n_1})$ y $(Y_1,\dots,Y_{n_2})$ m.a.s. de $X$ e $Y$, variables que siguen
        $N(\mu_1,4)$ y $N(\mu_2,5)$ respectivamente.

        \begin{enumerate}
            \item[a)] Si $\mu_1=2$, $\mu_2=3$ y sean $(X_1,\dots,X_8)$ y $(Y_1,\dots,Y_{10})$ dos muestras
            de tamaño $8$ y $10$ respectivamente con medias muestrales $\overline X$, $\overline Y$,
            calcular el percentil $99$ de
            \begin{gather}
                V=\frac{\overline X-\overline Y+1}
                {\sqrt{\dfrac{\displaystyle\sum_{i=1}^8 (X_i-\overline X)^2}{4}
                +\dfrac{\displaystyle\sum_{i=1}^{10}(Y_i-\overline Y)^2}{5}}}.
            \end{gather}

            \item[b)] Calcular el intervalo de confianza para $\mu_1-\mu_2$ de menor longitud esperada
            uniformemente a nivel de confianza $1-\alpha$. ¿Cómo sería el intervalo si las varianzas 
            fueran desconocidas pero iguales?
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}~
        \begin{enumerate}
            \item[a)] Sea $X$ una v.a. con función de densidad
            \begin{gather}
                f_\theta(x)=\frac{\theta}{2}\,x^{-3/2}, \qquad x>\theta^2.
            \end{gather}
            Calcular el UMVUE y determinar para qué valores de $n$ existe.
            ¿Es eficiente?

            \item[b)] Sea
            $$
            f_\theta(x)=\theta\,T(x)e^{-\theta x}, \qquad x>0,\ \theta>0,
            $$
            en una familia regular según Fréchet--Cramér--Rao.
            \begin{enumerate}
                \item[b1)] Sabiendo que $I_{X_1,\dots,X_n}(\theta)=\dfrac{2n}{\theta^2}$, calcular
                $\mathrm{E}[X]$ y $\mathrm{Var}[X]$.
                \item[b2)] Sabiendo que $$\sum_{i=1}^n \dfrac{T(X_i)}{n}$$ es un estimador eficiente de $2/\theta$,
                calcular $T(x)$.
            \end{enumerate}
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}~
        \begin{enumerate}
            \item[a)] Enunciar y demostrar el Teorema de Zehna definiendo previamente los siguientes
            conceptos: función de verosimilitud de un parámetro, función de verosimilitud de una
            función paramétrica y estimador máximo verosímil para funciones paramétricas.

            \item[b)] Calcular la función de verosimilitud de $$\lambda=(\theta-1)^2$$
            asociada a una realización muestral cuyo máximo valor es $3$ si $$f_\theta(x)=e^{x-\theta}, \qquad x \leqslant \theta, \theta>0.$$
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}
        Sea $(X_1,\dots,X_n)$ una m.a.s.\ de $X$ v.a.\ con función de densidad
        \begin{gather}
            f_\theta(x)=\frac{1}{\sqrt{8\theta}\sqrt{x-1}}, \qquad 1<x<2\theta+1.
        \end{gather}
        Deducir el test más potente de tamaño arbitrario para contrastar
        $$H_0:\theta=\theta_0 \quad \text{frente a} \quad H_1:\theta=\theta_1$$
        siendo $\theta_1<\theta_0$. Calcular la potencia.
    \end{ejercicio}

    \begin{ejercicio}~
        \begin{enumerate}
            \item[a)] Test de Kolmogorov--Smirnov.
            \begin{enumerate}
                \item[i)] Plantear el problema de contraste.
                \item[ii)] Dar el valor del estadístico.
                \item[iii)] Enunciar el teorema que justifica su uso.
                \item[iv)] Ventajas frente al test $\chi^2$.
            \end{enumerate}
            \item[b)] Se cuentan el número de tutorías a lo largo de un curso por $50$ profesores.
            Se quiere contrastar si el número de tutorías por profesor se puede describir
            mediante una distribución de Poisson.

            \begin{center}
                \begin{tabular}{c|cccccc}
                    Número de tutorías & 0 & 1 & 2 & 3 & 4 & 5 \\ \hline
                    Número de profesores & 2 & 5 & 10 & 14 & 12 & 7
                \end{tabular}
            \end{center}
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \setcounter{ejercicio}{0}

    \begin{ejercicio}
        Sean $(X_1,\dots,X_{n_1})$ y $(Y_1,\dots,Y_{n_2})$ m.a.s.\ de $X$ e $Y$, variables que siguen
        $N(\mu_1,4)$ y $N(\mu_2,5)$ respectivamente.

        \begin{enumerate}
            \item[a)] Si $\mu_1=2$, $\mu_2=3$ y sean $(X_1,\dots,X_8)$ y $(Y_1,\dots,Y_{10})$ dos muestras
            de tamaño $8$ y $10$ respectivamente con medias muestrales $\overline X$, $\overline Y$,
            calcular el percentil $99$ de
            \begin{gather}
                V=\frac{\overline X-\overline Y+1}
                {\sqrt{\dfrac{\displaystyle\sum_{i=1}^8 (X_i-\overline X)^2}{4}
                +\dfrac{\displaystyle\sum_{i=1}^{10}(Y_i-\overline Y)^2}{5}}}.
                \tag{1}
            \end{gather}

            Del enunciado del problema sacamos que 
            $$X_i \rightsquigarrow N(\mu_1, 4), \quad i=1,\ldots,n_1$$
            $$Y_j \rightsquigarrow N(\mu_2,5) \quad j=1,\ldots, n_2$$
            y en este apartado, $n_1 = 8$ y $n_2 = 10$. Buscamos obtener la 
            distribución de $V$. Para ello, primero hallamos la distribución del numerador. \\

            Como $\mu_1 = 2$ y $\mu_2 = 3$, entonces 
            $$\E[\overline{X} - \overline{Y} + 1] = \E[\overline{X}] - \E[\overline{Y}] + 1 = 
            \mu_1 - \mu_2 + 1 = 2 - 3 + 1 = 0$$

            Por teoría sabemos que
            $$X \rightsquigarrow N(\mu_1, 4) \Longrightarrow \overline{X} \rightsquigarrow N \left( \mu_1, \dfrac{4}{n_1} \right)$$
            $$Y \rightsquigarrow N(\mu_2, 5) \Longrightarrow \overline{Y} \rightsquigarrow N \left( \mu_2, \dfrac{5}{n_2} \right)$$

            Así, asumiendo que $(X_1,\dots,X_{n_1})$ y $(Y_1,\dots,Y_{n_2})$ son independientes, entonces
            
            $$\overline{X} - \overline{Y} \rightsquigarrow N \left( \mu_1 - \mu_2, \dfrac{4}{n_1} + \dfrac{5}{n_2} \right)$$

            y se obtiene que
            $$\V(\overline{X} - \overline{Y} + 1) = \V(\overline{X} - \overline{Y}) = \dfrac{4}{n_1} + \dfrac{5}{n_2} = \dfrac{4}{8} + \dfrac{5}{10} = \dfrac{1}{2} + \dfrac{1}{2} = 1$$

            Deducimos entonces que $Z = \overline{X} - \overline{Y} + 1 \rightsquigarrow N(0,1)$ \\

            Falta hallar la distribución del denominador. Usaremos que 

            $$\dfrac{(n_1 - 1) S_1^2}{\sigma_1^2} = \dfrac{\displaystyle\sum_{i=1}^{n_1} (X_i - \overline{X})^2}{\sigma_1^2} \rightsquigarrow \chi^2(n_1 -1)$$
            $$\dfrac{(n_2 - 1) S_2^2}{\sigma_2^2} = \dfrac{\displaystyle\sum_{j=1}^{n_2} (Y_j - \overline{Y})^2}{\sigma_2^2} \rightsquigarrow \chi^2(n_2 -1)$$

            y como estas últimas v.a. son independientes, por serlo las m.a.s. de $X$ e $Y$, podemos aplicar la reproductividad de la distribución $\chi^2$:
            $$W = \dfrac{\displaystyle\sum_{i=1}^{n_1} (X_i - \overline{X})^2}{\sigma_1^2} + \dfrac{\displaystyle\sum_{j=1}^{n_2} (Y_j - \overline{Y})^2}{\sigma_2^2}
            \rightsquigarrow \chi^2(n_1 - 1 + n_2 - 1) = \chi^2(n_1 + n_2 - 2) = \chi^2(16)$$

            En estas condiciones puede aplicarse la construcción de la distribución $T$ de Student
            $$T = \dfrac{Z}{\sqrt{W/16}} \rightsquigarrow t(16)$$
            y basta notar que 
            $$V = \dfrac{Z}{\sqrt{W}} = \dfrac{Z}{\sqrt{16 \cdot W/16}} = \dfrac{Z}{\sqrt{16} \sqrt{W/16}} = \dfrac{1}{4} \dfrac{Z}{\sqrt{W/16}} = 
            \dfrac{1}{4} T$$

            La tabla proporcionada para la $T$ de Student cumple que 
            $$P(T \leqslant q_p) = p \iff P(T_n > t_{n,{1-p}}) = 1-p$$
            donde $q_p$ es el percentil $p \in ]0,1[$, $t_{n,1-p}$ es el valor tabulado para la fila $n$ y la columna $1-p$,
            y $T_n \stackrel{not}{\equiv} T \rightsquigarrow t(n)$. El percentil $99$ de $V$ será consecuentemente
            $$q_{0.99}(V) = \dfrac{1}{4} t_{16; 0.01} \approx \dfrac{1}{4} \cdot 2.5835 \approx 0.645875$$

            \item[b)] Calcular el intervalo de confianza para $\mu_1-\mu_2$ de menor longitud esperada
            uniformemente a nivel de confianza $1-\alpha$. ¿Cómo sería el intervalo si las varianzas 
            fueran desconocidas pero iguales? \\

            Ya sabemos que $$\overline{X} - \overline{Y} \rightsquigarrow 
            N \left( \mu_1 - \mu_2, \dfrac{4}{n_1} + \dfrac{5}{n_2} \right)$$

            Y como nos piden el intervalo de confianza (se asume que bilateral), entonces por el método del pivote 
            visto en teoría, usando como pivote 
            $$T(X_1, \ldots, X_{n_1}, Y_1, \ldots, Y_{n_2}; \mu_1 - \mu_2) = \dfrac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}
            {\sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}}} \rightsquigarrow N(0,1)$$
            este debe verificar, por definición de intervalo de confianza, lo siguiente 
            $$P_{\mu_1, \mu_2} \left( \lambda_1 < \dfrac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}
            {\sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}}} < \lambda_2 \right) \geqslant 1 - \alpha \quad \forall (\mu_1, \mu_2) \in \R^2$$

            para ciertos $\lambda_1, \lambda_2 \in \R$. Buscamos aislar la diferencia de medias poblacionales en el centro de la 
            cadena de desigualdades, luego

            $$\lambda_1 < \dfrac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}
            {\sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}}} < \lambda_2$$ $$\iff 
            \lambda_1 \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}} < (\overline{X} - \overline{Y}) - (\mu_1 - \mu_2) < 
            \lambda_2 \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}}$$ $$\iff 
            \overline{Y} - \overline{X} + \lambda_1 \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}} < 
            -(\mu_1 - \mu_2) < \overline{Y} - \overline{X} + \lambda_2 \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}} $$ $$\iff
            \overline{X} - \overline{Y} - \lambda_2 \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}} < 
            \mu_1 - \mu_2 < \overline{X} - \overline{Y} - \lambda_1 \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}}$$

            La solución para obtener el intervalo de confianza de menor longitud esperada uniformemente a nivel de confianza $1-\alpha$
            se alcanza con $\lambda_1 = - z_{\alpha/2}$, $\lambda_2 = z_{\alpha/2}$, donde $P[Z > z_{\alpha}] > \alpha$, y el intervalo
            de confianza bilateral es 
            $$\left(\overline{X} - \overline{Y} - z_{\alpha/2} \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}},
            \overline{X} - \overline{Y} + z_{\alpha/2} \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}} \right)$$
            Si se pidiera cualquiera de los dos unilaterales, entonces
            \begin{enumerate}
                \item Si $\lambda_1 = -\infty$, se sustituye el extremo superior del intervalo bilateral anterior por $+\infty$,
                y $z_{\alpha/2}$ por $z_{\alpha}$.
                \item Si $\lambda_2 = +\infty$, se sustituye el extremo inferior del intervalo bilateral anterior por $-\infty$,
                y $z_{\alpha/2}$ por $z_{\alpha}$.
            \end{enumerate}

            Si las varianzas fueran desconocidas pero iguales, entonces el pivote sería
            $$T(X_1, \ldots, X_{n_1}, Y_1, \ldots, Y_{n_2}; \mu_1 - \mu_2) = \dfrac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}
            {S_p \sqrt{\dfrac{1}{n_1} + \dfrac{1}{n_2}}} \rightsquigarrow t(n_1 + n_2 - 2)$$
            donde $$S_p = \dfrac{(n_1 - 1) S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}$$
            y como la $T$ de Student tiene las mismas propiedades de simetría respecto al origen que $N(0,1)$, entonces
            el intervalo de confianza bilateral a nivel de confianza $1 - \alpha$ sería
            $$\left(\overline{X} - \overline{Y} - t_{n_1+n_2-2;\alpha/2} S_p \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}},
            \overline{X} - \overline{Y} + t_{n_1+n_2-2;\alpha/2} S_p \sqrt{\dfrac{4}{n_1} + \dfrac{5}{n_2}} \right)$$
            donde $P[T_{n_1+n_2-2} > t_{n_1+n_2-2;\alpha}] = \alpha$ y nuevamente si se pidiera cualquiera de los dos unilaterales, entonces
            \begin{enumerate}
                \item Si $\lambda_1 = -\infty$, se sustituye el extremo superior del intervalo bilateral anterior por $+\infty$,
                y $t_{n_1+n_2-2;\alpha/2}$ por $t_{n_1+n_2-2;\alpha}$.
                \item Si $\lambda_2 = +\infty$, se sustituye el extremo inferior del intervalo bilateral anterior por $-\infty$,
                y $t_{n_1+n_2-2;\alpha/2}$ por $t_{n_1+n_2-2;\alpha}$.
            \end{enumerate}
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}~
        \begin{enumerate}
            \item[a)] Sea $X$ una v.a. con función de densidad
            \begin{gather}
                f_\theta(x)=\dfrac{\theta}{2} x^{-3/2}, \qquad x>\theta^2.
                \tag{2}
            \end{gather}
            Calcular el UMVUE y determinar para qué valores de $n$ existe.
            ¿Es eficiente? \\

            Buscamos obtener el UMVUE mediante el método alternativo visto en teoría. Para ello, en primer lugar
            hay que encontrar un estadístico suficiente y completo $T$, y luego una función del estadístico $h(T)$
            (denotaremos indistintamente $T \stackrel{not}{\equiv} T(X_1, \ldots, X_n)$) insesgada en 
            $g(\theta) = \theta$ (como no se especifica de quién es el UMVUE, se asume que del parámetro)
            estimadora y con momento de segundo orden finito. Entonces $h(T)$ será el UMVUE. \\

            Podría pensarse en un principio en buscar el estadístico suficiente y completo si la familia fuera de tipo exponencial,
            sin embargo, como $\X = \left]-\theta^2, +\infty \right[$, vemos que el conjunto de valores de la variable depende de $\theta$,
            luego ya no tendríamos que la familia es de tipo exponencial. \\ 

            La suficiencia se obtendrá entonces por medio del Teorema de Factorización de Neyman-Fisher. Calculamos
            la función conjunta 
            $$f^n_\theta(x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^{n} f_\theta(x_i)$$
            Ahora, vemos que $x > \theta^2 \iff I_{\Rmas}(x - \theta^2) = 1$, de donde se deduce que 
            $$\prod_{i=1}^{n} f_\theta(x_i) = \prod_{i=1}^{n} \dfrac{\theta}{2} x_i^{-3/2} \neq 0 \iff I_{\Rmas} (x_{(1)} - \theta^2) = 1$$ luego
            $$f^n_\theta(X_1, \ldots, X_n) = \prod_{i=1}^{n} \dfrac{\theta}{2} x_i^{-3/2} I_{\Rmas}(x_{(1)} - \theta^2) = 
            \left( \dfrac{\theta}{2} \right)^n \prod_{i=1}^{n} x_i^{-3/2} I_{\Rmas}(x_{(1)} - \theta^2) = $$
            $$\left( \dfrac{\theta}{2} \right)^n \prod_{i=1}^{n} x_i^{-3/2} I_{\Rmas}(x_{(1)} - \theta^2)$$
            Tomando $T(X_1, \ldots, X_n) = X_{(1)}$ y 
            $$h(x_1, \ldots, x_n) = \prod_{i=1}^{n} x_i^{-3/2}, \quad g_\theta(t) = 
            \left( \dfrac{\theta}{2} \right)^n I_{\Rmas}(t - \theta^2)$$
            Se cumple que 
            $$f^n_\theta(x_1, \ldots, x_n) = h(x_1, \ldots, x_n) g_\theta(T(x_1, \ldots, x_n)) \quad \forall (x_1, \ldots, x_n) \in \X^n$$
            donde $h$ es independiente del parámetro $\theta$ y $g_\theta$ depende de la muestra solo a través del estadístico, luego,
            por el Teorema de Factorización de Neyman-Fisher, el estadístico $T$ es suficiente. \\

            Ahora, hay que comprobar que este estadístico es completo, lo cual se hará por definición. Sabemos por teoría que la distribución del mínimo
            es 
            $$F_T(t) = 1 - (1 - F_X(t))^n \Longrightarrow f_T(t) = n(1-F_X(t))^{n-1} f_\theta(t)$$
            Hallamos ahora la función de distribución de $X$:
            $$F_X(t) = \int_{\theta^2}^{t} f_\theta(x) dx = \int_{\theta^2}^{t} \dfrac{\theta}{2} x^{-3/2} dx = 
            \dfrac{\theta}{2} \int_{\theta^2}^{t} x^{-3/2} dx = \dfrac{\theta}{2} \cdot \dfrac{1}{-1/2} \left[ x^{-1/2} \right]_{\theta^2}^{t} = $$
            $$\dfrac{-2 \cdot \theta}{2} (t^{-1/2} - \theta^{2^{-1/2}}) = -\theta \left(\dfrac{1}{\sqrt{t}} - \dfrac{1}{\theta} \right) \stackrel{(*)}{=} 
            1-\dfrac{\theta}{\sqrt{t}} \quad t > \theta^2$$
            donde en $(*)$ se ha usado que, para que $f_\theta(x)$ sea función de densidad, debe ser no negativa e integrar a $1$, lo cual implica
            necesariamente que $\theta > 0$. \\

            La función de densidad del estadístico será entonces
            $$f_T(t) = n(1-F_X(t))^{n-1} f_\theta(t) = n \left(\dfrac{\theta}{\sqrt{t}} \right)^{n-1} \dfrac{\theta}{2} t^{-3/2} = 
            n \dfrac{\theta^{n-1}}{(\sqrt{t})^{n-1}} \dfrac{\theta}{2} t^{-3/2} = $$ 
            $$\dfrac{n}{2} \theta^n t^{-3/2 - \left(\dfrac{n-1}{2}\right)} = \dfrac{n \theta^n}{2} t^{-(n/2 + 1)} \quad t > \theta^2$$

            Ahora, consideramos $h$ una función medible verificando
            $$0 = \E[h(T)] \stackrel{def}{=} \int_{\theta^2}^{+\infty} h(t) f_T(t) dt = 
            \int_{\theta^2}^{+\infty} h(t) \dfrac{n \theta^n}{2} t^{-(n/2 + 1)} dt =$$
            $$\dfrac{n \theta^n}{2} \int_{\theta^2}^{+\infty} h(t) t^{-(n/2 + 1)} dt$$
            como $(n \theta^n)/2 \neq 0 \quad \forall n \in \N, \quad \forall \theta>0$, debe ser
            $$\int_{\theta^2}^{+\infty} h(t) t^{-(n/2 + 1)} dt = 0$$
            Por el Teorema Fundamental del Cálculo, podemos considerar una primitiva $H(t)$ del integrando $h(t) t^{-(n/2 + 1)}$, y esta cumple,
            por la Regla de Barrow, que $\lim\limits_{m \to +\infty}H(m) - H(\theta^2) = 0 \quad \forall \theta \in \Rmas$. Derivando respecto 
            de $\theta$, se obtiene que
            $$- H(\theta^2) = 0 \iff h(\theta^2) \theta^{2^{-(n/2 + 1)}} = 0 \stackrel{\theta > 0}{\iff} h(\theta^2) = 0$$

            Por tanto, $\Rmas \subseteq \{t : h(t) = 0\}$, y consecuentemente $$1 \geqslant P[h(T) = 0] \geqslant P[T \in \Rmas] = 1
            \Longrightarrow P[h(T) = 0] = 1$$

            y entonces por definición concluimos que $T$ es un estadístico completo. Tenemos entonces en este punto
            que $T$ es un estadístico suficiente y completo. \\

            Ahora hay que buscar un estimador insesgado en $\theta$ y de segundo orden finito. Sea $h$ (independiente de la anterior) función medible tal que
            $$\theta = \E[h(T)] = \dfrac{n \theta^n}{2} \int_{\theta^2}^{+\infty} h(t) t^{-(n/2 + 1)} dt \iff$$
            $$\int_{\theta^2}^{+\infty} h(t) t^{-(n/2 + 1)} dt = \dfrac{2}{n \theta^{n-1}}$$

            Supongamos que $h(t) = c t^{\alpha}$, con $\alpha, c \in \R$ a priori. Entonces 

            $$\int_{\theta^2}^{+\infty} c t^{\alpha - (n/2+1)} dt = c \int_{\theta^2}^{+\infty} t^{\alpha - (n/2+1)} dt$$

            y esta última integral sabemos que converge si y solo si $\alpha - (n/2 + 1) < -1$, lo cual equivale a que
            $$\alpha - (n/2 + 1) < -1 \iff \alpha - \dfrac{n}{2} < 0 \iff \alpha < \dfrac{n}{2}$$
            En tal caso, obtenemos el resultado de la integral impropia
            $$\int_{\theta^2}^{+\infty} t^{\alpha-(n/2 + 1)} dt = \lim\limits_{m \to +\infty} \int_{\theta^2}^{m} t^{\alpha-(n/2 + 1)} dt = 
            \lim\limits_{m \to +\infty} \dfrac{1}{\alpha-n/2} \left[t^{\alpha-n/2}\right]_{\theta^2}^{m} = $$
            $$\lim\limits_{m \to +\infty} \dfrac{1}{\alpha-n/2} \left( m^{\alpha-n/2} - \theta^{2^{(\alpha - n/2)}} \right) = 
            \dfrac{1}{\alpha-n/2} \cdot \left( \lim\limits_{m \to +\infty} \dfrac{1}{m^{n/2 - \alpha}} - \theta^{2^{(\alpha - n/2)}} \right) = $$
            $$\dfrac{\theta^{2^{(\alpha - n/2)}}}{n/2 - \alpha} $$

            Sustituyendo $$c \int_{\theta^2}^{+\infty} t^{\alpha - (n/2+1)} dt = \dfrac{2}{n \theta^{n-1}} \iff 
            c \cdot \dfrac{\theta^{2^{(\alpha - n/2)}}}{n/2 - \alpha} = \dfrac{2}{n \theta^{n-1}} = \dfrac{2}{n} \theta^{1-n} $$

            y por comparación entre exponentes de $\theta$ de las expresiones de los dos miembros de la igualdad
            $$\theta^{2^{(\alpha - n/2)}} = \theta^{1-n} \iff 2\alpha - n = 1-n \iff 2 \alpha = 1 \iff \alpha = \dfrac{1}{2}$$
            Debe considerarse a partir de ahora que $n \geqslant 2$ (para que la integral converja), 
            y usando ese valor de $\alpha$ para determinar la constante e igualando constantes nuevamente obtenemos el valor de $c$
            $$c \cdot \dfrac{1}{n/2 - \alpha} = \dfrac{2}{n} \iff c \cdot \dfrac{2}{n-1} = c \cdot \dfrac{1}{n/2 - 1/2} = \dfrac{2}{n} \iff c = \dfrac{n-1}{n}$$
            Y hemos llegado a que $$h(t) = c t^\alpha = \dfrac{n-1}{n} t^{1/2} = \dfrac{n-1}{n} \sqrt{t} \quad t > \theta^2$$

            Por construcción $h(T)$ es insesgada en $\theta$, y además $$h(t) = \dfrac{n-1}{n} \sqrt{t} > 0$$ puesto que $n \geqslant 2$ y $t > \theta^2 > 0$
            luego $\I(h) \subseteq \Rmas = \Theta$, de tal manera que $h(T)$ también es estimador. Queda comprobar que tiene momento de segundo orden finito. \\

            Ello se cumplirá en caso de que $$E_\theta[h(T)^2] < + \infty \iff \left( \dfrac{n-1}{n} \right)^2 E_\theta[T] < +\infty 
            \iff E_\theta[T] < +\infty$$

            Calculamos $E_{\theta}[T]$

            $$E_\theta[T] \stackrel{def}{=} \int_{\theta^2}^{+\infty} t f_T(t) dt = \dfrac{n \theta^n}{2} \int_{\theta^2}^{+\infty}
            t^{-n/2} dt$$

            Y como ya se ha visto antes, dicha integral converge si y solo si $$-n/2 < -1 \iff -n < -2 \iff n > 2 \iff n \geqslant 3$$ En tal caso,
            la integral resulta ser

            $$\int_{\theta^2}^{+\infty} t^{-n/2} dt = \dfrac{\theta^{2^{1-n/2}}}{n/2-1}$$

            y juntando todo 
            $$E_\theta[(h(T))^2] = \left( \dfrac{n-1}{n} \right)^2 E_\theta[T] = 
            \left( \dfrac{n-1}{n} \right)^2 \dfrac{n \theta^n}{2} \dfrac{\theta^{2^{(1-n/2)}}}{n/2-1} = $$
            $$\dfrac{(n-1)^2}{n^{\cancel{2}}} \dfrac{\cancel{n} \cancel{\theta^n}}{\cancel{2}} \dfrac{\cancel{2}}{n-2} 
            \dfrac{\theta^{2}}{\cancel{\theta^{n}}} = \dfrac{(n-1)^2}{n(n-2)} \theta^2 < +\infty$$

            imponiendo que $n \geqslant 3$ (para $n=2$ existe el estimador insesgado, pero no tiene momento de segundo orden finito).
            Por lo tanto, por el Teorema de Lehmann-Scheffé, podemos concluir que $E[h(T)/T] = h(T)$ es el UMVUE para $\theta$, y existe 
            siempre que $n \geqslant 3$. \\

            Respecto a la eficiencia, sabemos por un corolario visto en teoría que solo existen estimadores eficientes para familias
            de tipo exponencial (además de regulares en el sentido de Fréchet--Cramér--Rao), pero al principio se ha comprobado que la 
            familia del problema no podía serlo, dado que el espacio muestral $\X$ dependía del parámetro $\theta$, por lo que,
            como no existen estimadores insesgados eficientes de $\theta$, en particular el UMVUE obtenido no es eficiente.


            \item[b)] Sea $$ f_\theta(x)=\theta T(x)e^{-\theta x}, \qquad x>0,\ \theta>0,$$
            en una familia regular según Fréchet--Cramér--Rao.
            \begin{enumerate}
                \item[b1)] Sabiendo que $I_{X_1,\dots,X_n}(\theta)=\dfrac{2n}{\theta^2}$, calcular
                $\mathrm{E}[X]$ y $\mathrm{Var}[X]$. \\

                Por ser regular según FCR, sabemos que se verifica que $$\E_\theta \left[\dfrac{\partial \ln f_\theta(x)}{\partial \theta}\right] = 0$$
                Así
                $$\ln f_\theta(x) = \ln \theta + \ln (T(x)) - \theta x \Longrightarrow \dfrac{\partial \ln f_\theta(x)}{\partial \theta} = \dfrac{1}{\theta} - x$$
                La esperanza de $X$ puede obtenerse ya
                $$\E_\theta \left[\dfrac{1}{\theta} - X \right] = \E_\theta \left[\dfrac{\partial \ln f_\theta(x)}{\partial \theta}\right] = 0 
                \iff \E_\theta \left[\dfrac{1}{\theta} \right] - E[X] = 0 \iff E[X] = \dfrac{1}{\theta}$$
                
                Como $$\V_\theta \left[ \dfrac{\partial \ln f_\theta(x)}{\partial \theta} \right] = I_X(\theta)$$ usando la aditividad de la función de información de Fisher respecto a la v.a. y respecto a la m.a.s. de la v.a., 
                deducimos que $$I_{X_1, \ldots, X_n}(\theta) = n I_X(\theta) \iff \dfrac{2n}{\theta^2} = n I_X(\theta) \iff I_X(\theta) = \dfrac{2}{\theta^2}$$
                de donde 
                $$\V_\theta \left[ \dfrac{1}{\theta} - X \right] = \V_\theta(X) = I_X(\theta) = \dfrac{2}{\theta^2}$$

                \item[b2)] Sabiendo que $$\sum_{i=1}^n \dfrac{T(X_i)}{n}$$ es un estimador eficiente de $2/\theta$, calcular $T(x)$. \\
                
                Sea $g(\theta) = 2 / \theta$. Denotamos $\overline{T} \stackrel{not}{\equiv} \sum_{i=1}^n T(X_i)/n$. Como $\overline{T}$ es 
                eficiente, en particular, por definición, alcanza la cota de FCR, es decir, existe $a(\theta) \neq 0$ tal que
                $$P_\theta \left[\dfrac{\partial \ln f_{\theta}^n (X_1, \ldots, X_n)}{\partial \theta} = a(\theta) (\overline{T} - g(\theta)) \right] = 1$$

                Obtenemos la función conjunta
                $$f_{\theta}^n (x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^n f_\theta(x_i) = \prod_{i=1}^n 
                \theta T(x_i)e^{-\theta x_i} = \theta^n \prod_{i=1}^n T(x_i)e^{-\theta x_i}$$
                ahora aplicamos logaritmos y calculamos su parcial respecto de $\theta$
                $$\ln f_{\theta}^n (x_1, \ldots, x_n) = n \ln \theta + \sum_{i=1}^n (\ln T(x_i) - \theta x_i) \Longrightarrow$$
                $$\dfrac{\partial \ln f_{\theta}^n (X_1, \ldots, X_n)}{\partial \theta} = \dfrac{n}{\theta} - \sum_{i=1}^n X_i $$
                Por otro lado,
                $$a(\theta) (\overline{T} - g(\theta)) = a(\theta) \left(\sum_{i=1}^n \dfrac{T(X_i)}{n} - \dfrac{2}{\theta} \right)$$
                Igualando
                $$\dfrac{n}{\theta} - \sum_{i=1}^n X_i = \dfrac{\partial \ln f_{\theta}^n (X_1, \ldots, X_n)}{\partial \theta}
                a(\theta) (\overline{T} - g(\theta)) = a(\theta) \left(\sum_{i=1}^n \dfrac{T(X_i)}{n} - \dfrac{2}{\theta} \right)$$
                El lado izquierdo es una función lineal, que depende de la muestra solo a través de $\sum_{i=1}^n X_i$, 
                Por la igualdad de la cota de FCR, el lado derecho debe hacerlo de la misma forma, luego deben existir constantes, $\alpha, \beta \in \R$
                tales que $$\sum_{i=1}^n T(X_i) = \sum_{i=1}^n (\alpha \cdot X_i + \beta)$$ 
                de donde se deduce que $T(x) = \alpha x + \beta$. 
                Usando que $\overline{T}$ es eficiente para $g(\theta)$,
                en particular también es insesgado en $g(\theta)$, es decir, verifica
                $$\E_\theta[\overline{T}] = g(\theta) = \dfrac{2}{\theta}$$
                y por linealidad de la esperanza, y por ser $X_i$ pertenecientes a una m.a.s. de $X$ v.a. $i=1, \ldots, n$, entonces
                $$\E_\theta[\overline{T}] = \E_\theta \left[ \dfrac{1}{n} \sum_{i=1}^n T(X_i) \right] = 
                \dfrac{1}{n} \sum_{i=1}^n \E_\theta[T(X_i)] \stackrel{i.d.}{=} \dfrac{1}{n} \cdot n \E_\theta[T(X)] = \E_\theta[T(X)]$$

                Podemos entonces hallar $\E_\theta[T]$

                $$\dfrac{2}{\theta} = \E_\theta[\overline{T}] = \E_\theta[T(X)] = \alpha \E_\theta[X] + \beta = \alpha \dfrac{1}{\theta} + \beta = 
                \dfrac{\alpha}{\theta} + \beta$$

                Necesariamente, $\alpha = 2$ y $\beta = 0$, luego

                $$T(x) = 2x$$
            \end{enumerate}
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}~
        \begin{enumerate}
            \item[a)] Enunciar y demostrar el Teorema de Zehna definiendo previamente los siguientes
            conceptos: función de verosimilitud de un parámetro, función de verosimilitud de una
            función paramétrica y estimador máximo verosímil para funciones paramétricas. \\
            \begin{definicion}[Función de Verosimilitud de un Parámetro]\label{definicion:d1}
                Sea $X$ una v.a. con distribución en una familia paramétrica de distribuciones, $\{F_\theta, \theta \in \Theta\}$. Sea
                $f_\theta(x)$ la f.m.p. (caso discreto) ó la f.d.d. (caso continuo) de $X$. Se considera $X_1, \ldots, X_n$ una m.a.s. de $X$ y 
                sea $f_\theta^n(x_1, \ldots, x_n)$ su f.m.p. ó f.d.d. (respectivamente) conjunta con $\theta \in \Theta$. Para cada $x_1, \ldots, x_n$
                realización muestral, se define la \textit{función de verosimilitud} asociada a dichos valores de la muestra como una función de $\theta$
                de la siguiente forma: 
                \Func{L_{x_1, \ldots, x_n}}{\Theta}{\Rmas \cup \{0\}}{\theta}{L_{x_1, \ldots, x_n}(\theta) = f_\theta^n(x_1, \ldots, x_n)}
            \end{definicion}

            \begin{definicion}[Función de Verosimilitud de una Función Paramétrica]\label{definicion:d2}
                Sea $g: \Theta \to \Lambda$ una función paramétrica. En el contexto de la Definición \ref{definicion:d1}, para cada $x_1, \ldots, x_n$
                realización muestral, se define la \textit{función de verosimilitud} de $\lambda = g(\theta)$ asociada a dicha realización como: 
                \Func{M_{x_1, \ldots, x_n}}{\Lambda}{\Rmas \cup \{0\}}{\lambda}{M_{x_1, \ldots, x_n}(\lambda) = \sup_{\theta \in g^{-1}(\lambda)} L_{x_1, \ldots, x_n}(\theta)}
            \end{definicion}

            \begin{definicion}[Estimador Máximo Verosímil para Funciones Paramétricas]\label{definicion:d3}
                Un estimador $\hat{\lambda}(X_1, \ldots, X_n)$ de $\lambda$ es \textit{estimador de máxima verosimilitud} (EMV) de $\lambda$ si:
                $$\forall (x_1, \ldots, x_n) \in \X^n, \quad M_{x_1, \ldots, x_n}(\hat{\lambda}(x_1, \ldots, x_n)) = \max_{\lambda \in \Lambda} M_{x_1, \ldots,
                x_n} (\lambda)$$
            \end{definicion}

            \begin{teo}[de Invarianza de Zehna]\label{teo:t1}
                Sea $X$ una v.a. con distribución en una familia paramétrica de distribuciones, $\{F_\theta, \theta \in \Theta\}$. Sea $X_1, \ldots, X_n$
                una m.a.s. de $X$. Sea $g$ una función medible. Si $\hat{\theta}(X_1, \ldots, X_n)$ es EMV de $\theta$, entonces 
                $g(\hat{\theta}(X_1, \ldots, X_n))$ es EMV de $g(\theta)$.
            \end{teo}

            \begin{proof}
                Sea $\lambda = g(\theta)$ y, fijada una realización muestral, $(x_1, \ldots, x_n) \in \X^n$, notemos 
                $\hat{\lambda} \equiv g(\hat{\theta}(x_1, \ldots, x_n))$ (de esta manera, $\hat{\theta}(x_1, \ldots, x_n) \in g^{-1}(\hat{\lambda})$). \\

                Obtenemos la función de verosimilitud de la función paramétrica usando la Definición \ref{definicion:d2}:
                $$M_{x_1, \ldots, x_n} (\hat{\lambda}) \stackrel{def}{=} \sup_{\theta \in g^{-1}(\hat{\lambda})} L_{x_1, \ldots, x_n}(\theta) 
                \stackrel{\hat{\theta} \in g^{-1}(\hat{\lambda})}{=} L_{x_1, \ldots, x_n} (\hat{\theta}(x_1, \ldots, x_n)) \quad (*)$$
                donde en la última igualdad se ha usado que $\hat{\theta}(X_1, \ldots, X_n)$ es EMV de $\theta$, luego maximiza la función de 
                verosimilitud de la Definición \ref{definicion:d1}. Ahora, vemos que 
                $$\forall \lambda \in \Lambda, \quad M_{x_1, \ldots, x_n} (\lambda) = \sup_{\theta \in g^{-1}(\lambda)} L_{x_1, \ldots, x_n}(\theta) \leqslant
                \sup_{\theta \in \Theta} L_{x_1, \ldots, x_n}(\theta) = L_{x_1, \ldots, x_n} (\hat{\theta}(x_1, \ldots, x_n)) \quad (**)$$
                la última igualdad nuevamente por ser $\hat{\theta}(X_1, \ldots, X_n)$ es EMV de $\theta$. Deducimos entonces que 
                $$\forall \lambda \in \Lambda, \quad M_{x_1, \ldots, x_n} (\lambda) \stackrel{(**)}{\leqslant} L_{x_1, \ldots, x_n} (\hat{\theta}(x_1, \ldots, x_n))
                \stackrel{(*)}{=} M_{x_1, \ldots, x_n} (\hat{\lambda}) $$
                Es decir, $\hat{\lambda} = g(\hat{\theta}(x_1, \ldots, x_n))$ maximiza $M_{x_1, \ldots, x_n}$, para cada $(x_1, \ldots, x_n) \in \X^n$. 
                Por la Definición \ref{definicion:d3}, $g(\hat{\theta}(X_1, \ldots, X_n))$ es el EMV de $\lambda = g(\theta)$.
            \end{proof}

            \item[b)] Calcular la función de verosimilitud de $$\lambda=(\theta-1)^2$$
            asociada a una realización muestral cuyo máximo valor es $3$ si $$ f_\theta(x)=e^{x-\theta}, \qquad x \leqslant \theta, \theta>0.$$
            Vemos que $\X = \left]-\infty, \theta \right]$. Sea $(x_1, \ldots, x_n) \in \X^n$ tal que $x_{(n)} = 3$. Calculamos la función de densidad 
            conjunta, asumiendo a partir de ahora que $\theta > 0$ (en otro caso, $f_\theta(x) = 0$).
            $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^n f_\theta(x_i)$$
            Ahora, vemos que $x \leqslant \theta \iff I_{\R_0^{-}}(x - \theta) = 1$ de donde se deduce que 
            $$\prod_{i=1}^n f_\theta(x_i) = \prod_{i=1}^n e^{x_i - \theta} \neq 0 \iff I_{\R_0^{-}}(x_i - \theta) = 1 \quad \forall i = 1, \ldots, n$$
            y a su vez
            $$x_i \leqslant \theta \quad \forall i = 1, \ldots, n \iff x_{(n)} \leqslant \theta \iff I_{\R_0^{-}}(x_{(n)} - \theta) = 1$$
            luego
            $$f_\theta^n(x_1, \ldots, x_n) = \prod_{i=1}^n e^{x_i - \theta} I_{\R_0^{-}}(x_{(n)} - \theta) = 
            e^{\left(\displaystyle\sum_{i=1}^n x_i - n \theta \right)} I_{\R_0^{-}}(x_{(n)} - \theta)$$
            Por la Definición \ref{definicion:d1}
            $$L_{x_1, \ldots, x_n}(\theta) = f_\theta^n(x_1, \ldots, x_n) = e^{\left(\displaystyle\sum_{i=1}^n x_i - n \theta \right)} I_{\R_0^{-}}(x_{(n)} - \theta)
            \quad \forall \theta \in \Theta = \Rmas$$
            El enunciado nos dice que $\lambda = g(\theta) = (\theta-1)^2$. Por la Definición \ref{definicion:d2}
            $$M_{x_1, \ldots, x_n}(\lambda) = \sup_{\theta \in g^{-1}(\lambda)} L_{x_1, \ldots, x_n}(\theta)$$ 
            Resolvemos $(\theta-1)^2 = \lambda$ para expresar $\theta$ explícitamente en función de $\lambda$. Vemos que $\lambda \geqslant 0$, y 
            $(\theta - 1)^2 = \lambda \iff \theta - 1 = \pm \sqrt{\lambda} \iff \theta = 1 \pm \sqrt{\lambda}$. \\
            A priori habría dos candidatos para cada $\lambda$. Sin embargo, por restricciones del problema, $3 = x_{(n)} \leqslant \theta$, lo que implica que
            $$\theta = 1 + \sqrt{\lambda} \geqslant 3 \iff \sqrt{\lambda} \geqslant 2 \iff \lambda \geqslant 4$$
            $$\theta = 1 - \sqrt{\lambda} \geqslant 3 \iff - \sqrt{\lambda} \geqslant 2 \iff \sqrt{\lambda} \leqslant -2$$
            La última opción no puede darse por ser $\lambda \geqslant 0$, por tanto, nos quedamos con la primera. Así, si $\lambda \geqslant 4$ (en otro caso,
            $M_{x_1, \ldots, x_n}(\lambda) = 0$)
            $$M_{x_1, \ldots, x_n}(\lambda) = L_{x_1, \ldots, x_n} (1 + \sqrt{\lambda}) = e^{\left(\displaystyle\sum_{i=1}^n x_i - n (1 + \sqrt{\lambda}) \right)}$$
            Aunque no se pide, como $e^{\sum_{i=1}^n x_i}$ es fijo, y $1 + \sqrt{\lambda}$ es creciente como función de $\lambda$ y 
            $e^{-n(1+\sqrt{\lambda})}$ es decreciente como función de $\lambda$, y $M_{x_1, \ldots, x_n}(\lambda) \neq 0 \iff \lambda \in [4, +\infty[$, entonces
            el máximo se alcanza en el extremo inferior del intervalo, es decir $\hat{\lambda}(x_1, \ldots, x_n) = 4$. Esto puede comprobarse también con el 
            Teorema \ref{teo:t1} pues $L_{x_1, \ldots, x_n}(\theta)$ es decreciente en $\left[x_{(n)}, +\infty \right[ = 
            \left[3, + \infty \right[$. Por el mismo razonamiento, $\hat{\theta}(x_1, \ldots, x_n) = 3$, luego $\hat{\lambda} = (\hat{\theta} - 1)^2 = (3-1)^2 = 4$.
        \end{enumerate}
    \end{ejercicio}

    \newpage

    \begin{ejercicio}
        Sea $(X_1,\dots,X_n)$ una m.a.s.\ de $X$ v.a.\ con función de densidad
        \begin{gather}
            f_\theta(x)=\dfrac{1}{\sqrt{8\theta}\sqrt{x-1}}, \qquad 1<x<2\theta+1.
            \tag{3}
        \end{gather}
        Deducir el test más potente de tamaño arbitrario para contrastar
        $$H_0:\theta=\theta_0 \quad \text{frente a} \quad H_1:\theta=\theta_1$$
        siendo $\theta_1<\theta_0$. Calcular la potencia. \\

        Tenemos un contraste de hipótesis simple frente a hipótesis simple, por lo que sabemos por el Lema de Neyman-Pearson que el 
        Test de Neyman-Pearson será el más potente, para cualquier tamaño $\alpha \in \left]0,1 \right]$. Si el tamaño fuera $\alpha = 0$, 
        sabemos entonces que el test más potente es 
        $$\varphi_{0}(X_1, \ldots, X_n) = \begin{cases}
            1 &\text{si } f_0^n (X_1, \ldots, X_n) = 0 \\
            0 &\text{si } f_0^n (X_1, \ldots, X_n) > 0
        \end{cases}$$
        denotando por $f^n_0 \stackrel{not}{\equiv} f^n_{\theta_0}$ a la función conjunta bajo la hipótesis nula, y lo mismo con $f^n_1$ y la función conjunta
        bajo la hipótesis alternativa. Como el tamaño es $0$, entonces $\E_{\theta_0}[\varphi] = 0 \Longrightarrow \varphi = 0$ bajo $H_0$ (no rechaza
        nunca). Como $\theta_1 < \theta_0$, no existe ninguna región en la que $f_{\theta_0} = 0$ y $f_{\theta_1} > 0$, luego el test
        más potente es $\varphi \equiv 0$, y entonces su potencia en $\theta_1$ es 
        $\beta_{\varphi}(\theta_1) = \E_{\theta_1}[\varphi] = \E_{\theta_1}[0] = 0$.\\
        
        Para dar el test de hipótesis, obtenemos las funciones de densidad conjuntas bajo la hipótesis nula y 
        bajo la hipótesis alternativa. \\

        A partir de ahora asumimos que $1 < x_i \quad \forall i = 1, \ldots, n$. De otra manera, $f_\theta^n(x_1, \ldots, x_n) = 0$. 
        La función de densidad conjunta es $$f_\theta^n(x_1, \ldots, x_n) \stackrel{indep.}{=} \prod_{i=1}^n \frac{1}{\sqrt{8\theta}\sqrt{x_i-1}}$$ 
        Y vemos que $$x<2\theta+1 \iff I_{\Rmenos}(x - (2\theta + 1)) = 1$$
        luego $$x_i < 2 \theta + 1 \quad \forall i = 1, \ldots, n \iff x_{(n)} < 2 \theta + 1 \iff I_{\Rmenos}(x_{(n)} - (2\theta + 1)) = 1$$
        Por tanto $$f_\theta^n(x_1, \ldots, x_n) = \prod_{i=1}^n \frac{1}{\sqrt{8\theta}\sqrt{x_i-1}} I_{\Rmenos}(x_{(n)} - (2\theta + 1)) = $$
        $$\dfrac{1}{(\sqrt{8 \theta})^n} \prod_{i=1}^n \frac{1}{\sqrt{x_i-1}} I_{\Rmenos}(x_{(n)} - (2\theta + 1)) =
        (8\theta)^{-n/2} \prod_{i=1}^n (x_i - 1)^{-1/2} I_{\Rmenos}(x_{(n)} - (2\theta + 1))$$
        Se tiene entonces que
        $$f_0^n(X_1, \ldots, X_n) = (8\theta_0)^{-n/2} \prod_{i=1}^n (X_i - 1)^{-1/2} I_{\Rmenos}(X_{(n)} - (2\theta_0 + 1))$$
        $$f_1^n(X_1, \ldots, X_n) = (8\theta_1)^{-n/2} \prod_{i=1}^n (X_i - 1)^{-1/2} I_{\Rmenos}(X_{(n)} - (2\theta_1 + 1))$$
        Consideramos el cociente 
        $$\dfrac{f_1^n(X_1, \ldots, X_n)}{f_0^n(X_1, \ldots, X_n)} = 
        \dfrac{(\cancel{8}\theta_1)^{-n/2} \cancel{\prod_{i=1}^n (X_i - 1)^{-1/2}} I_{\Rmenos}(X_{(n)} - (2\theta_1 + 1))}
        {(\cancel{8}\theta_0)^{-n/2} \cancel{\prod_{i=1}^n (X_i - 1)^{-1/2}} I_{\Rmenos}(X_{(n)} - (2\theta_0 + 1))} = $$
        $$\left(\dfrac{\theta_0}{\theta_1} \right)^{n/2} \dfrac{I_{\Rmenos}(X_{(n)} - (2\theta_1 + 1))}{I_{\Rmenos}(X_{(n)} - (2\theta_0 + 1))}$$
        y como $\theta_1 < \theta_0$ por el enunciado, entonces $2 \theta_1 + 1 < 2 \theta_0 + 1$, y distinguimos como sigue:
        \begin{enumerate}
            \item Si $X_{(n)} \leqslant 2 \theta_1 + 1$, ambas funciones indicadoras valen $1$
            $$\dfrac{f_1^n(X_1, \ldots, X_n)}{f_0^n(X_1, \ldots, X_n)} = \left(\dfrac{\theta_0}{\theta_1} \right)^{n/2}$$
            \item Si $2 \theta_1 +1 < X_{(n)} \leqslant 2 \theta_0 + 1$, la función indicadora del numerador valdría $0$ y la del denominador $1$
            $$\dfrac{f_1^n(X_1, \ldots, X_n)}{f_0^n(X_1, \ldots, X_n)} = 0$$
            \item Si $X_{(n)} > 2 \theta_0 + 1$, entonces $f_0^n(X_1, \ldots, X_n) = 0$, luego el suceso tendrá probabilidad $0$ bajo
            la hipótesis nula, y no influiría en el tamaño del test.
        \end{enumerate}

        Tenemos hasta ahora que
        $$\dfrac{f_1^n(X_1, \ldots, X_n)}{f_0^n(X_1, \ldots, X_n)} = \begin{cases}
            k \quad \text{ si } X_{(n)} \leqslant 2 \theta_1 + 1 \\
            0 \quad \text{ si } 2 \theta_1 +1 < X_{(n)} \leqslant 2 \theta_0 + 1
        \end{cases} \quad k = \left(\dfrac{\theta_0}{\theta_1} \right)^{n/2}$$

        El cociente solo toma dos valores, y, en particular, nunca ocurre que $$f_1^n(X_1, \ldots, X_n) > k f_0^n(X_1, \ldots, X_n)$$
        de tal manera que el test con tamaño $\alpha$ más potente nunca rechazará (a priori) con probabilidad $1$ la hipótesis nula. La región en la que
        se puede producir el rechazo de $H_0$ es 
        $$\mathcal{C} = \{(x_1, \ldots, x_n) \in \X^n : x_{(n)} \leqslant 2 \theta_1 + 1\}$$
        Dicho rechazo, por la forma del test, solo puede darse cuando $f_1^n(X_1, \ldots, X_n) = k f_0^n(X_1, \ldots, X_n)$, y el 
        Test de Neyman-Pearson de tamaño $\alpha$ es 
        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            \gamma(X_1, \ldots, X_n) &\text{si } X_{(n)} \leqslant 2 \theta_1 + 1 \\
            0 &\text{si } X_{(n)} > 2 \theta_1 + 1
        \end{cases}$$
        donde $\gamma(X_1, \ldots, X_n) = \gamma \in [0,1]$ sabemos por teoría que es constante. Para determinar $\gamma$, se impone el tamaño $\alpha$,
        es decir $$\alpha = P_{\theta_0}[(X_1, \ldots, X_n) \in \mathcal{C}] = \gamma P_{\theta_0} (X_{(n)} \leqslant 2 \theta_1 + 1)$$
        Hallamos $F_{\theta_0}(t)$
        $$F_{\theta_0}(t) = \int_{1}^{t} \dfrac{1}{\sqrt{8\theta_0}\sqrt{x-1}} dx = 
        \dfrac{1}{\sqrt{8\theta_0}} \int_{1}^{t} (x-1)^{-1/2} dx = \dfrac{1}{\sqrt{8\theta_0}} \left[ \dfrac{(x-1)^{1/2}}{1/2} \right]_1^{t} = $$
        $$\dfrac{2 \sqrt{t-1}}{\sqrt{8 \theta_0}} = \dfrac{\cancel{2} \sqrt{t-1}}{\cancel{2} \sqrt{2 \theta_0}} = \sqrt{\dfrac{t-1}{2 \theta_0}}$$
        Sabemos que la función de distribución del máximo verifica
        $$P_{\theta_0}(X_{(n)} \leqslant 2 \theta_1 + 1) = (F_{\theta_0}(2 \theta_1 + 1))^n = \left( \sqrt{\dfrac{2 \theta_1}{2 \theta_0}} \right)^n = 
        \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}$$
        y juntando todo
        $$\alpha = \gamma P_{\theta_0} (X_{(n)} \leqslant 2 \theta_1 + 1) = \gamma \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}$$
        de donde
        $$\gamma = \dfrac{\alpha}{p_0}, \quad p_0 = \left( \dfrac{\theta_1}{\theta_0} \right)^{n/2}$$

        Como $\gamma \in [0,1] \iff 0 \leqslant \alpha / p_0 \leqslant 1$, pero $\alpha \in [0,1]$ y 
        $p_0 > 0$, se tiene que $$\gamma \in [0,1] \iff \alpha / p_0 \leqslant 1
        \iff \alpha \leqslant p_0$$

        Así, si $\alpha \leqslant p_0$, la potencia en $\theta_1$ es 

        $$\beta_\varphi(\theta_1) = \gamma \cdot P_{\theta_1}[(X_1, \ldots, X_n) \in \mathcal{C}]$$
        Como bajo la hipótesis alternativa $H_1$, siempre se cumple que $X_{(n)} \leqslant 2 \theta_1 + 1$, entonces $\beta_\varphi(\theta_1) = \gamma$. \\

        Si, por el contrario, $\alpha > p_0$, hay que rechazar también fuera de $\mathcal{C}$ para conseguir el tamaño $\alpha$, 
        por lo tanto, el Test de Neyman-Pearson sería

        $$\varphi(X_1, \ldots, X_n) = \begin{cases}
            1 & \text{si } X_{(n)} \leqslant 2 \theta_1 + 1 \\
            \gamma(X_1, \ldots, X_n) \equiv \gamma &\text{si } 2 \theta_1 + 1 < X_{(n)} \leqslant 2 \theta_0 + 1 \\
            0 &\text{si } X_{(n)} > 2 \theta_0 + 1
        \end{cases}$$

        Imponemos el tamaño
        $$\alpha = p_0 + \gamma(1-p_0) \iff \gamma = \dfrac{\alpha - p_0}{1 - p_0}$$
        Y en este caso, bajo $H_1$, $0 \leqslant P_{\theta_1}[2 \theta_1 + 1 < X_{(n)} \leqslant 2 \theta_0 + 1] \leqslant P_{\theta_1}[2 \theta_1 + 1 < X_{(n)}] = 0$, 
        donde el primera desigualdad se ha usado que 
        $$\{(x_1, \ldots, x_n) \in \X^n : 2 \theta_1 + 1 < X_{(n)} \leqslant 2 \theta_0 + 1 \} \subseteq 
        \{(x_1, \ldots, x_n) \in \X^n : 2 \theta_1 + 1 < X_{(n)} \}$$
        y luego que $P_{\theta_1}[X_{(n)} \leqslant 2 \theta_1 + 1] = 1 \iff P_{\theta_1}[2 \theta_1 + 1 < X_{(n)}] = 0$. Finalmente, la
        potencia en este caso es 
        $$\beta_\varphi(\theta_1) = E_{\theta_1}[\varphi(X_1, \ldots, X_n)] = 1 \cdot P_{\theta_1}[X_{(n)} \leqslant 2 \theta_1 + 1] + $$$$
        \gamma \cdot P_{\theta_1}[2 \theta_1 + 1 < X_{(n)} \leqslant 2 \theta_0 + 1] + 
        0 \cdot P_{\theta_1}[X_{(n)} > 2 \theta_0 + 1] = 1 \cdot 1 + \gamma \cdot 0 + 0 \cdot 0 = 1$$
    \end{ejercicio}

    \begin{ejercicio}~
        \begin{enumerate}
            \item[a)] Test de Kolmogorov--Smirnov.
            \begin{enumerate}
                \item[i)] Plantear el problema de contraste. \\
                
                Sea una función de distribución específica $F_0$, y sea $(X_1, \ldots, X_n)$ una m.a.s. de una v.a. $X$ continua que se distribuye
                según una función de distribución $F$ que es completamente desconocida. El constraste a resolver es
                $$\begin{cases}
                    H_0 : F = F_0 \\
                    H_1 : F \neq F_0
                \end{cases}$$

                \item[ii)] Dar el valor del estadístico. \\
                
                El estadístico que se usa para resolver el problema es el estadístico de Kolmogorov-Smirnov

                $$D(X_1, \ldots, X_n) = \sup_{x \in \R} |F_{X_1, \ldots, X_n}^{*}(x) - F_0(x)|$$

                \item[iii)] Enunciar el teorema que justifica su uso. \\
                
                El test se basa en el teorema de Glivenko-Cantelli: \\

                \begin{teo}[de Glivenko-Cantelli]
                    Sea $\{X_n\}_{n \in \N}$ una sucesión de v.a.i.i.d. con función de distribución común $F$. Si $F_{X_1, \ldots, X_n}^{*}$
                    es la función de distribución muestral asociada a la m.a.s. $(X_1, \ldots, X_n)$, se verifica que $F_{X_1, \ldots, X_n}^{*}$
                    converge casi seguramente y uniformemente a la función de distribución de $X$, $F$.
                    $$P \left\{ \lim\limits_{n \to +\infty} \sup_{x \in \R} |F_{X_1, \ldots, X_n}^{*}(x) - F(x)| = 0 \right\} = 1$$
                \end{teo}

                \item[iv)] Ventajas frente al test $\chi^2$. \\
                
                Se asume que nos referimos al test $\chi^2$ de Pearson. \\
                \begin{enumerate}
                    \item El test de Kolmogorov-Smirnov no necesita hacer particiones de los datos, mientras que el test $\chi^2$ de Pearson sí.
                    \item Para v.a. continuas, es más apropiado usar el test de Kolmogorov-Smirnov que el test $\chi^2$ de Pearson.
                    \item Bajo $H_0$, si $F_0$ es continua, la distribución del estadístico de Kolmogorov-Smirnov no depende de $F_0$, mientras
                    que el test $\chi^2$ de Pearson es asintótico.
                \end{enumerate}

            \end{enumerate}
            \item[b)] Se cuentan el número de tutorías a lo largo de un curso por $50$ profesores.
            Se quiere contrastar si el número de tutorías por profesor se puede describir
            mediante una distribución de Poisson.

            \begin{center}
                \begin{tabular}{c|cccccc}
                    Número de tutorías   & 0 & 1 &  2 & 3  & 4  & 5 \\ \hline
                    Número de profesores & 2 & 5 & 10 & 14 & 12 & 7
                \end{tabular}
            \end{center}

            En este caso, como la variable aleatoria es discreta, y tenemos frecuencias, es más apropiado usar el test $\chi^2$ de Pearson.
            Sea $$X \equiv \text{``Número de tutorías por un profesor''}$$ El contraste a resolver es
            $$\begin{cases}
                H_0 : X \rightsquigarrow \mathcal{P}(\lambda) \\
                H_1 : X \not\rightsquigarrow \mathcal{P}(\lambda)
            \end{cases} \quad \lambda > 0$$
            Vemos que la hipótesis nula es compuesta, luego primeramente debemos estimar el valor del parámetro $\lambda$. Por el método de los momentos, 
            un estimador del parámetro de la distribución de Poisson
            $\lambda$ es la media muestral. Así
            $$\hat{\lambda} = \overline{X} = \dfrac{0 \cdot 2 + 1 \cdot 5 + 2 \cdot 10 + 3 \cdot 14 + 4 \cdot 12 + 5 \cdot 7}{50} = \dfrac{150}{50} = 3$$
            El constraste adaptado sería 
            $$\begin{cases}
                H_0 : X \rightsquigarrow \mathcal{P}(3) \\
                H_1 : X \not\rightsquigarrow \mathcal{P}(3)
            \end{cases}$$
            Denotemos por $N_1, \ldots, N_k$ las frecuencias observadas en las $k$ clases consideradas, y por
            $$\hat{p}_i = P_{\hat{\lambda}}(X \in A_i), \quad i = 1, \ldots, k$$
            las probabilidades teóricas bajo $H_0$ con el parámetro $\lambda$ estimado por $\hat{\lambda}$. \\

            El estadístico de contraste viene dado por

            $$\hat{\chi}(N_1, \ldots, N_k) = \sum_{i=1}^k \dfrac{(N_i - n \hat{p}_i)^2}{n \hat{p}_i}$$

            Como el parámetro se ha estimado a partir de los mismos datos, la distribución asintótica bajo $H_0$ es 
            $$\hat{\chi}(N_1, \ldots, N_k) \rightsquigarrow_{H_0} \chi^2(k - q - 1)$$
            con $q=1$ el número de parámetros estimados.

            Por teoría, para poder aplicar el test hay que verificar que $$E^*_{i} = n\hat{p}_i \geqslant 5 \quad \forall i = 1, \ldots, k, \quad n = 50, \quad \hat{p}_i = P[X=i] = e^{-3} \dfrac{3^i}{i!}$$
            Primero vamos tanteando.
            $$E^*_0 = 50 \cdot \hat{p}_0 = 50 \cdot e^{-3} \dfrac{3^0}{0!} \approx 2.49$$
            $$E^*_1 = 50 \cdot e^{-3} \dfrac{3^1}{1!} \approx 7.47$$
            $$E^*_2 = 50 \cdot e^{-3} \dfrac{3^2}{2!} \approx 11.2$$
            $$E^*_3 = 50 \cdot e^{-3} \dfrac{3^3}{3!} \approx 11.2$$
            $$E^*_4 = 50 \cdot e^{-3} \dfrac{3^4}{4!} \approx 8.4$$
            $$E^*_5 = 50 \cdot e^{-3} \dfrac{3^5}{5!} \approx 5.05$$
            $$E^*_6 = 50 \cdot e^{-3} \dfrac{3^6}{6!} \approx 2.52$$

            Vemos que $E^{*}_i \geqslant 5 \quad \forall i = 1, \ldots, 5$.
            Sin embargo, $E^{*}_0, E^{*}_6 < 5$, y $x_{(k)} = 5$, por lo que una partición sería la siguiente
            $$A_1 = \{0,1\}, \quad A_i = \{i\}, \quad A_5 = \{\geqslant 5\}, \quad i = 2,3,4$$
            En este caso, $k=5$ (número de clases tras agrupar), luego
            $\hat{\chi}^2(N_1, \ldots, N_k) \rightsquigarrow \chi^2(3)$. Las frecuencias observadas, denotadas por $O_i, i= 1,\ldots,k$, son 
            $$O_1 = N_0 + N_1 = 2 + 5 = 7, \quad O_2 = 10, \quad O_3 = 14, \quad O_4 = 12, \quad O_5 = 7$$
            Las frecuencias esperadas son 
            $$E_1 = E^*_0 + E^*_1 \approx 2.49 + 7.47 = 9.96, \quad E_2 = E^*_2 \approx 11.2 \quad E_3 = E^*_3 \approx 11.2 
            \quad E_4 = E^*_4 \approx 8.4$$$$E_5 = 50 - \sum_{i=1}^{4} E_j \approx 50 - (9.96 + 11.2 + 11.2 + 8.4) = 9.24$$

            Obtenemos $$\chi^2_{exp} = \sum_{i=1}^{5} \dfrac{(O_i - E_i)^2}{E_i} = \dfrac{(7 - 9.96)^2}{9.96} + \dfrac{(10 - 11.2)^2}{11.2} +
            \dfrac{(14 - 11.2)^2}{11.2} + \dfrac{(12 - 8.4)^2}{8.4} + $$$$\dfrac{(7 - 9.24)^2}{9.24} \approx 3.79$$

            \newpage

            El test asintótico de tamaño $\alpha$ es

            $$\varphi(X_1, \ldots, X_n) = \begin{cases}
                1 &\text{si } \hat{\chi}^2(N_1, \ldots, N_k) \geqslant \chi^2_{k-1;\alpha} \\
                0 &\text{si } \hat{\chi}^2(N_1, \ldots, N_k) < \chi^2_{k-1;\alpha}
            \end{cases}$$

            con $$p-valor = P_{H_0}[\hat{\chi}^2(N_1, \ldots, N_k) \geqslant \chi^2_{exp}] \approx_{n \to +\infty} P[\chi^2(k-q-1) \geqslant \chi^2_{exp}]$$

            y $\chi^2_{exp}$ el valor del estadístico obtenido con la muestra observada. Usando que $k-q-1=3$, obtenemos
            $$p-valor \approx P[\chi^2(3) \geqslant 3.79] \approx 0.3$$

            Como el p-valor es grande (respecto a los niveles habituales de significación), no hay evidencia para rechazar $H_0$, luego puede suponerse que $X \rightsquigarrow \mathcal{P}(3)$.
        \end{enumerate}
    \end{ejercicio}


\end{document}
