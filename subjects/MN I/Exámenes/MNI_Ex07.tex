\documentclass[12pt]{article}

\input{../../_assets/preambulo.tex}
\usetikzlibrary{backgrounds}
\begin{document}
	
	% 1. Foto de fondo
	% 2. Título
	% 3. Encabezado Izquierdo
	% 4. Color de fondo
	% 5. Coord x del titulo
	% 6. Coord y del titulo
	% 7. Fecha
	
	
	\input{../../_assets/portada.tex}
	\portadaExamen{ffccA4.jpg}{MN I\\Examen VII}{Métodos Numéricos I. Examen VII}{MidnightBlue}{-8}{28}{2024}{Roxana Acedo Parra}
	
	\begin{description}
		\item[Asignatura] Métodos Numéricos I.
		\item[Curso Académico] 2024-25.
		\item[Grado] Doble Grado en Ingeniería Informática y Matemáticas.
		\item[Grupo] Único.
		\item[Profesor] Juan José Nieto Muñoz.
		\item[Fecha] 28 de mayo de 2025.
		\item[Duración] 2 horas.
		\item[Descripción] Segundo Parcial.
		\item[Observaciones] 
		Contiene respuestas a varias versiones, no a una única prueba. Los dos últimos ejercicios fueron comunes para todos y del primero eran 6 apartados distintos, no comunes entre los que estábamos haciendo el examen.
	\end{description}
	\newpage
	
	\begin{ejercicio}[3 puntos]
		Dado un natural $n \geq 3$, un vector no nulo $b \in \mathbb{R}^n$ y una matriz $A \in \mathcal{M}_{n \times n}(\mathbb{R})$, verificando:
		
		$$ \forall i = 1, 2, \ldots, n : a_{i,i} = -1, \quad \sum_{j=1}^{i-1} |a_{i,j}| + \sum_{j=i+1}^{n} |a_{i,j}| < 1, $$
		
		determina razonadamente la veracidad o falsedad de las siguientes afirmaciones.
		\begin{enumerate}[label=\alph*)]
			\item Existen matrices $A$ en estas condiciones que no tienen inversa.
			
			\item El sistema $(-A)x = b$ puede ser incompatible.
			
			\item El sistema $(-A)x = b$ tiene infinitas soluciones.
			
			\item El método iterativo de Jacobi aplicado a $Ax = 0$ converge.
			
			\item El método iterativo de Gauss-Seidel aplicado a $Ax = 0$ converge.
			
			\item La matriz, sin permutar ninguna fila, admite descomposición de Crout.
			
			\item La matriz, sin permutar ninguna fila, admite descomposición de Doolitle.
			
			\item La matriz, sin permutar ninguna fila, admite una descomposición de tipo $A = -LL^T$.
			
			\item La matriz $(-A)$ admite una descomposición de Cholesky.
			
			\item Si $\lambda \in \sigma(A + 2I) \cap \mathbb{R}$, entonces $\lambda > 0$.
			
			\item Si $\lambda \in \sigma(I - A)$, entonces $\lambda > 0$.
			
			\item Si $\lambda \in \sigma(A) \cap \mathbb{C}$, entonces $\Re(\lambda) < 0$.
			
			\item La matriz $A$ no tiene valor propio dominante.
			
			\item La matriz $A$ tiene valor propio dominante.
			
			\item La matriz $A$ tiene valor propio dominante $\lambda = -1$.
		\end{enumerate}
	\end{ejercicio}
	
	\begin{ejercicio}[3 puntos]
		Sea $A \in \mathbb{M}_{n \times n}(\mathbb{R})$ una matriz simétrica que, por lo tanto, sabemos que tiene $n$ vectores propios $\{v_1, \dots, v_n\}$ perpendiculares y se pueden tomar formando una base ortonormal de $\mathbb{R}^n$, es decir
		
		$$\|v_i\|_2^2 = \langle v_i, v_i \rangle = v_i^T v_i = 1 \quad \forall i, \quad \text{ y } \quad \langle v_i, v_j \rangle = v_i^T v_j = 0 \, \forall i \neq j.$$
		
		Además, en este caso suponemos que sus $n$ valores propios reales son distintos. Tomando cualquiera de ellos, por ejemplo $\lambda_n$, y su vector propio asociado $v_n$, consideramos la matriz: \\
		
		$A^* = A - \lambda_n v_n v_n^T,$ (observa que $v_n$ es columna y $v_n^T$ es fila, por lo que $v_n v_n^T$ es una matriz $n \times n$).
		
		\begin{enumerate}[label=\alph*)]
			\item Demuestra que $A^* v_n = 0$ y que $A^* v_i = \lambda_i v_i$ para $i \neq n$. 
			
			\item Determina el conjunto $\sigma(A^*)$ en función de $\sigma(A)$.
			
			\item Utiliza estos apartados para idear un método, combinando si es preciso el de las potencias usado varias veces, para calcular el espectro completo de $A$.
		\end{enumerate}
	\end{ejercicio}	
	
	\begin{ejercicio}[3 puntos]
		Considere el método iterativo: 
		$$\begin{cases}
			x^{(k+1)}_1 = \displaystyle \frac{1}{5} (4 x^{(k)}_2 + 3) \\
			\\
			x^{(k+1)}_2 = \displaystyle \frac{1}{6} (4 x^{(k+1)}_1 - 8)
		\end{cases}$$
		
		\begin{enumerate}[label=\alph*)]
			\item Determina un sistema $Ax = b$ con el que sea consistente.
			
			\item ¿Es convergente? En caso afirmativo, ¿a qué converge?
			
			\item Comenzando con $(x_1, x_2)^{(0)} = (1,0)$, determina el número mínimo de iteraciones que garantice que el error absoluto cometido al aproximar $A^{-1}b$ sea inferior a $10^{-5}$.
			
		\end{enumerate}
	\end{ejercicio}
	
	\newpage
	\setcounter{ejercicio}{0}
	
	% R (1)
	\begin{ejercicio}[3 puntos]
		Dado un natural $n \geq 3$, un vector no nulo $b \in \mathbb{R}^n$ y una matriz $A \in \mathcal{M}_{n \times n}(\mathbb{R})$, verificando:
		$$ \forall i = 1, 2, \ldots, n : a_{i,i} = -1, \quad \sum_{j=1}^{i-1} |a_{i,j}| + \sum_{j=i+1}^{n} |a_{i,j}| < 1, $$
		
		determina razonadamente la veracidad o falsedad de las siguientes afirmaciones.
		\begin{enumerate}[label=\alph*)]
			\item Existen matrices $A$ en estas condiciones que no tienen inversa.
			
				\textbf{Solución. }\fbox{Falso.} Resultado de clase: Toda matriz EDD tiene inversa.
			
			\item El sistema $(-A)x = b$ puede ser incompatible.
			\item El sistema $(-A)x = b$ tiene infinitas soluciones.
				
				\textbf{Solución. }\fbox{Falso.} Como $A$ es EDD, $-A$ también, tiene inversa y el SEL es compatible determinado.
				
			\item El método iterativo de Jacobi aplicado a $Ax = 0$ converge.
			\item El método iterativo de Gauss-Seidel aplicado a $Ax = 0$ converge.
			
				\textbf{Solución. }\fbox{Verdadero.} Resultado de clase: cuando $A$ es EDD, Jacobi y Gauss-Seidel convergen.
			
			\item La matriz, sin permutar ninguna fila, admite descomposición de Crout.
			\item La matriz, sin permutar ninguna fila, admite descomposición de Doolitle.
			
				\textbf{Solución. }\fbox{Verdadero.} Como A es EDD, entonces todas sus submatrices principales $A_k$ lo son, por tanto, todas las $A_k$ tienen inversa y, por ende, determinante no nulo. Resultado de clase: cuando todas las $A_k$ tienen determinante no nulo, la matriz admite factorización LU (en cualquier variante, Doolittle y Crout).
			
			\item La matriz, sin permutar ninguna fila, admite una descomposición de tipo $A=~-LL^T$.
			\item La matriz $(-A)$ admite una descomposición de Cholesky.
			
				\textbf{Solución. }\fbox{Falso.} En ambos casos, $A$ tendría que ser simétrica. Vale como contraejemplo cualquier matriz que cumpla lo de arriba y que no sea simétrica.
				
			\item Si $\lambda \in \sigma(A + 2I) \cap \mathbb{R}$, entonces $\lambda > 0$.
			\item Si $\lambda \in \sigma(I - A)$, entonces $\lambda > 0$.
			\item Si $\lambda \in \sigma(A) \cap \mathbb{C}$, entonces $\Re(\lambda) < 0$.
			
				\textbf{Solución. }\fbox{Verdadero.} Todos los casos se resuelven aplicando los discos de Gerchgorin. El número de la diagonal, dentro de los discos, es fijo en todos los casos, y los radios no varían:

				$$ r_i = \sum_{\substack{j=1 \, j \ne i}}^{n} |a_{i,j}| < 1, \quad r = \max_i r_i < 1;$$
				
				En el dibujo se aprecian los 3 casos y cómo los valores propios se quedan a la izquierda o  
				a la derecha de 0, según cada uno.
				\begin{figure}[H]
					\centering 
					\begin{tikzpicture}[scale=1]
						\begin{axis}[xmin=-2, xmax=3.5, ymin=-1, ymax= 1, axis x line=center, axis y line=center, xlabel=$\bb{R}$, ylabel=$i\bb{R}$, xtick={-1,0,1,2}, ytick={0}, thick, height=7cm, width=10cm]
							\filldraw[black] (-1, 0) circle (2pt);
							\filldraw[black] (1, 0) circle (2pt);
							\filldraw[black] (2, 0) circle (2pt);
							
							\draw[dashed] (-1, 0) -- (-1.6,0.3);
							\draw[dashed] (1, 0) -- (0.4,0.3);
							\draw[dashed] (2, 0) -- (1.4,0.3);
							
							\draw node at (-1.2, 0.25){$r$};
							\draw node at (0.8, 0.25){$r$};
							\draw node at (1.8, 0.25){$r$};
							
							\draw node[anchor=south] at (-1.6,0.45){\fbox{1.l)}};
							\draw node[anchor=south] at (0.5,0.45){\fbox{1.j)}};
							\draw node[anchor=south] at (1.8,0.45){\fbox{1.k)}};
							
							\begin{scope}[on background layer]
								\filldraw[fill=cyan!40!white, draw=cyan!40!white] (-1, 0) circle (1.3cm);
								\filldraw[fill=yellow!50!white, draw=yellow!30!white] (1, 0) circle (1.3cm);
								\filldraw[fill=lime!50!white, draw=lime!50!white] (2, 0) circle (1.3cm);
							\end{scope}
						\end{axis}
					\end{tikzpicture}
				\end{figure}
			
			\item La matriz $A$ no tiene valor propio dominante.\\
				
				\textbf{Solución. }\fbox{Falso.} Jugando con matrices sencillas por bloques
				$$\begin{pmatrix} -1 & a & 0 \\ a & -1 & 0 \\ 0 & 0 & -1 \end{pmatrix},$$ 
				cuyos valores propios son fáciles de calcular: $\lambda_1 = -1 - a$, $\lambda_2 = -1$ y $\lambda_3 = -1 + a$. Podemos hallar un contraejemplo tomando cualquier $0 < a < 1$.
				
				
			\item La matriz $A$ tiene valor propio dominante.
			\item La matriz $A$ tiene valor propio dominante $\lambda = -1$. \\
			
				\textbf{Solución. }\fbox{Falso.} Por ejemplo una matriz 
				$$\begin{pmatrix} -1 & a & 0 \\ 0 & -1 & 0 \\ 0 & 0 & -1 \end{pmatrix},$$ 
				tomando cualquier $0 \leq a < 1$, que sólo tiene un valor propio: $\lambda_1 = -1$ repetido 3 veces y por tanto, no es dominante.
		\end{enumerate}
	\end{ejercicio}

	% R (2)
	\begin{ejercicio}[3 puntos]
		Sea $A \in \mathbb{M}_{n \times n}(\mathbb{R})$ una matriz simétrica que, por lo tanto, sabemos que tiene $n$ vectores propios $\{v_1, \dots, v_n\}$ perpendiculares y se pueden tomar formando una base ortonormal de $\mathbb{R}^n$, es decir
		
		$$\|v_i\|_2^2 = \langle v_i, v_i \rangle = v_i^T v_i = 1 \quad \forall i, \quad \text{ y } \quad \langle v_i, v_j \rangle = v_i^T v_j = 0 \, \forall i \neq j.$$
		
		Además, en este caso suponemos que sus $n$ valores propios reales son distintos. Tomando cualquiera de ellos, por ejemplo $\lambda_n$, y su vector propio asociado $v_n$, consideramos la matriz: \\
		
		$A^* = A - \lambda_n v_n v_n^T,$ (observa que $v_n$ es columna y $v_n^T$ es fila, por lo que $v_n v_n^T$ es una matriz $n \times n$).
		
		\begin{enumerate}[label=\alph*)]
			\item Demuestra que $A^* v_n = 0$ y que $A^* v_i = \lambda_i v_i$ para $i \neq n$. \\
				Simplemente, calculamos:
				$$A^* v_n = (A - \lambda_n v_n v_n^T) v_n = A v_n - \lambda_n v_n \overbrace{(v_n^T v_n)}^{=1} = \lambda_n v_n - \lambda_n v_n = 0.$$
				
				Análogamente, para $i \neq n$:
				$$A^* v_i = (A - \lambda_n v_n v_n^T) v_i = A v_i - \lambda_n v_n \overbrace{(v_n^T v_i)}^{=0} = \lambda_i v_i - 0 = \lambda_i v_i.$$
				
			\item Determina el conjunto $\sigma(A^*)$ en función de $\sigma(A)$. \\
			
				Como consecuencia del apartado anterior, los valores propios de $A^*$ son los mismos que los de $A$, salvo el $\lambda_n$, que ha sido sustituido por $\lambda = 0$. Así ya vale; con símbolos sería, por ejemplo:
				$$\sigma(A^*) = (\sigma(A) \setminus \{\lambda_n\}) \cup \{0\}.$$
				
			\item Utiliza estos apartados para idear un método, combinando si es preciso el de las potencias usado varias veces, para calcular el espectro completo de $A$. \\
			
				La idea es aplicar el ejercicio de forma iterada para, en cada paso, sacar el dominante, “transformarlo en 0” e iterar para sacar el siguiente. Previamente, para evitar que en un paso aparezca un “dominante” y su opuesto, trasladaremos $A$ para que todos sus valores propios estén a un solo lado de $0$, pues ordenados ya están (son $\leq$). El proceso podría ser:
				
				\textbf{Paso 1.} Aplicar Discos de Gerchgorin para hallar $a > 0$ tal que $\ldots \sigma(A) \subseteq [-a, a]$. \\
				Traslado $A_1 = (A - aI)$: valores propios $\mu_i = \lambda_i - a$ \\
				$\Rightarrow A_1$ tiene espectro ordenado y negativo: $\sigma(A_1) = \{\mu_1 < \mu_2 < \cdots < \mu_n < 0\}$.
				
				\textbf{Paso 2, bucle:} para $k=1,2,\dots,n$
				
				\begin{itemize}
					\item Para $k=1$: aplico potencias a $A_1$ y obtengo $\mu_1$ y $v_1$; $\ldots \lambda_1 = \mu_1 + a$. \\
					Defino $A_2 = A_1^* = A_1 - \mu_1 v_1 v_1^T$, así su espectro: $\ldots \sigma(A_2) = \{\mu_2, \ldots, \mu_n, 0\}$.
					
					\item Para $k=2$: aplico potencias a $A_2$ y obtengo $\mu_2$ y $v_2$; $\ldots\lambda_2 = \mu_2 + a$. \\
					Defino $A_3 = A_2^* = A_2 - \mu_2 v_2 v_2^T$, así su espectro: $\ldots \sigma(A_3) = \{\mu_3, \ldots, \mu_n, 0, 0\}$.
					
					\item Caso genérico $k$: aplico potencias a $A_k$ y obtengo $\mu_k$ y $v_k$; $\ldots\lambda_k = \mu_k + a$. \\
					Defino $A_{k+1} = A_k^* = A_k - \mu_k v_k v_k^T$, su espectro $\ldots\sigma(A_{k+1}) = \{\mu_{k+1}, \ldots, \mu_n, 0, \ldots, 0\}$.
				\end{itemize}
				
				Al concluir el bucle, tenemos todos los $\lambda_k$ del espectro de $A$.
		\end{enumerate}
	\end{ejercicio}	
	
	% R (3)
	\begin{ejercicio}[3 puntos]
		Considere el método iterativo: 
		$$\begin{cases}
			x^{(k+1)}_1 = \displaystyle \frac{1}{5} (4 x^{(k)}_2 + 3) \\
			\\
			x^{(k+1)}_2 = \displaystyle \frac{1}{6} (4 x^{(k+1)}_1 - 8)
		\end{cases}$$
		
		\begin{enumerate}[label=\alph*)]
			\item Determina un sistema $Ax = b$ con el que sea consistente. \\
				
				Quienes solo mirando que es Gauss-Seidel aplicado a un sistema $2 \times 2$, llevan ventaja:
				$$\begin{cases}
					x_1^{(k+1)} = \displaystyle \frac{1}{5} \left( 4 x_2^{(k)} + 3 \right) = \frac{1}{a_{1,1}} \left( -a_{1,2} x_2^{(k)} + b_1 \right) \\
					\\
					x_2^{(k+1)} = \displaystyle \frac{1}{6} \left( 4 x_1^{(k+1)} - 8 \right) = \frac{1}{a_{2,2}} \left( -a_{2,1} x_1^{(k+1)} + b_2 \right)
				\end{cases} \Rightarrow
				A = \begin{pmatrix} 5 & -4 \\ -4 & 6 \end{pmatrix}, \quad 
				b = \begin{pmatrix} 3 \\ -8 \end{pmatrix}.$$
				
				Para el resto, basta recordar que para que $Ax = b$ sea consistente, el punto fijo del método y su solución han de ser iguales; por tanto, partimos del punto fijo y despejamos:
				$$\begin{cases}
					x_1 = \displaystyle \frac{1}{5} \left( 4 x_2 + 3 \right) \quad \Rightarrow \quad 5 x_1 - 4 x_2 = 3 \\
					\\
					x_2 = \displaystyle \frac{1}{6} \left( 4 x_1 - 8 \right) \quad \Rightarrow \quad -4 x_1 + 6 x_2 = -8
				\end{cases}$$
				
				Obteniendo el mismo sistema, ¡aunque valdría cualquier otro equivalente!
				
				De camino, si lo vemos como Gauss-Seidel (que lo es), la matriz $B$ del método será:
				$$B = 
				\begin{pmatrix}
					5 & 0 \\
					-4 & 6
				\end{pmatrix}^{-1}
				\begin{pmatrix}
					0 & 4 \\
					0 & 0
				\end{pmatrix}
				= \frac{1}{30}
				\begin{pmatrix}
					6 & 0 \\
					4 & 5
				\end{pmatrix}
				\begin{pmatrix}
					0 & 4 \\
					0 & 0
				\end{pmatrix}
				= 
				\begin{pmatrix}
					0 & \nicefrac{4}{5} \\
					0 & \nicefrac{8}{15}
				\end{pmatrix}
				= 
				\begin{pmatrix}
					0 & 0.8 \\
					0 & 0.5333
				\end{pmatrix}.$$
				
			
			\item ¿Es convergente? En caso afirmativo, ¿a qué converge? \\
			
				Por supuesto, la misma $B$ (obviamente) también puede deducirse directamente desde el método dado para responder este apartado (y me sale la c, aunque no la necesito):
				
				$$\begin{cases}
					x_1^{(k+1)} = \displaystyle \frac{1}{5} \left( 4 x_2^{(k)} + 3 \right) = 0.8\, x_2^{(k)} + 0.6, \text{ y lo sustituyo en la de abajo:} \\ 
					\\
					x_2^{(k+1)} = \displaystyle \frac{1}{6} \left( \frac{4}{5} \left( 4 x_2^{(k)} + 3 \right) - 8 \right) = \frac{16}{30} x_2^{(k)} + \frac{12}{30} - \frac{8}{6} = 0.5333\, x_2^{(k)} - 0.9333
				\end{cases}$$
				$$ \Rightarrow
				\begin{pmatrix}
					x_1^{(k+1)} \\
					x_2^{(k+1)}
				\end{pmatrix}
				=
				\begin{pmatrix}
					0.8x_2^{(k)} + 0.6 \\
					0.5333x_2^{(k)} - 0.9333
				\end{pmatrix}
				=
				\begin{pmatrix}
					0 & 0.8 \\
					0 & 0.5333
				\end{pmatrix}
				\begin{pmatrix}
					x_1^{(k)} \\
					x_2^{(k)}
				\end{pmatrix}
				+
				\begin{pmatrix}
					0.6 \\
					-0.9333
				\end{pmatrix}$$
				
				En este caso, se puede ver sin hacer cuentas que su radio espectral es $\rho(B) = 0.5333 < 1$, por ser $B$ triangular, como su norma infinito: $\|B\|_\infty = 0.8 < 1$, por lo que el método converge.
				
				Y converge, bien a la solución $x = A^{-1} b$ del sistema dado en el apartado a) y recordado en el apartado c, bien al punto fijo del método original, ¡que son iguales por consistencia!
				
				Podemos usar por tanto ambas cosas para determinarlo, y sale fácilmente $x = (-1, -2)$.
				
			
			\item Comenzando con $(x_1, x_2)^{(0)} = (1,0)$, determina el número mínimo de iteraciones que garantice que el error absoluto cometido al aproximar $A^{-1}b$ sea inferior a $10^{-5}$. \\
			
				Como ya tenemos: $\|B\|_{\infty} = 0.8 < 1$, usamos el resultado de clase que sigue, e imponemos la segunda desigualdad (nótese que solo usamos la norma $\|\cdot\|_{\infty}$):
				
				$$
				\|x^{(k)} - x\|_{\infty} \leq \frac{\|B\|_{\infty}^k}{1 - \|B\|_{\infty}} \|x^{(1)} - x^{(0)}\|_{\infty} \Rightarrow
				\frac{(0.8)^k}{0.2} \cdot \|x^{(1)} - x^{(0)}\|_{\infty} \leq 10^{-5}
				$$
				
				Para concluir, calculamos una iteración $\displaystyle x^{(1)} = \left( \frac{3}{5}, \frac{-14}{15} \right)$, de donde: $$\displaystyle \|x^{(1)} - x^{(0)}\|_{\infty} = \frac{14}{15} = 0.933$$
				
				Por tanto, solo resta despejar $k$, tomando logaritmos:
				$$k \geq \frac{1}{\ln(0.8)} \ln\left( \frac{10^{-5} \cdot 0.2}{0.933} \right) \approx 58.5 \Rightarrow \text{ Hay que dar al menos 59 pasos.}
				$$
				
			
		\end{enumerate}
	\end{ejercicio}

\end{document}