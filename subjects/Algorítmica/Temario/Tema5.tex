\chapter{Programación Dinámica}
\begin{itemize}
    \item Para problemas en los que necesitamos estados anteriores (en fibonacci, para calcular fibonacci(n) necesitamos tener fibonacci($n-1$) y fibonacci($n-2$), y para fibonacci($n-1$) necesitamos, \ldots).
    \item En el camino aparecen requisitos que se repiten (necesitamos calcular varias veces fibonacci(k)). En vez de calcularlo todas las veces, calcularlo una sola vez. Para calcular fibonacci(6) necesitamos 5 veces fibonacci(2).
    \item Antes de calcular el subproblema, mira si lo tienes ya resuelto (si lo tiene, lo usa y si no lo tiene, lo calcula).
    \item Es necesaria una estructura para almacenar las soluciones a los subproblemas, con la finalidad de ahorrar llamadas recursivas.
\end{itemize}
Ejemplos de dónde usar programación dinámica son:
\begin{itemize}
    \item Fibonacci.
    \item Calcular números combinatorios.
    \item Calcular potencias naturales.
    \item Cualquier problema con solapamiento de subproblemas (encontramos subproblemas que se repiten).
\end{itemize}
Una cosa es la programación dinámica y otra es la memorización:
\begin{description}
    \item [Memorización.] Almacenamos en una estructura (como un diccionario) los resultados.
    \item [Programación dinámica.] Una vez que los hemos almacenado, buscamos un patrón para ver cómo se completan las soluciones de alguna forma más eficiente (quitando sobrecarga por la recursividad). Buscamos una forma de rellenar la estructura de datos.
\end{description}

\subsubsection{Cuando aplicar programación dinámica}
Normalmente para problemas de optimización (minimizar o maximizar). La solución al problema la tenemos que ver como un proceso de selección de varias etapas.
\begin{itemize}
    \item Se aplica a problemas que pueden suponer un alto coste computacional que dispone de subestructuras optimales que se solapan (se repiten a lo largo del cálculo de la solución).
\end{itemize}
La eficiencia del algoritmo suele ser polinomial. Normalmente suele ser $O(n\cdot m)$ donde $n$ es el tamaño de la estructura de datos y $m$ el tiempo para cada casilla.

\subsubsection{Comparación con Divide y Vencerás}
\begin{description}
    \item [Divide y vencerás.] \
        \begin{itemize}
            \item Se aplica a subproblemas independientes.
            \item La técnica suele ser descendente.
        \end{itemize}
    \item [Programación dinámica.]  \
        \begin{itemize}
            \item Se aplica a subproblemas que se solapan (que se resuelven más de una vez).
            \item La técnica suele ser ascendente.
            \item Asegura optimilidad pero puede llevar a un algoritmo que no sea eficiente.
        \end{itemize}
\end{description}

\begin{teo}[Principio de Optimalidad de Bellman]\label{principio_optimalidad}
    Una solución óptima está compuesta de subsoluciones óptimas.
\end{teo}
Cuando se cumpla el principio, se podrá utilizar la programción dinámica.
\begin{ejemplo}
    Un ejemplo que no cumple el Principio de Optimalidad de Bellman es el problema del camino más largo entre dos nodos en un grafo. 

    Por tanto, no podemos aplicar programación dinámica para resolverlo.
\end{ejemplo}

\section{Pasos para desarrollar un algoritmo}
\begin{enumerate}
    \item Plantear la solución a un problema como una secuencia de decisiones.
    \item Ver que se verifica el principio de optimalidad~\ref{principio_optimalidad}.
    \item Plantear la solución como una función recursiva y ver la tipología de los subprobelmas.
    \item Ver cómo un problema grande se puede calcular a partir de los problemas más pequeños.
    \item Tratar de buscar un enfoque ascendente (resolver problemas pequeños y resolver problemas mayores).
\end{enumerate}
Ante un problema del estilo buscar un camino óptimo, es capaz de decir el costo del camino pero no de decir el camino. Para ello:
\begin{itemize}
    \item O se puede deducir el camino a partir del costo.
    \item O apuntar en una tabla auxiliar las decisiones tomadas.
\end{itemize}

\section{Ejemplos de Programación Dinámica}
\begin{ejemplo}
    Problema de devolver cambio: ¿cómo devolver el cambio con el menor número de monedas?\\

    Podemos considerarlo como un problema de resolución multietápica. Cumple el Principio de Optimalidad de Bellman, luego podemos aplicar programación dinámica para resolver el problema.

\end{ejemplo}

\begin{ejemplo}
    Producto encadenado de matrices. Su solución es similar al árbol binario de búsqueda óptima (por ejemplo, en un árbol de palabras, poner las más frecuentes arriba y las menos abajo).

    \begin{itemize}
        \item Ante una solución que es multiplicar al tun tún, es mejor calcular primero el número de operaciones a realizar y quedarse con la multiplicación que dé el menor.
    \end{itemize}
    Se cumple el Principio de Optimalidad de Bellman, luego puede hacerse con programación dinámica.\\

    Buscamos hacer la definición recursiva de la solución. Para ello vemos dónde colocar el primer paréntesis:
    \begin{gather*}
        N[i,j] = 0 \qquad \text{\ si\ } i =j \\
        N[i,j] = \min_{k \in [i,j]} \{N[i,k] + N[k+1,j] + p_{i-1}p_kp_j \} \text{\ si\ } i <j
    \end{gather*}
    Siendo $N[i,j]$ el costo de multiplicar desde la matriz $i$-ésima a la $j$-ésima.
    \begin{itemize}
        \item Tenemos $O(n^2)$ subproblemas.
        \item Los subproblemas triviales son aquellos en los que $i=j$.
    \end{itemize}
    Representamos los $N[i,j]$ en una tabla. Formas de rellenar la tabla:
    \begin{itemize}
        \item Por diagonales.
        \item De abajo a arriba y de izquierda a derecha.
    \end{itemize}
    Una vez rellenada la tabla triangular superior, tenemos el número mínimo de operaciones para calcular el producto.\\

    Tenemos un algoritmo $O(n^3)$. Sabemos el costo mínimo pero no tenemos cuál es la parantización que nos lo da. Para ello, o volvemos sobre nuestros pasos o almacenamos el menor en cada momento (el $k$ de cada paso).
\end{ejemplo}

\begin{ejemplo}
    Subsecuencia común de mayor longitud (LCS): Dadas dos secuencias de símbolos $X$ e $Y$, buscamos la mayor subsecuencia común a ambas de mayor longitud (pudiendo estar tanto en $X$ como en $Y$ caraceteres intermedios):
    \begin{align*}
        X &= A\ B\ C\ \red{B}\ \red{D}\ \red{A}\ \red{B} \\
        Y &= \red{B}\ \red{D}\ C\ \red{A}\ \red{B}\ A
    \end{align*}
    La mayor subsecuencia común es $B\ D\ A\ B$, aunque también puede ser $B\ C\ B\ A$.\newline
    Tiene aplicaciones en genética, diferencias de archivos y detección de plagio.\\

    \begin{itemize}
        \item Un algoritmo de fuerza bruta compararía cualquier subsecuencia de $X$ con símbolos de $Y$. 

            Si $|X| = m$ y $|Y| = n$, hay que contrastar $2^m$ subsecuencias de $X$ contra los $n$ elementos de $Y$, o al revés. Siendo:
            \begin{gather*}
                k = \min \{n,m\} \qquad t = \max\{n,m\}
            \end{gather*}
            tenemos un orden de $O(t\cdot 2^k)$, en el mejor caso.
    \end{itemize}
    Cumple el Principio de Optimalidad de Bellman. Buscamos una secuencia de decisiones a desarrollar: Si meto o no un carácter.\\

    Definimos:
    \begin{equation*}
        X_i = (x_1, \ldots, x_i) \qquad Y_j = (y_1, \ldots, y_j)
    \end{equation*}
    Considerando que $X = (x_1, \ldots, x_m)$ con $m\geq i$ y $Y = (y1, \ldots, y_n)$ con $n\geq j$.
    \begin{equation*}
        c[i,j] = \left\{ \begin{array}{ll}
                c[i-1,j-1] + 1 & \text{\ si\ } x[i] = y[j] \\ 
                \max \{c[i,j-1], c[i-1,j]\} & \text{\ si\ } x[i] \neq y[j]
            \end{array}\right.
    \end{equation*}
    siendo $c[i,j]$ la longitud de la solución óptima al problema de $X_i$ y $Y_j$.\\

    Por tanto, la solución es $c[n,m]$. Rellenaríamos la matriz por filas o por columnas (ver qué necesitamos para calcular $c[i,j]$, en cada caso). Algoritmo de orden $O(n\cdot m)$.\\

    Para encontrar la solución miramos la solución recursiva que hicimos:
    \begin{itemize}
        \item Si son iguales la columna y la fila, disminuimos en 1 la fila y la columna.
        \item Si no son iguales, vuelvo al máximo.
    \end{itemize}
\end{ejemplo}

\begin{ejemplo}
Dado un grafo y dos nodos, buscar el camino mínimo entre ambos.

Secuencia de decisiones a tomar: si consideramos un vértice o no para ser vértice intermedio del camino.

\begin{notacion}
    Notaremos por $D[i,j]$ al camino mínimo entre $i$ y $j$. 

    Además, notaremos por $D_k[i,j]$ al camino mínimo entre $i$ y $j$ en un problema de los primeros $k$ nodos del grafo.
\end{notacion}
De esta forma, $D_0[i,j]<\infty \Longleftrightarrow $ hay un camino directo entre $i$ y $j$.

Definición recursiva:
\begin{itemize}
    \item Si el camino de $i$ a $j$ por $k$ vértices no pasa por $k$:
        \begin{equation*}
            D_k[i,j] = D_{k-1}[i,j]
        \end{equation*}
    \item Si pasar por $k$:
        \begin{equation*}
            D_k[i,j] = D_{k-1}[i,k] + D_{k-1}[k,j]
        \end{equation*}
\end{itemize}
De donde:
\begin{equation*}
    D_k[i,j] = \min \{D_{k-1}[i,j], D_{k-1}[i,k] + D_{k-1}[k,j]\}
\end{equation*}
Tenemos varias matrices, desde 0 hasta $n-1$ (siendo $n$ el tamaño del grafo). Notemos que a la hora de construir la matriz $D_k$, no alteramos ni la fila $k$ ni la columna $k$.\\

Obtenemos el \textbf{algoritmo de Floyd}, de eficiencia $O(n^3)$.
    
\end{ejemplo}

\begin{ejemplo}
    Distancia de edición. Dadas dos cadenas, buscar la transformación de menor coste para llegar de una palabra a otra.

    Podemos reemplazar, insertar y borrar caracteres.

    \begin{notacion}
        Notamos por $d(i,j)$ a la distancia de edición de transformar la cadena desde el carácter 1 hasta el $i$ hasta la que tenemos por transformar desde el 1 hasta el $j$.
    \end{notacion}
    Definición recursiva:
    \begin{equation*}
        d(i,j) = \left\{\begin{array}{lll}
                d(i-1, j-1) & \text{si} & s[i] = t[j] \\
                1 + \min \{ d(i-1,j),d(i,j-1),d(i-1,j-1)\} & \text{si} & s[i] \neq t[j]
        \end{array}\right.
    \end{equation*}
    Por ejemplo:
    \begin{align*}
        d(\text{casa}, \text{costa}) &= d(\text{cas},\text{cost}) \\
        d(\text{casas}, \text{cosa}) &= \min \left\{ \begin{array}{l}
            1 + d(\text{casa},\text{cosa}), \\
            1 + d(\text{casas}, \text{cos}), \\
            1 + d(\text{casa}, \text{cos})
        \end{array}\right\}
    \end{align*}
\end{ejemplo}

\begin{ejemplo}
    Problema de caminos mínimos con peso negativo. Estudiamos el algoritmos de Bellman-Ford.

    \begin{notacion}
        Notamos por $D_i(v)$ al camino de costo mínimo entre el nodo $v$ (origen) y el $t$ (destino) usando como mucho $i$ arcos.
    \end{notacion}
\end{ejemplo}
