\subsection{Relación 2}
\setcounter{ejercicio}{0}


\begin{ejercicio}
    Sea la función $f(x) = e^x - ax^2$ con $a \in [3, 4]$.
    \begin{enumerate}
        \item Demuestra que tiene una raíz negativa, otra raíz en $[0, 1]$ y otra mayor que 1.\\
        
        Evaluamos $f$ en los extremos del intervalo $[0,1]$:
        \begin{align*}
            f(0) &= e^0 - a\cdot 0^2 = 1\\
            f(1) &= e^1 - a\cdot 1^2 = e - a<0 \Longleftrightarrow e < a
        \end{align*}

        Calculamos además los límites de $f$ en $\pm \infty$:
        \begin{align*}
            \lim_{x\to -\infty} f(x) &= \lim_{x\to -\infty} e^x - ax^2 = \lim_{x\to -\infty} -ax^2 = -\infty\\
            \lim_{x\to \infty} f(x) &= \lim_{x\to \infty} e^x - ax^2 = \infty
        \end{align*}

        Para demostrar lo pedido, y debido a que $f$ es continua, emplearemos el Teorema de Bolzano.
        \begin{itemize}
            \item Como $\lim\limits_{x\to -\infty} f(x) = -\infty$ y $f(0) = 1>0$, por el Teorema de Bolzano, $f$ tiene una raíz en $\bb{R}^-$.
            \item Como $f(0) = 1>0$ y $f(1) = e - a < 0$, por el Teorema de Bolzano, $f$ tiene una raíz en $[0,1]$.
            \item Como $f(1) = e - a < 0$ y $\lim\limits_{x\to \infty} f(x) = \infty$, por el Teorema de Bolzano, $f$ tiene una raíz en $\left[1, \infty\right[$.
        \end{itemize}
        \item Demuestra que $x = g_1(x) = \sqrt{\frac{e^x}{a}}$ y $x = g_2(x) = -\sqrt{\frac{e^x}{a}}$ son ecuaciones equivalentes a la de partida.
        
        Partiendo de las ecuaciones dadas, elevamos al cuadrado ambos lados, llegando en ambos casos a:
        \begin{align*}
            x^2 &= \frac{e^x}{a} \iff a\cdot x^2 = e^x \iff e^x - ax^2 = 0
        \end{align*}

        Por tanto, efectivamente son equivalentes. La ecuación $x=g_1(x)$ tiene sentido para $x\geq 0$ y la ecuación $x=g_2(x)$ para $x\leq 0$.
        \item Toma $a = 3$. Demuestra la convergencia local hacia la raíz próxima a $-0.5$ partiendo de $x_0 = 0$ usando $g_2(x)$ y realiza dos iteraciones.\\
        
        Trabajaremos en el intervalo $[-1,0]$. Veamos que $g_2\left([-1,0]\right) \subset [-1,0]$. Para ello, como $g_2\in C^\infty(\bb{R})$, calculamos su derivada:
        \begin{align*}
            g_2'(x) = -\frac{1}{2\sqrt{\frac{e^x}{a}}}\cdot \frac{e^x}{a} = -\frac{e^x}{2a\sqrt{\frac{e^x}{a}}}<0\qquad \forall x\in \bb{R}
        \end{align*}
        Por tanto, tenemos que $g_2$ es continua y estrictamente decreciente en $\bb{R}$. Además, evaluamos $g_2$ en los extremos del intervalo:
        \begin{align*}
            g_2(-1) &= -\sqrt{\frac{e^{-1}}{3}} = -\frac{1}{\sqrt{3e}}\approx -0.35\\
            g_2(0) &= -\sqrt{\frac{e^0}{3}} = -\frac{1}{\sqrt{3}}\approx -0.58
        \end{align*}

        Por tanto, $g_2\left([-1,0]\right)=\left[-\frac{1}{\sqrt{3}}, -\frac{1}{\sqrt{3e}}\right]\subset [-1,0]$. Por tanto, podemos considerar $g_2: [-1,0]\to [-1,0]$.
        Veamos ahora que $g_2$ es una contracción en $[-1,0]$:
        \begin{equation*}
            |g_2'(x)|=\left|\frac{e^x}{6\sqrt{\frac{e^x}{3}}}\right|=\frac{e^x}{6\sqrt{\frac{e^x}{3}}}\leq 
            \frac{e^0}{6\sqrt{\frac{e^{-1}}{3}}}\approx 0.47<1\qquad \forall x\in [-1,0]
        \end{equation*}

        Por tanto, $g_2$ es una contracción en $[-1,0]$. Por el Teorema del Punto Fijo, $g_2$ tiene un único punto fijo en $[-1,0]$. Además, la sucesión $x_{n+1}=g_2(x_n)$ converge a dicho punto fijo para cualquier $x_0\in [-1,0]$.\\
        Veamos ahora las primeras dos iteraciones tomando $x_0=0$:
        \begin{equation*}
            \begin{array}{c|c|c}
                n & x_n & g_2(x_n)\\ \hline
                0 & 0 & -0.57735\\
                1 & -0.57735 & -0.4325829\\
                2 & -0.4325829 & -0.4650559
            \end{array}
        \end{equation*}

        
        \item Toma $a = 3$. Demuestra la convergencia local hacia la raíz próxima a 1 partiendo de $x_0 = 0$ usando $g_1(x)$ y realiza dos iteraciones.\\
        
        Trabajaremos en el intervalo $[0,2]$. Veamos que $g_1\left([0,2]\right) \subset [0,2]$. Para ello, como $g_1\in C^\infty(\bb{R})$, calculamos su derivada:
        \begin{align*}
            g_1'(x) = -g_2'(x) = \dfrac{e^x}{2a\sqrt{\frac{e^x}{a}}}>0\qquad \forall x\in \bb{R}
        \end{align*}
        Por tanto, tenemos que $g_1$ es continua y estrictamente creciente en $\bb{R}$. Además, evaluamos $g_1$ en los extremos del intervalo:
        \begin{align*}
            g_1(0) &= -g_2(0) = \sqrt{\frac{e^0}{3}} = \frac{1}{\sqrt{3}}\approx 0.58\\
            g_1(2) &= \sqrt{\frac{e^2}{3}} = \frac{e}{\sqrt{3}}\approx 1.5694
        \end{align*}

        Por tanto, $g_1\left([0,2]\right)=\left[\frac{1}{\sqrt{3}}, \frac{e}{\sqrt{3}}\right]\subset [0,2]$. Por tanto, podemos considerar $g_1: [0,2]\to [0,2]$.
        Veamos ahora que $g_1$ es una contracción en $[0,2]$:
        \begin{equation*}
            |g_1'(x)|=|g_2'(x)|=\dfrac{e^x}{6\sqrt{\frac{e^x}{3}}}<1\Longleftrightarrow
            e^{2x}<36\cdot \dfrac{e^x}{3}
            \iff e^x<12\iff x<\ln 12\approx 2.48
        \end{equation*}
        Por tanto, $g_1$ es una contracción en $[0,2]$. Por el Teorema del Punto Fijo, $g_1$ tiene un único punto fijo en $[0,2]$. Además, la sucesión $x_{n+1}=g_1(x_n)$ converge a dicho punto fijo para cualquier $x_0\in [0,2]$.\\
        Veamos ahora las primeras dos iteraciones tomando $x_0=0$:
        \begin{equation*}
            \begin{array}{c|c|c}
                n & x_n & g_1(x_n)\\ \hline
                0 & 0 & 0.57735\\
                1 & 0.57735 & 0.770565\\
                2 & 0.770565 & 0.848722
            \end{array}
        \end{equation*}
        \item Toma $a = 3$. Comprueba que la raíz mayor que 1 está en $[3, 4]$. Demuestra la no convergencia hacia la raíz próxima a 4 partiendo de $x_0$ muy próximo a ella (pero diferente de ella) usando $g_1(x)$ y encuentra una función para la iteración funcional, alternativa a las anteriores que converja a la raíz cercana a 4. Partiendo de $x_0 = 3.98$ obtén $x_1$ y $x_2$ con el método propuesto.\\
        
        Evaluamos $f$ en los extremos del intervalo $[3,4]$:
        \begin{align*}
            f(3) &= e^3 - 3\cdot 3^2 = e^3 - 3^3<0\\
            f(4) &= e^4 - 3\cdot 4^2 = e^4 - 3\cdot 4^2>0
        \end{align*}

        Por tanto, por el Teorema de Bolzano, $f$ tiene una raíz en $[3,4]$.\\

        Para estudiar la no convergencia hacia la raíz próxima a $4$ partiendo de $x_0$ muy próximo a ella, anteriormente vimos que:
        \begin{equation*}
            |g'(x)|<1\iff x<\ln 12\approx 2.48
        \end{equation*}

        Por tanto, como el punto fijo $s$ está en $[3,4]$, tenemos que $g'(s)>1$. Por tanto, la sucesión $x_{n+1}=g(x_n)$ no converge a $s$ si $x_0$ es muy próximo a $s$.\\

        Para encontrar una función que converja a la raíz próxima a 4, tenemos que:
        \begin{equation*}
            f(x)=0\iff
            e^x=ax^2\iff
            x=\ln(ax^2)
        \end{equation*}

        Por tanto, consideramos la función $h(x)=\ln(3x^2)$. Veamos que $h\left([3,5]\right)\subset [3,5]$. Para ello, calculamos la derivada de $h$:
        \begin{align*}
            h'(x) = \dfrac{6x}{3x^2} = \dfrac{2}{x}>0\qquad \forall x\in \bb{R}^+
        \end{align*}

        Por tanto, $h$ es continua y estrictamente creciente en $[3,5]$. Además, evaluamos $h$ en los extremos del intervalo:
        \begin{align*}
            h(3) &= \ln(3\cdot 3^2) = \ln 27\approx 3.3\\
            h(5) &= \ln(3\cdot 5^2) = \ln 75\approx 4.32
        \end{align*}

        Por tanto, $h\left([3,5]\right)=\left[\ln 27, \ln 75\right]\subset [3,5]$. Por tanto, podemos considerar $h: [3,5]\to [3,5]$.\\
        Veamos ahora que $h$ es una contracción en $[3,5]$:
        \begin{equation*}
            |h'(x)|=\left|\dfrac{2}{x}\right|=\dfrac{2}{x}<\frac{2}{3}<1\qquad \forall x\in [3,5]
        \end{equation*}
        Por tanto, $h$ es una contracción en $[3,5]$. Por el Teorema del Punto Fijo, $h$ tiene un único punto fijo en $[3,5]$. Además, la sucesión $x_{n+1}=h(x_n)$ converge a dicho punto fijo para cualquier $x_0\in [3,5]$.\\
        Veamos ahora las primeras dos iteraciones tomando $x_0=3.98$:
        \begin{equation*}
            \begin{array}{c|c|c}
                n & x_n & h(x_n)\\ \hline
                0 & 3.98 & 3.861176\\
                1 & 3.861176 & 3.800556\\
                2 & 3.800556 & 3.7689
            \end{array}
        \end{equation*}
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea la ecuación $p(x) = x^3 - 8x^2 + 20x - 15.2 = 0$.
    \begin{enumerate}
        \item Prueba que no tiene ninguna raíz menor que 1.
        
        Como $p\in C^\infty(\bb{R})$, podemos calcular la derivada de $p$ y estudiar su signo:
        \begin{multline*}
            p'(x) = 3x^2 - 16x + 20 = 0
            \iff \\ \iff x = \frac{16 \pm \sqrt{16^2 - 4\cdot 3\cdot 20}}{2\cdot 3}
            \iff x = \frac{16 \pm 4}{6}
            \iff x\in \{2, \nicefrac{10}{3}\}
        \end{multline*}

        Por tanto, $p$ es estrictamente creciente en $\left]-\infty, 2\right[$. Tenemos que:
        \begin{align*}
            p(1)=1-8+20-15.2= -2.2 < 0\\
            \lim_{x\to -\infty} p(x) = -\infty
        \end{align*}

        Por tanto, deducimos que $p$ no tiene raíces menores que 1.

        \item Prueba que Newton-Raphson converge partiendo de $x_0 = 0$ hacia la raíz más pequeña y realiza dos iteraciones.\\
        
        Tenemos que:
        \begin{align*}
            p(1)&=-2.2<0\\
            p(2)&=8-32+40-15.2=0.8>0
        \end{align*}

        Por tanto, por el Teorema de Bolzano, $p$ tiene una raíz en $\left[1,2\right]$, y sabemos que es única por ser $p$ estrictamente creciente en $\left]-\infty, 2\right[$. Por último, es la raíz más pequeña por no tener ninguna raíz menor que 1. Comprobemos ahora que cumple las condiciones del Teorema de Convergencia del Método de Newton-Raphson en $\left[1,2\right]$:
        \begin{enumerate}
            \item $p(1)p(2)<0$.
            \item $p'(x)\neq 0$ en $\left]1,2\right[$.
            \item $p''(x)=6x-16$ no cambia de signo en $\left]1,2\right[$, ya que:
            \begin{equation*}
                p''(x)=6x-16=0 \iff x=\frac{16}{6}=\frac{8}{3}\approx 2.67\notin \left]1,2\right[
            \end{equation*}
            \item $p(x_0)p''(x_0)=(-15.2)\cdot (-16)>0$.
        \end{enumerate}

        Por tanto, el método de Newton-Raphson converge en $\left[1,2\right]$ a la raíz más pequeña partiendo desde $x_0=0$. Este método genera la siguiente sucesión:
        \begin{align*}
            x_{n+1} &= x_n - \frac{p(x_n)}{p'(x_n)} = x_n - \frac{x_n^3 - 8x_n^2 + 20x_n - 15.2}{3x_n^2 - 16x_n + 20}=\\
            &= \frac{2x_n^3 - 8x_n^2 + 15.2}{3x_n^2 - 16x_n + 20}
        \end{align*}
        Por tanto, las dos primeras iteraciones son:
        \begin{equation*}
            \begin{array}{c|c}
                n & x_n\\ \hline
                0 & 0 \\
                1 & 0.76\\
                2 & 1.196844
            \end{array}
        \end{equation*}
        \item Calcula la sucesión de Sturm y decide si existen raíces múltiples.
        
        Definimos:
        \polymul\ResAnt{x^3-8x^2+20x-152/10}{1}
        \polymul\Res{3x^2-16x+20}{1}
        \begin{align*}
            f_0(x) &= p(x) = \polyprint\ResAnt\\
            f_1(x) &= f_0'(x) = \polyprint\Res
        \end{align*}

        Calculamos ahora $f_2(x)$.
        \begin{equation*}
            \polylongdiv[style=D]\ResAnt\Res
        \end{equation*}
        \polydiv\div\ResAnt\Res
        \polymul\ResAnt\Res{1}
        \polymul\Res\polyremainder{-45}

        Para simplificar, establecemos:
        \begin{align*}
            f_2(x) &= -45\cdot R(f_0(x), f_1(x))\\
            &= -45\cdot \left(\polyprint\polyremainder\right)\\
            &= \polyprint\Res
        \end{align*}

        Calculamos ahora $f_3(x)$.
        \begin{equation*}
            \polylongdiv[style=D]\ResAnt\Res
        \end{equation*}
        \polydiv\div\ResAnt\Res
        \polymul\ResAnt\Res{1}
        \polymul\Res\polyremainder{-100/117}

        Para simplificar, establecemos:
        \begin{align*}
            f_3(x) &= -\dfrac{100}{117}\cdot R(f_1(x), f_2(x))\\
            &= -\dfrac{100}{117}\cdot \left(\polyprint\polyremainder\right)\\
            &= \polyprint\Res
        \end{align*}

        Por tanto, la sucesión de Sturm es:
        \begin{align*}
            f_0(x) &= x^3 - 8x^2 + 20x - 15.2\\
            f_1(x) &= 3x^2 - 16x + 20\\
            f_2(x) &= 40x - 116\\
            f_3(x) &= 1
        \end{align*}

        Estudiemos ahora la existencia de raíces múltiples. Por el algoritmo extendido de euclídes, sabemos que:
        \begin{equation*}
            \mcd(f_0, f_1) = \mcd(p, p') = 1
        \end{equation*}

        Supongamos ahora que existe una raíz múltiple de multiplicidad $m>1$. Entonces:
        \begin{align*}
            p(x) &= (x-x_0)^m\cdot q(x)\\
            p'(x) &= (x-x_0)^{m-1}\left[m\cdot q(x) + (x-x_0)q'(x)\right]
        \end{align*}
        Entonces, $(x-x_0)^{m-1}$ divide a $p,p'$, lo que contradice que $\mcd(p,p')=1$. Por tanto, no existen raíces múltiples. De forma general, como el último resto no nulo es contante, entonces no existen raíces múltiples.

        \item Separa las raíces reales de dicha ecuación.
        
        En primer lugar, buscamos acotar las raíces reales. Tenemos:
        \begin{equation*}
            \alpha = 20\Longrightarrow
            |s|\leq 21
        \end{equation*}

        Por tanto, sabemos que todas las raíces reales están en $\left[-21, 21\right]$.
        Por el primer apartado, sabemos que están en $\left[1,21\right]$.
        Separamos ahora las raíces:
        \begin{equation*}
            \begin{array}{c|c|c|c|c|c}
                x & \sgn(f_0(x)) & \sgn(f_1(x)) & \sgn(f_2(x)) & \sgn(f_3(x))\ & \text{Nº Cambios Signo}\\ \hline
                1 & - & + & - & + & 3\\
                2 & + & 0 & - & + & 2\\
                3 & - & - & + & + & 1\\
                4 & + & + & + & + & 0\\
                21 & + & + & + & + & 0
            \end{array}
        \end{equation*}

        Por tanto, tenemos una raíz en $\left[1,2\right]$, otra en $\left[2,3\right]$ y otra en $\left[3,4\right]$.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea la ecuación $f(x) = e^{x-1} - ax^3 = 0$ siendo $a > 1$.
    \begin{enumerate}
        \item Demuestra que tiene al menos una raíz en $[0, 1]$.
        
        Como $f$ es continua, podemos aplicar el Teorema de Bolzano. Evaluamos $f$ en los extremos del intervalo:
        \begin{align*}
            f(0) &= e^{-1} > 0\\
            f(1) &= e^0 - a = 1 - a < 0
        \end{align*}
        Por tanto, por el Teorema de Bolzano, $f$ tiene al menos una raíz en $[0, 1]$.
        \item A partir de ahora considera $a = 2$. Calcula las dos primeras aproximaciones $x_1$ y $x_2$ obtenidas con bisección (siendo $x_0 = 0.5$). Indica el error máximo que se comete con $x_2$.
        \begin{equation*}
            \begin{array}{c|c|c|c|c}
                n & a_n & b_n & x_n & f(x_n) \\ \hline
                0 & 0 & 1 & 0.5 & 0.3565\\
                1 & 0.5 & 1 & 0.75 & -0.0649\\
                2 & 0.5 & 0.75 & 0.625 & 0.1990
            \end{array}
        \end{equation*}

        El error máximo que se comete con $x_2$ es:
        \begin{equation*}
            |e_2| < \dfrac{1-0}{2^{2+1}} = \dfrac{1}{8} = 0.125
        \end{equation*}
        \item Realiza dos iteraciones con el método de la secante tomando como valores iniciales (o semillas) $x_0 = 0$, $x_1 = 1$. Debes calcular $x_2$ y $x_3$.\\
        
        El método de la secante se define como:
        \begin{align*}
            x_{n+1} &= x_n - \frac{f(x_n)(x_n - x_{n-1})}{f(x_n) - f(x_{n-1})}
            =\\&= 
            x_n - \frac{(e^{x_n-1} - 2x_n^3)(x_n - x_{n-1})}{e^{x_n-1} - 2x_n^3 - e^{x_{n-1}-1} + 2x_{n-1}^3}
        \end{align*}

        Por tanto, las dos primeras iteraciones son:
        \begin{equation*}
            \begin{array}{c|c}
                n & x_n\\ \hline
                0 & 0 \\
                1 & 1 \\
                2 & 0.2689\\
                3 & 0.4932
            \end{array}
        \end{equation*}
        \item Evalúa la función en la segunda aproximación $x_2$ obtenida con bisección e indica, razonadamente con los resultados que se te han pedido, si se puede asegurar, o no, que la segunda aproximación obtenida con bisección está más cerca de la raíz que la segunda aproximación obtenida con la secante.\\
        
        Evaluamos $f$ en $x_2$ obtenido con el método de bisección:
        \begin{equation*}
            f(x_2) = f(0.625) = 0.199>0
        \end{equation*}

        Además, sabemos que $f(0.75)<0$. Por tanto, si la raíz es $s$:
        \begin{equation*}
            x_2 = 0.625 < s < 0.75
        \end{equation*}

        Como la segunda iteración del método de secante es $x_3 = 0.4932$, tenemos que:
        \begin{equation*}
            x_3=0.4932 < x_2=0.625 < s < 0.75
        \end{equation*}

        Por tanto, la segunda aproximación obtenida con bisección está más cerca de la raíz que la segunda aproximación obtenida con la secante.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Considera la ecuación $x^2 = a$ siendo $a > 0$.
    \begin{enumerate}
        \item Se pretende usar el método de Newton-Raphson en la ecuación anterior para hallar la raíz cuadrada de $a$. Deduce que el método se puede expresar, en este caso, de la forma
        \[
            x_{n+1} = \frac{1}{2}\left(x_n + \frac{a}{x_n}\right)
        \]

        Sea la función $f(x) = x^2 - a$. Aplicando el método de Newton-Raphson, tenemos:
        \begin{align*}
            x_{n+1} &= x_n - \frac{f(x_n)}{f'(x_n)} = x_n - \frac{x_n^2 - a}{2x_n} = x_n - \frac{x_n}{2} + \frac{a}{2x_n} = \dfrac{1}{2}\left(x_n + \dfrac{a}{x_n}\right)
        \end{align*}
        \item Demuestra que el método es convergente partiendo de $x_0 = \max\{1, a\}$.
        
        Si $a=1$, entonces $f(x_0)=f(1)=0$. Por tanto, $x_0$ es raíz de $f$ y hemos terminado. Suponemos por tanto en adelante que $a\neq 1$.\\
        
        Emplearemos el intervalo $\left[\min\{1,a\},\max\{1,a\}\right]$. Calculamos la derivada de $f$:
        \begin{align*}
            f'(x) = 2x\qquad \forall x\in \bb{R}
        \end{align*}
        
        Demostramos ahora los 4 puntos necesarios para el Teorema de Convergencia del Método de Newton-Raphson:
        \begin{enumerate}
            \item $f(a)f(1) < 0$:
            \begin{align*}
                f(a)=a^2 - a = a(a-1)
                f(1) = 1 - a
            \end{align*}

            Por tanto:
            \begin{equation*}
                f(a)f(1) = a(a-1)(1-a) = -a(a-1)^2<0
            \end{equation*}

            \item $f'(x) \neq 0$ en $\left[\min\{1,a\},\max\{1,a\}\right]$, pues el $0$ no pertenece a dicho intervalo.
            \item $f''(x)=2 > 0$ no cambia de signo en $\bb{R}$.
            \item $f(x_0)f''(x_0)$
            
            \begin{equation*}
                f(x_0)f''(x_0) = 2f(x_0)>0\iff
                x_0^2>a \iff (\max\{1,a\})^2>a
            \end{equation*}
            que sabemos que es cierto.
        \end{enumerate}

        Por tanto, sabemos que el método de Newton-Raphson es convergente partiendo de $x_0 = \max\{1, a\}$.
        \item Apoyándote en la expresión anterior obtén la segunda aproximación $x_2$ de la raíz cuadrada positiva de 13, partiendo de $x_0 = 13$.
        
        Tomando $a=13$, la segunda aproximación de la raíz cuadrada positiva de 13 es:
        \begin{equation*}
            \begin{array}{c|c}
                n & x_n\\ \hline
                0 & 13 \\
                1 & 7\\
                2 & 4.428571
            \end{array}
        \end{equation*}
        

        \item Determina la expresión del método de Newton-Raphson para la raíz cúbica de un número diferente de cero y aplícalo dos veces para aproximar la raíz cúbica de 13 partiendo de $x_0 = 13$.\\
        
        Sea la función $g(x) = x^3 - a$. Aplicando el método de Newton-Raphson, tenemos:
        \begin{align*}
            x_{n+1} &= x_n - \frac{g(x_n)}{g'(x_n)} = x_n - \frac{x_n^3 - a}{3x_n^2} = x_n - \frac{x_n}{3} + \frac{a}{3x_n^2} = \dfrac{1}{3}\left(2x_n + \dfrac{a}{x_n^2}\right)
        \end{align*}

        Por tanto, tomando $a=13$, las dos primeras iteraciones para aproximar la raíz cúbica de 13 son:
        \begin{equation*}
            \begin{array}{c|c}
                n & x_n\\ \hline
                0 & 13 \\
                1 & 8.692308\\
                2 & 5.852224
            \end{array}
        \end{equation*}

        Notemos que la convergencia a la raíz cúbica de 13 no la tenemos asegurada, sería necesario estudiarlo en este caso (similar al segundo apartado).
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Sea $S$ la única solución en el dominio cuadrado $D = [0, 1] \times [0, 1]$ del sistema no lineal
    \[
        \begin{cases}
            xy^2 + 4x - 1 = 0\\
            4yx^2 + 6y - 1 = 0
        \end{cases}
    \]
    ¿Es convergente a $S$ la sucesión de iteraciones del método de iteración funcional definido por
    \[
        \begin{cases}
            x_{n+1} = \dfrac{1}{4 + y_n^2}\\\\
            y_{n+1} = \dfrac{1}{6 + 4x_n^2}
        \end{cases}
    \]
    cualquiera que sea la aproximación inicial $(x_0, y_0) \in [0, 1] \times [0, 1]$?\\

    Podemos reescribir el sistema dado como sigue:
    \begin{equation*}
        \begin{cases}
            x = \dfrac{1}{4 + y^2}\\\\
            y = \dfrac{1}{6 + 4x^2}
        \end{cases}
    \end{equation*}

    Definimos ahora la función siguiente:
    \Func{G=(G_1,G_2)}{D}{D}{(x, y)}{\left(\dfrac{1}{4 + y^2}, \dfrac{1}{6 + 4x^2}\right)}

    Por tanto, la solución del sistema dado es el punto fijo de $G$. Veamos ahora si el método de iteración funcional converge a dicho punto fijo. En primer lugar, veamos que $G(D)\subset D$:
    \begin{align*}
        G_1(x, y) &= \dfrac{1}{4 + y^2}\in \left[0, 1\right]\\
        G_2(x, y) &= \dfrac{1}{6 + 4x^2}\in \left[0, 1\right]
    \end{align*}
    Por tanto, $G(D)\subset D$. Veamos ahora que $G$ es una contracción en $D$. Para ello, calculamos la matriz jacobiana de $G$:
    \begin{equation*}
        J_G(x, y) = \begin{pmatrix}
            0 & -\dfrac{2y}{(4 + y^2)^2}\\
            -\dfrac{8x}{(6 + 4x^2)^2} & 0
        \end{pmatrix}\qquad \forall (x, y)\in D
    \end{equation*}

    Además, tenemos que:
    \begin{align*}
        \frac{2y}{(4 + y^2)^2} &\leq \frac{2}{4^2} = \frac{1}{8}\qquad \forall y\in [0, 1]\\
        \frac{8x}{(6 + 4x^2)^2} &\leq \frac{8}{6^2} = \frac{8}{36} = \frac{2}{9}\qquad \forall x\in [0, 1]
    \end{align*}

    Por tanto, $\|J_G(x, y)\|_{\infty}\leq \nicefrac{2}{9}<1$. Como además sus derivadas parciales son continuas, $G$ es una contracción en $D$. Por tanto, por el Teorema del Punto Fijo, $G$ tiene un único punto fijo $S\in D$, y la sucesión:
    \begin{equation*}
        X_{n+1} = G(X_n)\qquad \forall n\in \bb{N}
    \end{equation*}
    converge a dicho punto fijo para cualquier $X_0\in D$. Por tanto, la sucesión de iteraciones del método de iteración funcional converge a $S$ para cualquier aproximación inicial $(x_0, y_0)\in [0, 1]\times [0, 1]$.
\end{ejercicio}

\begin{ejercicio}
    Se sabe que $f : [a, b] \to \bb{R}$ es una función continua en $[a, b]$ y posee un único cero en dicho intervalo. ¿Se puede aproximar siempre dicho cero mediante el método de bisección?\\

    No, no siempre se puede aproximar el cero de una función mediante el método de bisección. Por ejemplo, sea la siguiente función:
    \Func{f}{[-1, 1]}{\bb{R}}{x}{x^2}

    En este caso, sabemos que hay una única raíz en $[-1,1]$, que es $x=0$. Sin embargo, el método de bisección no converge a dicha raíz, puesto que no hay un cambio de signo en dicho intervalo, por lo que no podemos construir la sucesión $x_n$ que converge a la raíz. De hecho:
    \begin{equation*}
        f(-1) = f(1) = 1 > 0
    \end{equation*}

    Por tanto, la condición que falla es que no hay cambio de signo en el intervalo $[-1,1]$, por lo que no se puede aplicar el método de bisección en este caso.
\end{ejercicio}

\begin{ejercicio}
    Se considera la ecuación $f(x) = x^3 - x - 1 = 0$. Se pide:
    \begin{enumerate}
        \item Demuestra que la ecuación anterior tiene una única solución real $s$.
        
        Como $f\in C^\infty(\bb{R})$, podemos calcular la derivada de $f$ y estudiar su signo:
        \begin{align*}
            f'(x) = 3x^2 - 1 = 0
            \iff x = \pm \dfrac{1}{\sqrt{3}}
        \end{align*}

        Estudiamos la monotonía de $f$, estudiando a su vez si hay raíces en dicho intervalo (por el Teorema de Bolzano):
        \begin{itemize}
            \item \ul{En $\left]-\infty, -\dfrac{1}{\sqrt{3}}\right[$}: $f$ es estrictamente creciente. Además:
            \begin{equation*}
                f\left(-\dfrac{1}{\sqrt{3}}\right)<0
            \end{equation*}
            Por tanto, $f$ no tiene raíces en $\left]-\infty, -\dfrac{1}{\sqrt{3}}\right[$.
            
            \item \ul{En $\left]-\dfrac{1}{\sqrt{3}}, \dfrac{1}{\sqrt{3}}\right[$}: $f$ es estrictamente decreciente. Además:
            \begin{equation*}
                f\left(-\dfrac{1}{\sqrt{3}}\right)<0
            \end{equation*}
            Por tanto, $f$ no tiene raíces en $\left]-\dfrac{1}{\sqrt{3}}, \dfrac{1}{\sqrt{3}}\right[$.
            
            \item \ul{En $\left]\dfrac{1}{\sqrt{3}}, +\infty\right[$}: $f$ es estrictamente creciente. Además:
            \begin{align*}
                f\left(1\right)&=-1<0&
                f(2)&=5>0
            \end{align*}
            Por tanto, por el Teorema de Bolzano, $f$ tiene una raíz en $\left]1, 2\right[$.
        \end{itemize}

        Por tanto, $f$ tiene una única raíz real $s$, y sabemos que:
        \begin{equation*}
            s\in \left]1, 2\right[
        \end{equation*}
        \item Encuentra un intervalo $[a, b]$ en el que al tomar cualquier punto $x_0 \in [a, b]$ como aproximación inicial del método de Newton-Raphson aplicado a $f(x)$ se asegure que la sucesión de iteraciones de dicho método converja a $s$ con convergencia al menos cuadrática y demuestra que eso es así.\\
        
        Tomamos el intervalo $[1, 2]$. Estudiamos la función $f$ en dicho intervalo:
        \begin{enumerate}
            \item $f(1)f(2)<0$.
            \item $f'(x)\neq 0$ en $\left[1, 2\right]$.
            \begin{equation*}
                \frac{1}{\sqrt{3}}<1\iff 1<\sqrt{3}\iff 1<3
            \end{equation*}

            Por tanto, $f'(x)\neq 0$ en $\left[1, 2\right]$.

            \item $f''(x)=6x$ no cambia de signo en $\left[1, 2\right]$.
            
            \item Comprobemos la última condición:
            \begin{equation*}
                \left|\dfrac{f(1)}{f'(1)}\right| = \left|\dfrac{-1}{2}\right|<1
                \qquad \left|\dfrac{f(2)}{f'(2)}\right| = \dfrac{5}{11}<1
            \end{equation*}
        \end{enumerate}

        Por tanto, el método de Newton-Raphson converge a $s$ con convergencia al menos cuadrática partiendo de cualquier punto $x_0\in [1, 2]$.\\

        Para demostrar que efectivamente la convergencia es al menos cuadrática, consideramos la función $g$ siguiente:
        \Func{g}{[1, 2]}{\bb{R}}{x}{x - \dfrac{f(x)}{f'(x)}}

        Dada la raíz $s$ de $f$, entonces:
        \begin{equation*}
            g(s) = s - \dfrac{f(s)}{f'(s)} = s - \dfrac{0}{f'(s)} = s
        \end{equation*}

        Calculamos ahora la derivada de $g$:
        \begin{equation*}
            g'(s) = 1 - \dfrac{f'(s)f'(s) - f(s)f''(s)}{f'(s)^2} = 1-\dfrac{f'(s)^2}{f'(s)^2} = 1-1 = 0
        \end{equation*}

        Por tanto, el método iterativo de Newton-Raphson aplicado a $f$, que coincide con el método iterativo de punto fijo aplicado a $g$, converge a $s$ con convergencia al menos cuadrática.
        \item Calcula las dos primeras iteraciones del método de Newton-Raphson para resolver la ecuación dada tomando como aproximación inicial $x_0 = 1$.\\
        
        El método de Newton-Raphson se define como:
        \begin{equation*}
            x_{n+1} = x_n - \dfrac{f(x_n)}{f'(x_n)} = x_n - \dfrac{x_n^3 - x_n - 1}{3x_n^2 - 1}
        \end{equation*}

        Por tanto, las dos primeras iteraciones son:
        \begin{equation*}
            \begin{array}{c|c}
                n & x_n\\ \hline
                0 & 1 \\
                1 & 1.5\\
                2 & 1.347826
            \end{array}
        \end{equation*}

    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Se pretende estimar el valor de $\sqrt[7]{2}$ usando un método iterativo.
    \begin{enumerate}
        \item Determina justificadamente una función $f$ y un intervalo $[a, b]$ donde se pueda aplicar el método de bisección. ¿Cuántas iteraciones son necesarias para conseguir un error inferior a $10^{-4}$?\\
        
        Sea la función $f$ siguiente:
        \Func{f}{\bb{R}}{\bb{R}}{x}{x^7 - 2}

        Estudiamos la función $f$ en el intervalo $[1, 2]$:
        \begin{equation*}
            f(1)=-1<0\qquad f(2)=2^7-2>0
        \end{equation*}

        Por tanto, por el Teorema de Bolzano, $f$ tiene una raíz en $\left]1, 2\right[$; y podemos aplicar el método de bisección en dicho intervalo. Calculamos el número de iteraciones necesarias para conseguir un error inferior a $10^{-4}$:
        \begin{multline*}
            e_n < \dfrac{2-1}{2^{n+1}} = \dfrac{1}{2^{n+1}} < 10^{-4}\iff 2^{n+1} > 10^4\iff\\\iff n+1 > \log_2(10^4) = 13.2877\iff n>12.2877
        \end{multline*}
        Por tanto, necesitamos al menos 13 iteraciones para conseguir un error inferior a $10^{-4}$.


        \item Determina justificadamente un intervalo $[a, b]$ y un valor inicial $x_0$ que permita asegurar que el método de Newton-Raphson converge a $\sqrt[7]{2}$ y realiza 3 iteraciones del método.\\
        
        Consideramos el intervalo $[1, 2]$. Veamos que podemos aplicar el método de Newton-Raphson en dicho intervalo:
        \begin{enumerate}
            \item $f(1)f(2)<0$.
            \item $f'(x)=7x^6$ no se anula en $\left[1, 2\right]$.
            \item $f''(x)=42x^5$ no cambia de signo en $\left[1, 2\right]$.
            \item Comprobamos la última condición:
            \begin{equation*}
                \left|\dfrac{f(1)}{f'(1)}\right| = \left|\dfrac{-1}{7}\right|<1
                \qquad \left|\dfrac{f(2)}{f'(2)}\right| = \dfrac{2^7-2}{7\cdot 2^6} = \dfrac{2^6-1}{7\cdot 2^5} = \dfrac{63}{7\cdot 32}<1
            \end{equation*}
        \end{enumerate}

        Por tanto, el método de Newton-Raphson converge a $\sqrt[7]{2}$ para cualquier semilla $x_0\in [1, 2]$. Realizamos 3 iteraciones del método tomando $x_0=1$:
        \begin{equation*}
            \begin{array}{c|c}
                n & x_n\\ \hline
                0 & 1 \\
                1 & 1.142857\\
                2 & 1.107819\\
                3 & 1.104127
            \end{array}
        \end{equation*}
        \item Se propone el método iterativo
        \[
            x_{n+1} = \frac{8x_n + 3x_n^8}{6 + 4x_n^7}
        \]
        Realiza 3 iteraciones del método empezando en el mismo valor $x_0$ del apartado anterior.

        Realizamos 3 iteraciones del método propuesto:
        \begin{equation*}
            \begin{array}{c|c}
                n & x_n\\ \hline
                0 & 1 \\
                1 & 1.1\\
                2 & 1.1040893\\
                3 & 1.1040895
            \end{array}
        \end{equation*}
        \item ¿Cuál de los dos métodos converge más rápidamente a la solución? Justifica la respuesta.\\
        
        En primer lugar, hemos de demostrar que si $s\in \bb{R}$ es una raíz de $f$, $f(s)=0$, entonces $s$ es un punto fijo de la función $g$ definida como:
        \Func{g}{\bb{R}^+}{\bb{R}}{x}{\dfrac{8x + 3x^8}{6 + 4x^7}}
        Tenemos que:
        \begin{equation*}
            g(s)=\dfrac{8s + 3s^8}{6 + 4s^7} = \dfrac{8s+3\cdot 2s}{6+4\cdot 2} = \dfrac{14s}{14} = s
        \end{equation*}

        De forma análoga, definimos la función $h$ empleada en Newton-Raphson:
        \Func{h}{\bb{R}^+}{\bb{R}}{x}{x - \dfrac{f(x)}{f'(x)}}

        Tenemos que $h(s)=g(s)=s$. Estudiamos las primeras derivadas:
        \begin{align*}
            g'(s) &= \dfrac{(8+8\cdot 3s^7)(6+4s^7) - (8s+3s^8)(28s^6)}{(6+4s^7)^2}
            =\\&= \dfrac{8(1+3s^7)}{6+4s^7} - \dfrac{28\left(8s^7+3s^{14}\right)}{(6+4s^7)^2}
            =\\&= \dfrac{8(1+3\cdot 2)}{6+4\cdot 2} - \dfrac{28\left(8\cdot 2+3\cdot 2^2\right)}{(6+4\cdot 2)^2}=0\\
            h'(s) &= 1 - \dfrac{f'(s)f'(s) - f(s)f''(s)}{f'(s)^2} = \dfrac{f(s)f''(s)}{f'(s)^2} = 0
        \end{align*}

        Como $g,h\in \cc{C}(\bb{R}^+)$ y $s$ es un punto fijo de ambas funciones, entonces sabemos que ambos métodos convergen a la raíz $s$ con convergencia al menos cuadrática. Estudiamos si convergerán con orden cúbico:
        \begin{align*}
            g''(s) &= 8\cdot \dfrac{3\cdot 7s^6(6+4s^7) - (1+3s^7)(4\cdot 7s^6)}{(6+4s^7)^2} -\\&\hspace{1cm}- 28\cdot \dfrac{[8\cdot 7s^6+3\cdot 14s^{13}](6+4s^7)-[8s^7+3s^{14}]\cdot 2\cdot 4\cdot 7s^6}{(6+4s^7)^3}
            =\\&= 8\cdot \dfrac{3\cdot 7s^6\cdot 14 - 7(4\cdot 7s^6)}{{14}^2} - 28\cdot \dfrac{[8\cdot 7s^6+3\cdot 28s^{6}]14-28\cdot 2\cdot 4\cdot 7s^6}{{14}^3}
            =\\&= 4s^6 - 4s^6 = 0\\
            h''(s) &= \dfrac{[f'(s)f''(s)+f(s)f'''(s)]f'(s)^2 - 2f(s)f''(s)f'(s)f''(s)}{f'(s)^4}
            =\\&= \dfrac{[f'(s)f''(s)+\cancelto{0}{f(s)}f'''(s)]f'(s) - 2\cancelto{0}{f(s)}f''(s)^2}{f'(s)^3}
            =\\&= \dfrac{[f'(s)f''(s)]f'(s)}{f'(s)^3} = \dfrac{f''(s)}{f'(s)} = \dfrac{7\cdot 6\cdot s^5}{7\cdot s^6} = \dfrac{6}{s}\neq 0
        \end{align*}

        Por tanto, el método de Newton-Raphson converge a $s$ con convergencia al menos cuadrática, mientras que el método propuesto converge a $s$ con convergencia al menos cúbica. Por tanto, el método propuesto converge más rápidamente a la solución.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio} Relacionado con la Sucesión de Sturm:
    \begin{enumerate}
        \item Sea $\{f_0(x), f_1(x), \ldots, f_m(x)\}$ una sucesión de Sturm en el intervalo $[a, b]$ y $k_i \in \bb{R}$ con $k_i > 0$ para $i = 0, \ldots, m$. Demuestra que si se define $\tilde{f}_i = k_i f_i$, entonces $\left\{\tilde{f}_0(x), \tilde{f}_1(x), \ldots, \tilde{f}_m(x)\right\}$ es también una sucesión de Sturm en $[a, b]$.\\
        
        En primer lugar, como el producto de funciones continuas por una constante es una función continua, $\tilde{f}_i\in \cc{C}[a, b]$.
        Hemos de comprobar ahora las 4 propiedades de la sucesión de Sturm:
        \begin{enumerate}
            \item Como $f_0\in \cc{C}^1[a, b]$, entonces $\tilde{f}_0=k_0f_0\in \cc{C}^1[a, b]$.
            \item Tenemos que:
            \begin{multline*}
                \tilde{f}_0(x) = k_0f_0(x) = 0\iff f_0(x) = 0\Longrightarrow f_0'(x)f_1(x)\neq 0
                \Longrightarrow \\\Longrightarrow \tilde{f}_0'(x)\tilde{f}_1(x)=k_0k_1f_0'(x)f_1(x)\neq 0
            \end{multline*}
            donde hemos usado que $k_0,k_1>0$.
            \item Fijado $j=1,\ldots,m-1$, tenemos que:
            \begin{multline*}
                \tilde{f}_j(x) = k_jf_j(x) = 0\iff f_j(x) = 0\Longrightarrow f_{j-1}(x)f_{j+1}(x)\neq 0
                \Longrightarrow \\\Longrightarrow \tilde{f}_{j-1}(x)\tilde{f}_{j+1}(x)=k_{j-1}k_{j+1}f_{j-1}(x)f_{j+1}(x)\neq 0
            \end{multline*}
            donde hemos usado que $k_{j-1},k_{j+1}>0$.
            \item $\tilde{f}_m(x) = k_mf_m(x)\neq 0$ en $[a, b]$.
        \end{enumerate}
        Por tanto, $\left\{\tilde{f}_0(x), \tilde{f}_1(x), \ldots, \tilde{f}_m(x)\right\}$ es una sucesión de Sturm en $[a, b]$.
        \item Dado el polinomio $p(x) = x^3 - x + 1$, determina justificadamente un intervalo en el que estén contenidas todas sus raíces.\\
        
        Sea $\alpha\in \bb{R}$ el máximo de los valores absolutos de los coeficientes de $p$; es decir, $\alpha=\max\{1, 1, 1\}=1$.        
        Por el Teorema de Acotación de Raíces, sabemos que todas las raíces de $p$ están contenidas en el intervalo $[-1-\alpha, 1+\alpha]=[-2, 2]$.
        
        \item Construye una sucesión de Sturm para el polinomio $p$ y utilízala para determinar el número de raíces reales así como intervalos de amplitud 1 en los que se encuentran.
        
        \polymul\ResAnt{x^3-x+1}{1}
        \polymul\Res{3x^2-1}{1}
        Definimos:
        \begin{align*}
            f_0(x) &= \polyprint\ResAnt\\
            f_1(x) &= \polyprint\Res
        \end{align*}

        Calculamos ahora $f_2(x)$.
        \begin{equation*}
            \polylongdiv[style=D]\ResAnt\Res
        \end{equation*}
        \polydiv\div\ResAnt\Res
        \polymul\ResAnt\Res{1}
        \polymul\Res\polyremainder{-3}

        Para simplificar, establecemos:
        \begin{align*}
            f_2(x) &= -3\cdot R(f_0(x), f_1(x))\\
            &= -3\cdot \left(\polyprint\polyremainder\right)\\
            &= \polyprint\Res
        \end{align*}

        Calculamos ahora $f_3(x)$.
        \begin{equation*}
            \polylongdiv[style=D]\ResAnt\Res
        \end{equation*}
        \polydiv\div\ResAnt\Res
        \polymul\ResAnt\Res{1}
        \polymul\Res\polyremainder{-4/23}

        Para simplificar, establecemos:
        \begin{align*}
            f_3(x) &= -\frac{4}{23}\cdot R(f_1(x), f_2(x))\\
            &= -\frac{4}{23}\cdot \left(\polyprint\polyremainder\right)\\
            &= \polyprint\Res
        \end{align*}

        Por tanto, la sucesión de Sturm para el polinomio $p$ es:
        \begin{align*}
            f_0(x) &= x^3 - x + 1\\
            f_1(x) &= 3x^2 - 1\\
            f_2(x) &= 2x-3\\
            f_3(x) &= -1
        \end{align*}

        Separamos ahora las raíces:
        \begin{equation*}
            \begin{array}{c|c|c|c|c|c}
                x & \sgn(f_0(x)) & \sgn(f_1(x)) & \sgn(f_2(x)) & \sgn(f_3(x)) & \text{Nº Cambios Signo}\\ \hline
                -2 & - & + & - & - & 2\\
                -1 & + & + & - & - & 1\\
                0 & + & - & - & - & 1\\
                1 & + & + & - & - & 1\\
                2 & + & + & + & - & 1
            \end{array}
        \end{equation*}
        Por tanto, $p$ tiene una única raíz real, y está contenida en el intervalo $[-2,-1]$.
        \item Realiza dos iteraciones del método de la secante para calcular de forma aproximada el valor de la raíz más pequeña justificando la convergencia.\\
        
        Tomemos como semillas los extremos del intervalo, $x_0=-2$ y $x_1=-1$. El método de la secante se define, para $n\geq 1$, como:
        \begin{align*}
            x_{n+1} &= x_n - f(x_n)\cdot \dfrac{x_n-x_{n-1}}{f(x_n)-f(x_{n-1})}
        \end{align*}

        Realizamos dos iteraciones del método de la secante:
        \begin{equation*}
            \begin{array}{c|c|c}
                n & x_n & f(x_n)\\ \hline
                0 & -2 & -5\\
                1 & -1 & 1\\
                2 & \nicefrac{-7}{6} & 0.578704\\
                3 & 3.101627
            \end{array}
        \end{equation*}

        \begin{observacion}
            Con los conocimientos vistos, no podemos asegurar la convergencia del método de la secante.
        \end{observacion}
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    Contesta razonadamente a las siguientes preguntas:
    \begin{enumerate}
        \item Se pretende resolver la ecuación $f(x) = 0$ utilizando el método de Newton-Raphson sabiendo que es convergente localmente ¿Qué debe cumplir (condición suficiente, no necesaria) la función $f$ para que dicho método tenga convergencia local al menos cúbica?\\

        Sea $I$ el intervalo de convergencia en el que además se encuentra la raíz $s$ de $f$. Supongamos que $f\in \cc{C}^4(I)$ (algo que no es muy restrictivo por las funciones que se suelen estudiar en cálculo numérico).        

        Definimos la función $g$ auxiliar siguiente:
        \Func{g}{I}{\bb{R}}{x}{x - \dfrac{f(x)}{f'(x)}}

        Vemos que el método de Newton-Raphson para $f$ consiste en el método iterativo para $g$. Por tanto, estudiaremos el orden de convergencia para $g$. Como $f\in \cc{C}^4(I)$, entonces, $g\in \cc{C}^3(I)$.
        Calculamos las derivadas de $g$:
        \begin{align*}
            g'(x) &= 1 - \dfrac{f'(x)f'(x) - f(x)f''(x)}{f'(x)^2} = 1-\dfrac{f'(x)^2-f(x)f''(x)}{f'(x)^2} = \dfrac{f(x)f''(x)}{f'(x)^2}\\
            g''(x) &= \dfrac{[f'(x)f''(x)+f(x)f'''(x)]f'(x)^2 - 2f(x)f''(x)f'(x)f''(x)}{f'(x)^4}
        \end{align*}

        Supongamos ahora que $f'(s)=0$, es decir, que no se trata de una raíz simple. Distinguimos casos, donde $m$ es la multiplicidad de la raíz $s$:
        \begin{itemize}
            \item \ul{$m=2$}: $f(x)=(x-s)^2q(x)$ con $q(s)\neq 0$. Entonces:
            \begin{align*}
                f'(x) &= 2(x-s)q(x) + (x-s)^2q'(x) = (x-s)[2q(x)+(x-s)q'(x)]\\
                f''(x) &= 2q(x) + 2(x-s)q'(x) + (x-s)\left[2q'(x)+q'(x)+(x-s)q''(x)\right]
            \end{align*}

            Por tanto:
            \begin{align*}
                g'(x) &= \dfrac{\cancel{(x-s)^2}q(x)\left[2q(x) + 2(x-s)q'(x) + (x-s)\left[2q'(x)+q'(x)+(x-s)q''(x)\right]\right]}{\cancel{(x-s)^2}[2q(x)+(x-s)q'(x)]^2}\\
                g'(s) &= \dfrac{2q^2(s)}{4q^2(s)} = \dfrac{1}{2}\neq 0
            \end{align*}

            Por tanto, el método de Newton-Raphson para $f$ tiene convergencia lineal, pero no cuadrática (ni cúbica).

            \item \ul{$m\geq 3$}: $f(x)=(x-s)^mq(x)$ con $q(s)\neq 0$. Entonces:
            \begin{align*}
                f'(x) &= (x-s)^{m-1}r(x)\qquad r(s)\neq 0\\
                f''(x) &= (x-s)^{m-2}t(x)\qquad t(s)\neq 0
            \end{align*}

            Por tanto:
            \begin{align*}
                g'(x) &= \dfrac{(x-s)^mq(x)\ (x-s)^{m-2}t(x)}{(x-s)^{2m-2}r(x)^2} = \dfrac{(x-s)^{2m-2}q(x)t(x)}{(x-s)^{2m-2}r(x)^2} = \dfrac{q(x)t(x)}{r(x)^2}\\
                g'(s) &= \dfrac{q(s)t(s)}{r(s)^2}\neq 0
            \end{align*}

            Por tanto, el método de Newton-Raphson para $f$ tiene convergencia lineal, pero no cuadrática (ni cúbica).
        \end{itemize}

        Por tanto, hemos de suponer $m=1$, es decir, $f'(s)\neq 0$. En ese caso, los denominadores no se anulan, y tenemos que:
        \begin{align*}
            g'(s) &= \dfrac{0}{f'(s)^2} = 0\\
            g''(s) &= \dfrac{f''(s)}{f'(s)}=0\iff f''(s)=0
        \end{align*}

        Por tanto, si $f\in \cc{C}^4(I)$, $f'(s)\neq 0$ y $f''(s)=0$, entonces el método de Newton-Raphson para $f$ tiene convergencia al menos cúbica.

        \item Si sabemos que $f$ tiene una única raíz real en el intervalo $[-1, 1]$, ¿Cuántas iteraciones del método de bisección hay que realizar para conseguir un error menor que $10^{-7}$?\\
        
        Supongamos que podemos aplicar el método de bisección en dicho intervalo; es decir, que $f(-1)f(1)<0$. Entonces:
        \begin{align*}
            |e_n| < \dfrac{1+1}{2^{n+1}} = \dfrac{1}{2^n}\leq10^{-7}
            \iff 2^n\geq 10^7\iff n\geq 23.52
        \end{align*}
        Por tanto, serán necesarias, como máximo (ya que en iteraciones anteriores podría haber un error menor) 24 iteraciones, y a partir de entonces el error siempre será menor que $10^{-7}$.
        \item ¿Es el método de Newton-Raphson para resolver el sistema $F(X) = 0$ invariante frente a transformaciones lineales de $F$?
        \begin{observacion}
            Que sea invariante frente a transformaciones lineales quiere decir que la secuencia de aproximaciones $\{X_n\}$ es la misma si se aplica el método al sistema $F(X) = 0$ o si se aplica al sistema $AF(X) = 0$, siendo $A$ una matriz no singular, partiendo del mismo vector inicial $X_0$.
        \end{observacion}

        Consideramos el método de Newton-Raphson para resolver el sistema $F(X) = ~0$:
        \begin{align*}
            X_{n+1} &= X_n - JF(X_n)^{-1}F(X_n)\qquad \forall n\in \bb{N}
        \end{align*}

        Dada ahora $A\in GL_k(\bb{R})$, consideramos ahora el método aplicado a $AF(X) =~0$:
        \begin{align*}
            X'_{n+1} &= X'_n - J(AF)(X'_n)^{-1}(AF)(X'_n)
        \end{align*}

        Por conocimientos de Análisis Matemático I, por ser $F$ diferenciale, entonces $AF$ es diferenciable con:
        \begin{align*}
            J(AF)(X) &= A\cdot JF(X)\qquad \forall X\in \bb{R}^k
        \end{align*}

        Además, como $A$ es regular, si $JF(X)$ es regular, entonces $J(AF)(X)$ también lo es, cumpliendo:
        \begin{align*}
            (J(AF)(X))^{-1} &= (A\cdot JF(X))^{-1} = JF(X)^{-1}\cdot A^{-1}\qquad \forall X\in \bb{R}^k
        \end{align*}

        Por tanto, tenemos que:
        \begin{align*}
            X'_{n+1} &= X'_n - J(AF)(X'_n)^{-1}(AF)(X'_n)
            = X'_n -JF(X'_n)^{-1}\cdot A^{-1}\cdot A\cdot F(X'_n)\\
            &= X'_n - JF(X'_n)^{-1}F(X'_n)
        \end{align*}

        Por tanto, y partiendo de la misma semilla $X_0=X'_0$, el método de Newton-Raphson aplicado a $F(X) = 0$ es invariante frente a transformaciones lineales de $F$; es decir,
        \begin{equation*}
            X_n = X'_n\qquad \forall n\in \bb{N}
        \end{equation*}
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
    El problema de trisección de un ángulo consiste en hallar las razones trigonométricas de $\nicefrac{\alpha}{3}$, conociendo las de $\alpha \in \left]0,\nicefrac{\pi}{2}\right[$.
    \begin{enumerate}
        \item Llamando $x = \sen\left(\nicefrac{\alpha}{3}\right)$ y $a = \sen \alpha$, demuestra que $x$ es solución de la ecuación $-4x^3 + 3x - a = 0$.
        
        Tenemos que:
        \begin{align*}
            \sen(\alpha)
            &= \sen\left(\dfrac{\alpha}{3}+\dfrac{2\alpha}{3}\right)=\sen\left(\dfrac{\alpha}{3}\right)\cos\left(\dfrac{2\alpha}{3}\right)+\cos\left(\dfrac{\alpha}{3}\right)\sen\left(\dfrac{2\alpha}{3}\right)\\
            &= \sen\left(\dfrac{\alpha}{3}\right)\left[\cos^2\left(\dfrac{\alpha}{3}\right)-\sen^2\left(\dfrac{\alpha}{3}\right)\right]+\cos\left(\dfrac{\alpha}{3}\right)\left[2\sen\left(\dfrac{\alpha}{3}\right)\cos\left(\dfrac{\alpha}{3}\right)\right]\\
            &= \sen\left(\dfrac{\alpha}{3}\right)\left[1-2\sen^2\left(\dfrac{\alpha}{3}\right)\right]+2\cos^2\left(\dfrac{\alpha}{3}\right)\sen\left(\dfrac{\alpha}{3}\right)\\
            &= \sen\left(\dfrac{\alpha}{3}\right)\left[1-2\sen^2\left(\dfrac{\alpha}{3}\right)+2\left(1-\sen^2\left(\dfrac{\alpha}{3}\right)\right)\right]\\
            &= \sen\left(\dfrac{\alpha}{3}\right)\left[3-4\sen^2\left(\dfrac{\alpha}{3}\right)\right]
            = 3\sen\left(\dfrac{\alpha}{3}\right)-4\sen^3\left(\dfrac{\alpha}{3}\right)
        \end{align*}

        Por tanto, $x=\sen\left(\nicefrac{\alpha}{3}\right)$ es solución de la ecuación $-4x^3 + 3x - a = 0$.

        \item Construye una sucesión de Sturm de polinomios asociada al polinomio dado por $p(x) = -4x^3 + 3x - a$ y deduce que $p$ tiene exactamente 3 raíces reales.\\
        
        Definimos:
        \polymul\ResAnt{-4x^3+3x-a}{1}
        \polymul\Res{-12x^2+3}{1}
        \begin{align*}
            f_0(x) &= p(x)=\polyprint\ResAnt\\
            f_1(x) &= p'(x)=\polyprint\Res
        \end{align*}

        Calculamos ahora $f_2(x)$.
        \begin{equation*}
            \polylongdiv[style=D]\ResAnt\Res
        \end{equation*}
        \polydiv\div\ResAnt\Res
        \polymul\ResAnt\Res{1}
        \polymul\Res\polyremainder{-1}

        Para simplificar, establecemos:
        \begin{align*}
            f_2(x) &= -1\cdot R(f_0(x), f_1(x))\\
            &= -1\cdot \left(\polyprint\polyremainder\right)\\
            &= \polyprint\Res
        \end{align*}

        Calculamos ahora $f_3(x)$.
        \begin{equation*}
            \polylongdiv[style=D]\ResAnt\Res
        \end{equation*}
        \polydiv\div\ResAnt\Res
        \polymul\ResAnt\Res{1}
        \polymul\Res\polyremainder{-1/3}

        Para simplificar, establecemos:
        \begin{align*}
            f_3(x) &= -\frac{1}{3}\cdot R(f_1(x), f_2(x))\\
            &= -\frac{1}{3}\cdot \polyprint\polyremainder\\
            &= -1+a^2
        \end{align*}
        Además, como $\alpha\in \left]0,\nicefrac{\pi}{2}\right[$, entonces $a\in \left]0, 1\right[$, por lo que $-1+a^2<0$. Por tanto, la sucesión de Sturm para el polinomio $p$ es:
        \begin{align*}
            f_0(x) &= -4x^3 + 3x - a\\
            f_1(x) &= -12x^2 + 3\\
            f_2(x) &= -2x+a\\
            f_3(x) &= -1+a^2
        \end{align*}

        Para ver cuántas raíces reales tiene, en primer lugar hemos de acotarlas. Definimos:
        \begin{align*}
            \alpha=\max\left\{\dfrac{3}{4},\dfrac{a}{4}\right\}=\dfrac{3}{4}
        \end{align*}
        
        Por el Teorema de Acotación de Raíces, sabemos que todas las raíces de $p$ están contenidas en el intervalo $[-1-\alpha, 1+\alpha]=\left[-\dfrac{7}{4}, \dfrac{7}{4}\right]$.
        
        Para deducir cuántas raíces reales tiene, hemos de estudiar los cambios de signo de la sucesión de Sturm en los extremos del intervalo:
        \begin{equation*}
            \begin{array}{c|c|c|c|c|c}
                x & \sgn(f_0(x)) & \sgn(f_1(x)) & \sgn(f_2(x)) & \sgn(f_3(x)) & \text{Nº Cambios Signo}\\ \hline
                -2 & + & - & + & - & 3\\
                0 & - & + & + & - & 2\\
                \nicefrac{a}{2} & + & + & 0 & - & 1\\
                1 & - & - & - & - & 0\\
                2 & - & - & - & - & 0
            \end{array}
        \end{equation*}
        donde las evaluaciones más problemáticas han sido:
        \begin{align*}
            f_0\left(\nicefrac{a}{2}\right) &= -4\left(\dfrac{a}{2}\right)^3 + 3\left(\dfrac{a}{2}\right) - a = -\dfrac{a^3}{2} + \dfrac{3a}{2} - a = \dfrac{a}{2}(1-a^2)>0\\
            f_1\left(\nicefrac{a}{2}\right) &= -12\left(\dfrac{a}{2}\right)^2 + 3 = -3a^2+3 = 3(1-a^2)>0\\
            f_2\left(\nicefrac{a}{2}\right) &= -2\cdot \dfrac{a}{2} + a = -a+a = 0
        \end{align*}

        Por tanto, $p$ tiene exactamente 3 raíces reales.
        \item Demuestra que $\sen\left(\nicefrac{\alpha}{3}\right)$ es la única solución de la ecuación $p(x) = 0$, en el intervalo $\left]0,\nicefrac{a}{2}\right[$ y que, tomando como valores iniciales $x_0 = \nicefrac{a}{3}$ o $x_0 = \nicefrac{a}{2}$, el método de Newton-Raphson converge.
        
        Sabemos que $\sen\left(\nicefrac{\alpha}{3}\right)$ es solución de la ecuación $p(x) = 0$, y además sabemos que la solución del intervalo $\left]0,\nicefrac{a}{2}\right[$ es única. Por tanto, tan solo nos queda demostrar que $\sen\left(\nicefrac{\alpha}{3}\right)\in \left]0,\nicefrac{a}{2}\right[$.
        \begin{align*}
            \dfrac{\sen \alpha}{2}>\sen\left(\dfrac{\alpha}{3}\right)&\iff
            \sen\alpha = 3\sen\left(\dfrac{\alpha}{3}\right)-4\sen^3\left(\dfrac{\alpha}{3}\right)>2\sen\left(\dfrac{\alpha}{3}\right)\iff \\ &\iff
            3-4\sen^2\left(\dfrac{\alpha}{3}\right)>2\iff 
            \sen^2\left(\dfrac{\alpha}{3}\right)<\frac{1}{4}
            \iff \\ &\iff \sen\left(\dfrac{\alpha}{3}\right)<\frac{1}{2}
            \iff \frac{\alpha}{3}<\frac{\pi}{6}\iff \alpha<\frac{\pi}{2}
        \end{align*}
        Por tanto, $\sen\left(\nicefrac{\alpha}{3}\right)\in \left]0,\nicefrac{a}{2}\right[$, y te tiene demostrado que es la única solución de la ecuación $p(x) = 0$ en dicho intervalo.

        Comprobamos ahora que el método de Newton-Raphson converge para $x_0 = \nicefrac{a}{3}$ y $x_0 = \nicefrac{a}{2}$. Para ello, calculamos $p'(x)$:
        \begin{align*}
            p'(x) &= -12x^2 + 3=0\iff x=\pm \dfrac{1}{2}
        \end{align*}

        Comprobamos las $4$ condiciones de convergencia para aplicar el método de Newton-Raphson en el intervalo $\left[0,\nicefrac{a}{2}\right]$:
        \begin{enumerate}
            \item $p(0)p(\nicefrac{a}{2})<0$ se tiene, como hemos comprobado al estudiar los cambios de signo de la sucesión de Sturm.

            \item $p'(x)\neq 0$ para $x\in \left[0,\nicefrac{a}{2}\right]$ por ser $a<1$. 
            \item $p''(x)$ no cambia de signo en $\left[0,\nicefrac{a}{2}\right]$.
            \item Comprobemos la última condición:
            \begin{align*}
                p(0) &= -a\\
                p'(0) &= 3\\
                p\left(\dfrac{a}{2}\right) &= \frac{a}{2}(1-a^2)\\
                p'\left(\dfrac{a}{2}\right) &= 3(1-a^2)
            \end{align*}

            Por tanto:
            \begin{align*}
                \left|\dfrac{p(0)}{p'(0)}\right| &= \dfrac{a}{3}<\frac{a}{2}\\
                \left|\dfrac{p\left(\nicefrac{a}{2}\right)}{p'\left(\nicefrac{a}{2}\right)}\right| &= \dfrac{\nicefrac{a}{2}(1-a^2)}{3(1-a^2)} = \dfrac{a}{6}<\frac{a}{2}
            \end{align*}

            Por tanto, el método de Newton-Raphson converge para todo $x_0\in \left[0,\nicefrac{a}{2}\right]$, y en particular para $x_0 = \nicefrac{a}{3}$ y $x_0 = \nicefrac{a}{2}$.


        \end{enumerate}
        \item Para resolver la ecuación anterior, se propone el método iterativo
        \[
            x_{n+1} = \frac{a}{3- 4x_n^2}
        \]
        Estudia bajo qué condiciones el método converge localmente a la solución. ¿Cuál de los dos métodos converge más rápidamente?

        Vemos en primer lugar losiguiente:
        \begin{equation*}
            \frac{a}{2}<\frac{3}{4}\iff a<\frac{3}{2}
        \end{equation*}
        que se tiene de forma directa. Por tanto, podemos definir la siguiente función:
        \Func{g}{\left[0,\nicefrac{a}{2}\right]}{\bb{R}}{x}{\dfrac{a}{3-4x^2}}

        Tenemos que $g\in \cc{C}^1\left(\left[0,\nicefrac{a}{2}\right]\right)$, y que:
        \begin{equation*}
            \hspace{-2cm}|g'(x)| = \left|\dfrac{-8ax}{(3-4x^2)^2}\right|
            = \dfrac{8a|x|}{(3-4x^2)^2}\leq \dfrac{8a\cdot \nicefrac{a}{2}}{(3-4\cdot (\nicefrac{a}{2})^2)^2} = \dfrac{4a^2}{(3-a^2)^2}<\dfrac{4}{(3-1)^2}=1\qquad \forall x\in \left[0,\nicefrac{a}{2}\right]
        \end{equation*}

        Por tanto, el método propuesto converge localmente a la única solución de la ecuación $p(x) = 0$ en el intervalo $\left[0,\nicefrac{a}{2}\right]$.\\

        Respecto al orden de convergencia, como $p$ tiene tres raíces reales sabemos que la multiplicidad de cada una de ellas es simple, por lo que el orden de convergencia es cuadrático. Por otro lado, para que este método tenga convergencia cuadrática es necesario que $g'(s)=0$, siendo $s$ la solución de la ecuación $p(x)=0$.
        \begin{align*}
            g'(x)=0&\iff x=0\\
            g(0)= 0&\iff a=\sen\alpha=0
        \end{align*}
        No obstante, esto no es posible, ya que $\alpha\in \left]0,\nicefrac{\pi}{2}\right[$. Por tanto, $g'(s)\neq 0$, y el método de Newton-Raphson converge más rápidamente que el método propuesto.

        \item Tomando $a = \nicefrac{1}{2}$, realiza una iteración del método de Newton-Raphson partiendo de $x_0 = \nicefrac{1}{6}$ para obtener una aproximación de $\sen\left(\nicefrac{\pi}{18}\right)$.
        
        Tomamos $a=\nicefrac{1}{2}$, $x_0=\nicefrac{1}{6}$ y aplicamos una iteración del método de Newton-Raphson:
        \begin{multline*}
            x_1 = x_0 - \dfrac{p(x_0)}{p'(x_0)} = \dfrac{1}{6} - \dfrac{-4\left(\dfrac{1}{6}\right)^3 + 3\left(\dfrac{1}{6}\right) - \frac{1}{2}}{-12\left(\dfrac{1}{6}\right)^2 + 3} =\\= \dfrac{1}{6} - \dfrac{\nicefrac{-1}{54} + \nicefrac{1}{2} - \nicefrac{1}{2}}{\nicefrac{-1}{3}+3} = \dfrac{1}{6} + \frac{3}{54\cdot 8} = \frac{25}{144}\approx 0.173611
        \end{multline*}
        \begin{equation*}
            \begin{array}{c|c|c}
                n & x_n\\ \hline
                0 & \nicefrac{1}{6}\\
                1 & 0.173611
            \end{array}
        \end{equation*}
    \end{enumerate}
\end{ejercicio}